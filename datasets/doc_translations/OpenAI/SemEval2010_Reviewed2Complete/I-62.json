{
    "id": "I-62",
    "original_text": "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete. To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider. On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search. In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used. Bounded rtdp concentrates the planning on significant states only and prunes the action space. The pruning is accomplished by proposing tight upper and lower bounds on the value function. Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence. General Terms Algorithms, Performance, Experimentation. 1. INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems. In general, resource allocation problems are known to be NP-Complete [12]. In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment. When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state. In general, the number of states is the combination of all possible specific states of each task and available resources. In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks. The very high number of states and actions in this type of problem makes it very complex. There can be many types of resource allocation problems. Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent. A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9]. In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources. The planning process starts with the initial state s0. In s0, each agent computes their respective Q-value. Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents. When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments. On the other hand, when the resources are available to all agents, no Q-decomposition is possible. A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently. For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment. Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions. To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states. McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds. This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem. On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds. This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach. Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn. Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp. The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation. The problem is now modelled. 2. PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}. These two tasks are either in the realized state, or not realized state. To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}. A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks. In this problem, a state represents a conjunction of the particular state of each task, and the available resources. The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint). Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task. For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change. When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable. Indeed, our model may consider consumable and non-consumable resource types. A consumable resource type is one where the amount of available resource is decreased when it is used. On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used. For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known. A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process. This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world. An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process. Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total. The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta. Also, S contains a non empty set sg ⊆ S of goal states. A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments). The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta. The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta. The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta. For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1. A solution of an mdp is a policy π mapping states s into actions a ∈ A(s). In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks. The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a. Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a. Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s). The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res. The global constraint is defined according to all system trajectories tra ∈ T RA. A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π. For example, state s is entered, which may transit to s or to s , according to action a. The two possible system trajectories are (s, a), (s ) and (s, a), (s ) . The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra. Since the available consumable resources are represented in the state space, this condition is verified by itself. In other words, the model is Markovian as the history has not to be considered in the state space. Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp. Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed. Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems. Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent. A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. For instance, a group of agents which manages the oil consummated by a country falls in this group. These agents desire to maximize their specific reward by consuming the right amount of oil. However, all the agents are penalized when an agent consumes oil because of the pollution it generates. Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements). In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2. This can happen depending on the type of missiles, their range, and so on. In this case, two agents can plan for both set of tasks to determine the policy. However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned. IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together. To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning. The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents. That is, R = i∈Ag Ri. It requires each agent to compute a value, from its perspective, for every action. To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration. The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value. That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si). The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition. Russell and Zimdars called this approach local Sarsa. In this way, an ideal compromise can be found for the agents to reach a global optimum. Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem. Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum. Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space. For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution. Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1. In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value. Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s . An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach. In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents. Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation. An agent considers as a possible state transition only the possible states of the set of tasks it manages. Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly. Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents. Then, the arbitrator functionalities are in Lines 8 to 20. The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a. In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s). Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup. Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions. On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i. Since |SAg| is combinatorial with the number of tasks, so |Si| |S|. Also, |A| is combinatorial with the number of resource types. If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents. In these circumstances, |Ai| |A|. In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup. Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup. Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem. However, when the resources are available to all agents, no Q-decomposition is possible. In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state. Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized). For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem. Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version. It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4]. The convergence is accomplished by means of a labeling procedure called checkSolved(s, ). This procedure tries to label as solved each traversed state in the current trajectory. When the initial state is labelled as solved, the algorithm has converged. In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions. This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s). Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved. On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s). For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s). The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function. Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values. Only the values of the state transitions change. Thus, having to compute two Q-values instead of one does not augment the complexity of the approach. In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained. In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A. Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than . When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9. In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|). Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made. This strategy has been proven as efficient [11] [6]. As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked. Since the upper bound for state s is known, it may be estimated The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm. Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal. If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution. Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival. Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution. The next sections describe two separate methods to define hL(s) and hU (s). First of all, the method of Singh and Cohn [10] is briefly described. Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space. Their approach is pretty straightforward. First of all, a value function is computed for all tasks to realize, using a standard rtdp approach. Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined. In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta). For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL. The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state. To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s). The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria. Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state. This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6]. In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach. The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound. To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach. Theorem 2.1. The upper bound defined by Equation 4 is admissible. Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a. However, hU (s) still overestimates V (s) because the global resource constraint is not enforced. Indeed, each task may use all consumable resources for its own purpose. Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources. Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU. A standard Bellman backup has a complexity of O(|A| × |S|). Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state. Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks. When each task has its own set of resources, each task may be solved independently. The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori. The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2]. In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm. Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves. The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state. In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach. These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources. The Line 5 initializes the valueta variable. This variable is the estimated value of each task ta ∈ T a. In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a. Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated. Here, a domain expert may separate all available resources in many types or parts to be allocated. The resources are allocated in the order of its specialization. In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early. Allocating the resources in this order improves the quality of the resulting lower bound. The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a. For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable. The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta. This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation. In Line 21, the marginal revenue is updated in function of the resources already allocated to each task. R(sgta ) is the reward to realize task ta. Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks. The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high. Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value. In Line 24, the resource type res is allocated to the group of resources Resta of task ta. Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta. The first part of the equation to compute valueta represents the expected residual value for task ta. This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res. In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res). For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available. All resource types are allocated in this manner until Res is empty. All consumable and non-consumable resource types are allocated to each task. When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32. When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL. In particular, the SinghL bound may The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain. As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components. The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource. However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound. Theorem 2.2. The lower bound of Equation 6 is admissible. Proof: Lowta(sta) is computed with the resource being shared. Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints. Indeed, as the resources are shared, the tasks cannot overuse them. Thus, hL(s) is a realizable policy, and an admissible lower bound. 3. DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements). For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks. In our problem, |Sta| = 4, thus each task can be in four distinct states. There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states. The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states. In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task. When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered. The effectiveness of each resource is modified randomly by ±15% at the start of a scenario. There are also local and global resource constraints on the amount that may be used. For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state. This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies. Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type. The global constraint is generated randomly at the start of a scenario for each consumable resource type. The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types. For this problem a standard lrtdp approach has been implemented. A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved. This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta. Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident. Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy. The approaches described in this paper are compared in Figures 1 and 2. Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context. In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp. To implement Qdec-lrtdp, we divided the set of tasks in two equal parts. The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi . Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type. When the number of tasks is odd, one more task was assigned to T ai . There are constraint between the group of resource Resi and Resi such that some assignments are not possible. These constraints are managed by the arbitrator as described in Section 2.2. Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent. To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated. For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function. In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex. For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1). The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds. In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds. Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space. Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound. We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp. From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound. Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task. On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources. Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4. CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent. On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search. In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp. Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments. The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources. An interesting research avenue would be to experiment our bounds with other heuristic search algorithms. For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms. In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds. Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence. Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5. REFERENCES [1] A. Barto, S. Bradtke, and S. Singh. Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib. An iterative algorithm for solving constrained decentralized markov decision processes. In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner. Faster heuristic search algorithms for planning with uncertainty and full feedback. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner. Labeled lrtdp approach: Improving the convergence of real-time dynamic programming. In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops. Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon. Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees. In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005. ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld. Microeconomics. Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars. Q-decomposition for reinforcement learning agents. In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn. How to dynamically merge markov decision processes. In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998. MIT Press. [11] T. Smith and R. Simmons. Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic. In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang. Modeling and solving a resource allocation problem with soft constraint techniques. Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219",
    "original_translation": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219",
    "original_sentences": [
        "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
        "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
        "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
        "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
        "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
        "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
        "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
        "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
        "General Terms Algorithms, Performance, Experimentation. 1.",
        "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
        "In general, resource allocation problems are known to be NP-Complete [12].",
        "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
        "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
        "In general, the number of states is the combination of all possible specific states of each task and available resources.",
        "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
        "The very high number of states and actions in this type of problem makes it very complex.",
        "There can be many types of resource allocation problems.",
        "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
        "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
        "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
        "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
        "The planning process starts with the initial state s0.",
        "In s0, each agent computes their respective Q-value.",
        "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
        "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
        "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
        "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
        "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
        "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
        "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
        "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
        "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
        "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
        "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
        "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
        "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
        "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
        "The problem is now modelled. 2.",
        "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
        "These two tasks are either in the realized state, or not realized state.",
        "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
        "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
        "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
        "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
        "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
        "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
        "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
        "Indeed, our model may consider consumable and non-consumable resource types.",
        "A consumable resource type is one where the amount of available resource is decreased when it is used.",
        "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
        "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
        "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
        "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
        "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
        "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
        "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
        "Also, S contains a non empty set sg ⊆ S of goal states.",
        "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
        "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
        "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
        "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
        "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
        "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
        "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
        "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
        "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
        "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
        "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
        "The global constraint is defined according to all system trajectories tra ∈ T RA.",
        "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
        "For example, state s is entered, which may transit to s or to s , according to action a.",
        "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
        "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
        "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
        "In other words, the model is Markovian as the history has not to be considered in the state space.",
        "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
        "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
        "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
        "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
        "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
        "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
        "These agents desire to maximize their specific reward by consuming the right amount of oil.",
        "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
        "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
        "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
        "This can happen depending on the type of missiles, their range, and so on.",
        "In this case, two agents can plan for both set of tasks to determine the policy.",
        "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
        "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
        "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
        "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
        "That is, R = i∈Ag Ri.",
        "It requires each agent to compute a value, from its perspective, for every action.",
        "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
        "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
        "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
        "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
        "Russell and Zimdars called this approach local Sarsa.",
        "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
        "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
        "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
        "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
        "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
        "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
        "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
        "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
        "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
        "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
        "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
        "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
        "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
        "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
        "Then, the arbitrator functionalities are in Lines 8 to 20.",
        "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
        "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
        "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
        "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
        "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
        "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
        "Also, |A| is combinatorial with the number of resource types.",
        "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
        "In these circumstances, |Ai| |A|.",
        "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
        "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
        "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
        "However, when the resources are available to all agents, no Q-decomposition is possible.",
        "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
        "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
        "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
        "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
        "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
        "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
        "This procedure tries to label as solved each traversed state in the current trajectory.",
        "When the initial state is labelled as solved, the algorithm has converged.",
        "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
        "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
        "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
        "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
        "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
        "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
        "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
        "Only the values of the state transitions change.",
        "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
        "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
        "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
        "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
        "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
        "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
        "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
        "This strategy has been proven as efficient [11] [6].",
        "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
        "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
        "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
        "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
        "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
        "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
        "The next sections describe two separate methods to define hL(s) and hU (s).",
        "First of all, the method of Singh and Cohn [10] is briefly described.",
        "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
        "Their approach is pretty straightforward.",
        "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
        "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
        "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
        "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
        "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
        "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
        "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
        "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
        "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
        "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
        "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
        "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
        "Theorem 2.1.",
        "The upper bound defined by Equation 4 is admissible.",
        "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
        "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
        "Indeed, each task may use all consumable resources for its own purpose.",
        "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
        "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
        "A standard Bellman backup has a complexity of O(|A| × |S|).",
        "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
        "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
        "When each task has its own set of resources, each task may be solved independently.",
        "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
        "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
        "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
        "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
        "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
        "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
        "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
        "The Line 5 initializes the valueta variable.",
        "This variable is the estimated value of each task ta ∈ T a.",
        "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
        "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
        "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
        "The resources are allocated in the order of its specialization.",
        "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
        "Allocating the resources in this order improves the quality of the resulting lower bound.",
        "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
        "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
        "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
        "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
        "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
        "R(sgta ) is the reward to realize task ta.",
        "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
        "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
        "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
        "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
        "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
        "The first part of the equation to compute valueta represents the expected residual value for task ta.",
        "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
        "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
        "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
        "All resource types are allocated in this manner until Res is empty.",
        "All consumable and non-consumable resource types are allocated to each task.",
        "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
        "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
        "In particular, the SinghL bound may The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
        "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
        "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
        "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
        "Theorem 2.2.",
        "The lower bound of Equation 6 is admissible.",
        "Proof: Lowta(sta) is computed with the resource being shared.",
        "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
        "Indeed, as the resources are shared, the tasks cannot overuse them.",
        "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
        "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
        "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
        "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
        "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
        "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
        "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
        "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
        "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
        "There are also local and global resource constraints on the amount that may be used.",
        "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
        "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
        "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
        "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
        "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
        "For this problem a standard lrtdp approach has been implemented.",
        "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
        "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
        "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
        "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
        "The approaches described in this paper are compared in Figures 1 and 2.",
        "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
        "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
        "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
        "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
        "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
        "When the number of tasks is odd, one more task was assigned to T ai .",
        "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
        "These constraints are managed by the arbitrator as described in Section 2.2.",
        "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
        "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
        "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
        "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
        "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
        "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
        "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
        "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
        "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
        "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
        "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
        "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
        "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
        "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
        "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
        "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
        "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
        "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
        "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
        "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
        "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
        "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
        "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
        "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
        "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
        "Learning to act using real-time dynamic programming.",
        "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
        "An iterative algorithm for solving constrained decentralized markov decision processes.",
        "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
        "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
        "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
        "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
        "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
        "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
        "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
        "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
        "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
        "Microeconomics.",
        "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
        "On-line Q-learning using connectionist systems.",
        "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
        "Q-decomposition for reinforcement learning agents.",
        "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
        "How to dynamically merge markov decision processes.",
        "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
        "MIT Press. [11] T. Smith and R. Simmons.",
        "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
        "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
        "Modeling and solving a resource allocation problem with soft constraint techniques.",
        "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
    ],
    "translated_text_sentences": [
        "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado.",
        "Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente.",
        "La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar.",
        "Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística.",
        "En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp).",
        "El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones.",
        "La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor.",
        "Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida.",
        "Términos generales Algoritmos, Rendimiento, Experimentación. 1.",
        "INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos.",
        "En general, se sabe que los problemas de asignación de recursos son NP-Completos [12].",
        "En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno.",
        "Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado.",
        "En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles.",
        "En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales.",
        "El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo.",
        "Pueden existir muchos tipos de problemas de asignación de recursos.",
        "En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente.",
        "Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente.",
        "Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9].",
        "En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados.",
        "El proceso de planificación comienza con el estado inicial s0.",
        "En s0, cada agente calcula su valor Q respectivo.",
        "Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente.",
        "Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos.",
        "Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q.",
        "Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente.",
        "Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico.",
        "Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas.",
        "Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados.",
        "McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores.",
        "Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos.",
        "Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos.",
        "Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp.",
        "Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn.",
        "Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP.",
        "La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones.",
        "El problema ahora está modelado. 2.",
        "FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}.",
        "Estas dos tareas están en el estado realizado o no realizado.",
        "Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}.",
        "Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas.",
        "En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles.",
        "Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global).",
        "Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea.",
        "Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar.",
        "Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible.",
        "De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles.",
        "Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza.",
        "Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza.",
        "Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas.",
        "Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico.",
        "Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo.",
        "Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación.",
        "Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total.",
        "La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta.",
        "Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo.",
        "Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones).",
        "Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta.",
        "Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta.",
        "La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta.",
        "Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1.",
        "Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s).",
        "En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas.",
        "El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a.",
        "De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a.",
        "Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s).",
        "La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res.",
        "La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA.",
        "Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π.",
        "Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a.",
        "Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s).",
        "La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra.",
        "Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma.",
        "En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados.",
        "Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito.",
        "Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas.",
        "La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos.",
        "En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente.",
        "Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente.",
        "Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo.",
        "Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo.",
        "Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera.",
        "Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos).",
        "En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2.",
        "Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente.",
        "En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política.",
        "Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse.",
        "En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas.",
        "Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo.",
        "La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes.",
        "Es decir, R = i∈Ag Ri.",
        "Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción.",
        "Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje.",
        "El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo.",
        "Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si).",
        "El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q.",
        "Russell y Zimdars llamaron a este enfoque Sarsa local.",
        "De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global.",
        "De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos.",
        "Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo.",
        "Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido.",
        "Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima.",
        "De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1.",
        "En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q.",
        "Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s.",
        "Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q.",
        "En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes.",
        "Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación.",
        "Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona.",
        "Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación.",
        "Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes.",
        "Entonces, las funcionalidades del árbitro están en las líneas 8 a 20.",
        "El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a.",
        "En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s).",
        "La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman.",
        "Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas.",
        "Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i.",
        "Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|.",
        "Además, |A| es combinatorio con el número de tipos de recursos.",
        "Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes.",
        "En estas circunstancias, |Ai| |A|.",
        "En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q.",
        "Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar.",
        "Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado.",
        "Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q.",
        "En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto.",
        "Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza).",
        "Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto.",
        "De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista.",
        "Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4].",
        "La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ).",
        "Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual.",
        "Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido.",
        "En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas.",
        "Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s).",
        "Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto.",
        "Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s).",
        "Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s).",
        "Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada.",
        "Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q.",
        "Solo cambian los valores de las transiciones de estado.",
        "Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque.",
        "De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos.",
        "En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A.",
        "De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que .",
        "Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9.",
        "En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|).",
        "Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada.",
        "Esta estrategia ha demostrado ser eficiente [11] [6].",
        "Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto.",
        "Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp.",
        "Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo.",
        "Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima.",
        "Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada.",
        "Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima.",
        "Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s).",
        "En primer lugar, se describe brevemente el método de Singh y Cohn [10].",
        "Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción.",
        "Su enfoque es bastante directo.",
        "En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP.",
        "Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU.",
        "En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta).",
        "Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL.",
        "La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado.",
        "Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s).",
        "La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada.",
        "Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado.",
        "Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6].",
        "En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp.",
        "Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior.",
        "Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp.",
        "Teorema 2.1.",
        "El límite superior definido por la Ecuación 4 es admisible.",
        "Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a.",
        "Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple.",
        "De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito.",
        "Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos.",
        "Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU.",
        "Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|).",
        "Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado.",
        "Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas.",
        "Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente.",
        "El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano.",
        "La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2].",
        "En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa.",
        "Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica.",
        "El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado.",
        "En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4].",
        "Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles.",
        "La línea 5 inicializa la variable valueta.",
        "Esta variable es el valor estimado de cada tarea ta ∈ T a.",
        "Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a.",
        "Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado.",
        "Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados.",
        "Los recursos se asignan en orden de su especialización.",
        "En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano.",
        "Asignar los recursos en este orden mejora la calidad del límite inferior resultante.",
        "La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a.",
        "Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable.",
        "El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta.",
        "Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo.",
        "En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea.",
        "R(sgta) es la recompensa por realizar la tarea ta.",
        "Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas.",
        "El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal.",
        "Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual.",
        "En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta.",
        "Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta.",
        "La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta.",
        "Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res.",
        "En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res).",
        "Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles.",
        "Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío.",
        "Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea.",
        "Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32.",
        "Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL.",
        "En particular, el límite de SinghL puede ser el Sexto Internacional.",
        "La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas.",
        "Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta.",
        "La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido.",
        "Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos.",
        "Teorema 2.2.",
        "El límite inferior de la Ecuación 6 es admisible.",
        "Prueba: Lowta(sta) se calcula con el recurso que se comparte.",
        "La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos.",
        "De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos.",
        "Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3.",
        "DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos).",
        "Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas.",
        "En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos.",
        "Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo.",
        "Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles.",
        "En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea.",
        "Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado.",
        "La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario.",
        "También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar.",
        "Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico.",
        "Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso.",
        "Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo.",
        "La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible.",
        "El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles.",
        "Para este problema se ha implementado un enfoque estándar de lrtdp.",
        "Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas.",
        "De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta.",
        "Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes.",
        "Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima.",
        "Los enfoques descritos en este documento se comparan en las Figuras 1 y 2.",
        "Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp.",
        "En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp.",
        "Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales.",
        "El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi.",
        "Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible.",
        "Cuando el número de tareas es impar, se asignó una tarea más a T ai.",
        "Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles.",
        "Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2.",
        "La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente.",
        "Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados.",
        "Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos.",
        "En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos.",
        "Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1).",
        "El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados.",
        "En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos.",
        "De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones.",
        "Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU.",
        "También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp.",
        "A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU.",
        "De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea.",
        "Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos.",
        "De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4.",
        "CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente.",
        "Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística.",
        "En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp.",
        "Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos.",
        "La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles.",
        "Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística.",
        "Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes.",
        "En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores.",
        "Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia.",
        "Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5.",
        "REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh.",
        "Aprendiendo a actuar utilizando programación dinámica en tiempo real.",
        "Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib.",
        "Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones.",
        "En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner.",
        "Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa.",
        "En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner.",
        "Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real.",
        "En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles.",
        "Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon.",
        "Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento.",
        "En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005.",
        "ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld.",
        "Microeconomía.",
        "Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan.",
        "Aprendizaje Q en línea utilizando sistemas conexionistas.",
        "Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars.",
        "Descomposición Q para agentes de aprendizaje por refuerzo.",
        "En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn.",
        "Cómo fusionar dinámicamente procesos de decisión de Markov.",
        "En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998.",
        "MIT Press. [11] T. Smith y R. Simmons.",
        "Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística.",
        "En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang.",
        "Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles.",
        "Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002.",
        "El Sexto Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219"
    ],
    "error_count": 0,
    "keys": {
        "resource allocation": {
            "translated_key": "asignación de recursos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to <br>resource allocation</br> Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic <br>resource allocation</br> problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic <br>resource allocation</br> problems.",
                "In general, <br>resource allocation</br> problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of <br>resource allocation</br> problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of <br>resource allocation</br> problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained <br>resource allocation</br> problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained <br>resource allocation</br> problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained <br>resource allocation</br>.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple <br>resource allocation</br> problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 <br>resource allocation</br> as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic <br>resource allocation</br> problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a <br>resource allocation</br> problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the <br>resource allocation</br> to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since <br>resource allocation</br> in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for <br>resource allocation</br> There can be many types of <br>resource allocation</br> problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of <br>resource allocation</br> problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a <br>resource allocation</br> problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our <br>resource allocation</br> problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible <br>resource allocation</br> in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic <br>resource allocation</br> problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly <br>resource allocation</br> problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for <br>resource allocation</br>, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a <br>resource allocation</br> problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "A Q-decomposition and Bounded RTDP Approach to <br>resource allocation</br> Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic <br>resource allocation</br> problems known to be NP-Complete.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic <br>resource allocation</br> problems.",
                "In general, <br>resource allocation</br> problems are known to be NP-Complete [12].",
                "There can be many types of <br>resource allocation</br> problems.",
                "A second type of <br>resource allocation</br> problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent."
            ],
            "translated_annotated_samples": [
                "Este artículo contribuye a resolver de manera efectiva problemas de <br>asignación de recursos</br> estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado.",
                "INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de <br>asignación de recursos</br> estocásticos.",
                "En general, se sabe que los problemas de <br>asignación de recursos</br> son NP-Completos [12].",
                "Pueden existir muchos tipos de problemas de <br>asignación de recursos</br>.",
                "Un segundo tipo de problema de <br>asignación de recursos</br> es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de <br>asignación de recursos</br> estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de <br>asignación de recursos</br> estocásticos. En general, se sabe que los problemas de <br>asignación de recursos</br> son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de <br>asignación de recursos</br>. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de <br>asignación de recursos</br> es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "resource management": {
            "translated_key": "gestión de recursos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex <br>resource management</br> problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "To address this complex <br>resource management</br> problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent."
            ],
            "translated_annotated_samples": [
                "Para abordar este complejo problema de <br>gestión de recursos</br>, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de <br>gestión de recursos</br>, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "q-decomposition": {
            "translated_key": "descomposición Q",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A <br>q-decomposition</br> and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The <br>q-decomposition</br> allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our <br>q-decomposition</br> approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, <br>q-decomposition</br> allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no <br>q-decomposition</br> is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "<br>q-decomposition</br> which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 <br>q-decomposition</br> for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use <br>q-decomposition</br> proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to <br>q-decomposition</br>.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, <br>q-decomposition</br> can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original <br>q-decomposition</br> approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use <br>q-decomposition</br> resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using <br>q-decomposition</br> should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The <br>q-decomposition</br> Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the <br>q-decomposition</br> Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the <br>q-decomposition</br> Bellman backup.",
                "Thus, the <br>q-decomposition</br> Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no <br>q-decomposition</br> is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "<br>q-decomposition</br> permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of <br>q-decomposition</br> LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that <br>q-decomposition</br> seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no <br>q-decomposition</br> is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "<br>q-decomposition</br> for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "A <br>q-decomposition</br> and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "The <br>q-decomposition</br> allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "In our <br>q-decomposition</br> approach, a planning agent manages each task and all agents have to share the limited resources.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, <br>q-decomposition</br> allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no <br>q-decomposition</br> is possible."
            ],
            "translated_annotated_samples": [
                "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de <br>descomposición Q</br> y RTDP acotado.",
                "La <br>descomposición Q</br> permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar.",
                "En nuestro enfoque de <br>descomposición Q</br>, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados.",
                "Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la <br>descomposición Q</br> permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos.",
                "Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna <br>descomposición Q</br>."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de <br>descomposición Q</br> y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La <br>descomposición Q</br> permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de <br>descomposición Q</br>, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la <br>descomposición Q</br> permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna <br>descomposición Q</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "real-time dynamic programming": {
            "translated_key": "Programación Dinámica en Tiempo Real",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded <br>real-time dynamic programming</br> (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance <br>real-time dynamic programming</br> (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using <br>real-time dynamic programming</br>.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of <br>real-time dynamic programming</br>.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded <br>real-time dynamic programming</br>: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused <br>real-time dynamic programming</br> for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "In particular, the bounded <br>real-time dynamic programming</br> (bounded rtdp) is used.",
                "For instance <br>real-time dynamic programming</br> (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Learning to act using <br>real-time dynamic programming</br>.",
                "Labeled lrtdp approach: Improving the convergence of <br>real-time dynamic programming</br>.",
                "Bounded <br>real-time dynamic programming</br>: rtdp with monotone upper bounds and performance guarantees."
            ],
            "translated_annotated_samples": [
                "En particular, se utiliza la <br>Programación Dinámica en Tiempo Real</br> Acotada (bounded rtdp).",
                "Por ejemplo, la <br>Programación Dinámica en Tiempo Real</br> (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico.",
                "Aprendiendo a actuar utilizando <br>programación dinámica en tiempo real</br>.",
                "Enfoque lrtdp etiquetado: Mejorando la convergencia de la <br>programación dinámica en tiempo real</br>.",
                "Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la <br>Programación Dinámica en Tiempo Real</br> Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la <br>Programación Dinámica en Tiempo Real</br> (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando <br>programación dinámica en tiempo real</br>. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la <br>programación dinámica en tiempo real</br>. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "complex stochastic resource allocation problem": {
            "translated_key": "problemas complejos de asignación de recursos estocásticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve <br>complex stochastic resource allocation problem</br>s.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "INTRODUCTION This paper aims to contribute to solve <br>complex stochastic resource allocation problem</br>s."
            ],
            "translated_annotated_samples": [
                "INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver <br>problemas complejos de asignación de recursos estocásticos</br>."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver <br>problemas complejos de asignación de recursos estocásticos</br>. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "planning agent": {
            "translated_key": "agente de planificación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a <br>planning agent</br> manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "In our Q-decomposition approach, a <br>planning agent</br> manages each task and all agents have to share the limited resources."
            ],
            "translated_annotated_samples": [
                "En nuestro enfoque de descomposición Q, un <br>agente de planificación</br> gestiona cada tarea y todos los agentes deben compartir los recursos limitados."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un <br>agente de planificación</br> gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "heuristic search": {
            "translated_key": "búsqueda heurística",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use <br>heuristic search</br>.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with <br>heuristic search</br>, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed <br>heuristic search</br> algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art <br>heuristic search</br> approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type <br>heuristic search</br> with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp <br>heuristic search</br> approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for <br>heuristic search</br>.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other <br>heuristic search</br> algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient <br>heuristic search</br> algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster <br>heuristic search</br> algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A <br>heuristic search</br> algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use <br>heuristic search</br>.",
                "When implemented with <br>heuristic search</br>, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed <br>heuristic search</br> algorithm in a stochastic environments.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art <br>heuristic search</br> approaches in a stochastic environment.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type <br>heuristic search</br> with upper and lower bounds on the value of states.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp <br>heuristic search</br> approach."
            ],
            "translated_annotated_samples": [
                "Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la <br>búsqueda heurística</br>.",
                "Cuando se implementa con <br>búsqueda heurística</br>, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de <br>búsqueda heurística</br> descompuesta óptima en entornos estocásticos.",
                "Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de <br>búsqueda heurística</br> de vanguardia en un entorno estocástico.",
                "Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una <br>búsqueda heurística</br> tipo rtdp con límites superiores e inferiores en el valor de los estados.",
                "Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de <br>búsqueda heurística</br> rtdp."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la <br>búsqueda heurística</br>. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con <br>búsqueda heurística</br>, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de <br>búsqueda heurística</br> descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de <br>búsqueda heurística</br> de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una <br>búsqueda heurística</br> tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de <br>búsqueda heurística</br> rtdp. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "markov decision process": {
            "translated_key": "Proceso de Decisión de Markov",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A <br>markov decision process</br> (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "A <br>markov decision process</br> (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process."
            ],
            "translated_annotated_samples": [
                "Un marco de <br>Proceso de Decisión de Markov</br> (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de <br>Proceso de Decisión de Markov</br> (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reward separated agent": {
            "translated_key": "agentes con recompensas separadas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these <br>reward separated agent</br>s and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "The Q-decomposition allows to coordinate these <br>reward separated agent</br>s and thus permits to reduce the set of states and actions to consider."
            ],
            "translated_annotated_samples": [
                "La descomposición Q permite coordinar a estos <br>agentes con recompensas separadas</br> y, por lo tanto, reduce el conjunto de estados y acciones a considerar."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos <br>agentes con recompensas separadas</br> y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "stochastic environment": {
            "translated_key": "entorno estocástico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a <br>stochastic environment</br>.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a <br>stochastic environment</br> is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The marginal revenue bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a <br>stochastic environment</br>.",
                "Since resource allocation in a <br>stochastic environment</br> is NP-Complete, heuristics should be employed."
            ],
            "translated_annotated_samples": [
                "Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un <br>entorno estocástico</br>.",
                "Dado que la asignación de recursos en un <br>entorno estocástico</br> es NP-Completo, se deben emplear heurísticas."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un <br>entorno estocástico</br>. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un <br>entorno estocástico</br> es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el límite de ingreso marginal propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "marginal revenue bound": {
            "translated_key": "límite de ingreso marginal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our marginal revenue bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it involves.",
                "The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the marginal revenue is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The marginal revenue lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The <br>marginal revenue bound</br> proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "Furthermore, The <br>marginal revenue bound</br> proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments."
            ],
            "translated_annotated_samples": [
                "Además, el <br>límite de ingreso marginal</br> propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingreso marginal [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de ingreso marginal se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el ingreso marginal, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el ingreso marginal es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso res para una tarea ta en un estado sta se define de la siguiente manera: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5). El concepto de ingreso marginal de un recurso se utiliza en el Algoritmo 4 para asignar los recursos de antemano entre las tareas, lo que permite definir el valor del límite inferior de un estado. En la Línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque lrtdp estándar [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede utilizar todos los recursos disponibles. La línea 5 inicializa la variable valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al principio del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa en 0 para todas las ta ∈ T a. Entonces, en la línea 9, se selecciona un tipo de recurso res (consumible o no consumible) para ser asignado. Aquí, un experto en el dominio puede separar todos los recursos disponibles en varios tipos o partes para ser asignados. Los recursos se asignan en orden de su especialización. En otras palabras, cuanto más eficiente sea un recurso en un pequeño grupo de tareas, más se asignará temprano. Asignar los recursos en este orden mejora la calidad del límite inferior resultante. La Línea 12 calcula el ingreso marginal de un recurso consumible res para cada tarea ta ∈ T a. Para un recurso no consumible, dado que el recurso no se considera en el espacio de estados, todos los demás estados alcanzables desde sta consideran que el recurso res sigue siendo utilizable. El enfoque aquí es sumar la diferencia entre el valor real de un estado y el valor Q máximo de este estado si el recurso res no puede ser utilizado para todos los estados en una trayectoria dada por la política de la tarea ta. Esta heurística demostró obtener buenos resultados, pero se pueden probar otras, como la simulación de Monte Carlo. En la línea 21, el ingreso marginal se actualiza en función de los recursos ya asignados a cada tarea. R(sgta) es la recompensa por realizar la tarea ta. Por lo tanto, Vta(sta)−valueta R(sgta ) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea ta, y normalizado por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más alto sea el valor residual de una tarea, mayor será su ingreso marginal. Entonces, se selecciona una tarea ta en la línea 23 con el mayor ingreso marginal, ajustado con el valor residual. En la línea 24, se asigna el tipo de recurso res al grupo de recursos Resta de la tarea ta. Después, la Línea 29 recomAlgorithm 4 El algoritmo de límite inferior de ingresos marginales. 1: Función límite de ingresos(S) 2: devuelve un límite inferior LowT a 3: para todo ta ∈ T a hacer 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: fin para 7: s ← s0 8: repetir 9: res ← Seleccionar un tipo de recurso res ∈ Res 10: para todo ta ∈ T a hacer 11: si res es consumible entonces 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: sino 14: mrta(sta) ← 0 15: repetir 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: hasta que sta sea una meta 19: s ← s0 20: fin si 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: fin para 23: ta ← Tarea ta ∈ T a que maximiza mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: si res es consumible entonces 27: temp ← res 28: fin si 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: hasta que todos los tipos de recursos res ∈ Res estén asignados 31: para todo ta ∈ T a hacer 32: Lowta ←lrtdp(Sta, Resta) 33: fin para 34: devolver LowT a calcula valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea ta. Este término se multiplica por max ata∈A(sta) Qta(ata,sta(res)) Vta(sta), que es la proporción de la eficiencia del tipo de recurso res. En otras palabras, se asigna valueta a valueta + (el valor residual × la proporción de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo el recurso res en el espacio de estado, mientras que para un recurso no consumible, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que Res esté vacío. Todos los tipos de recursos consumibles y no consumibles se asignan a cada tarea. Cuando todos los recursos están asignados, los componentes de límite inferior Lowta de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) Utilizamos el máximo entre el límite SinghL y la suma de los componentes del límite inferior Lowta, por lo tanto, el ingreso marginal ≥ SinghL. En particular, el límite de SinghL puede ser el Sexto Internacional. La conf. conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1217 será mayor cuando quede un pequeño número de tareas. Dado que los componentes de Lowta se calculan considerando s0; por ejemplo, si en un estado posterior solo queda una tarea, el límite de SinghL será mayor que cualquiera de los componentes de Lowta. La principal diferencia de complejidad entre SinghL y revenue-bound está en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que los recursos se comparten, el espacio de estados y el espacio de acciones se reducen considerablemente para cada tarea, lo que reduce en gran medida el cálculo en comparación con las funciones de valor calculadas en la Línea 4, que se realiza tanto para SinghL como para el límite de ingresos. Teorema 2.2. El límite inferior de la Ecuación 6 es admisible. Prueba: Lowta(sta) se calcula con el recurso que se comparte. La suma de las funciones de valor Lowta(sta) para cada ta ∈ T a no viola las restricciones locales y globales de recursos. De hecho, dado que los recursos se comparten, las tareas no pueden abusar de ellos. Por lo tanto, hL(s) es una política realizable y un límite inferior admisible. 3. DISCUSIÓN Y EXPERIMENTOS El ámbito de los experimentos es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos de forma aleatoria para cada enfoque y número posible de tareas. En nuestro problema, |Sta| = 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados; en primer lugar, estados donde las acciones modifican las probabilidades de transición; y luego, están los estados objetivo. Las transiciones de estado son todas estocásticas porque cuando un misil se encuentra en un estado dado, siempre puede transitar a muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre el 45% y el 65% dependiendo del estado de la tarea. Cuando un misil no es contrarrestado, transita a otro estado, que puede ser preferido o no al estado actual, donde el estado más preferido para una tarea es cuando es contrarrestado. La efectividad de cada recurso se modifica aleatoriamente en un ±15% al inicio de un escenario. También existen restricciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las restricciones locales, como máximo se puede asignar 1 recurso de cada tipo para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a restricciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos consumibles, la cantidad total de recurso consumible disponible está entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al inicio de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado en 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumibles. Para este problema se ha implementado un enfoque estándar de lrtdp. Se ha utilizado una heurística simple donde el valor de un estado no visitado se asigna como el valor de un estado objetivo de manera que se logren todas las tareas. De esta manera, se asegura que el valor de cada estado no visitado sobreestime su valor real, ya que el valor de lograr una tarea ta es el más alto que el planificador puede obtener para ta. Dado que esta heurística es bastante directa, las ventajas de utilizar heurísticas mejores son más evidentes. Sin embargo, aunque el enfoque lrtdp utilice una heurística simple, todavía una gran parte del espacio de estados no es visitada al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos aquí estos enfoques: • Qdec-lrtdp: Las copias de seguridad se calculan utilizando la función de copia de seguridad Qdec-backup (Algoritmo 1), pero en un contexto lrtdp. En particular, las actualizaciones realizadas en la función checkSolved también se realizan utilizando la función Qdecbackup. • lrtdp-up: El límite superior de maxU se utiliza para lrtdp. • Singh-rtdp: Los límites SinghL y SinghU se utilizan para bounded-rtdp. • mr-rtdp: Los límites de ingresos y maxU se utilizan para bounded-rtdp. Para implementar Qdec-lrtdp, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas T ai, gestionado por el agente i, puede ser completado con el conjunto de recursos Resi, mientras que el segundo conjunto de tareas T ai, gestionado por el agente Agi, puede ser completado con el conjunto de recursos Resi. Resi tenía un tipo de recurso consumible y un tipo de recurso no consumible, mientras que Resi tenía dos tipos de recursos consumibles y un tipo de recurso no consumible. Cuando el número de tareas es impar, se asignó una tarea más a T ai. Hay restricciones entre el grupo de recursos Resi y Resi, de modo que algunas asignaciones no son posibles. Estas restricciones son gestionadas por el árbitro según se describe en la Sección 2.2. La descomposición Q permite disminuir significativamente el tiempo de planificación en nuestra configuración de problemas, y parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben ser separados en varios tipos o partes para ser asignados. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización, tal como dijimos al describir la función de límite de ingresos. En términos de experimentos, se observa que los enfoques lrtdp, lrtdp-up y para la asignación de recursos, que no podan el espacio de acciones, son mucho más complejos. Por ejemplo, se tardó un promedio de 1512 segundos en planificar el enfoque lrtdp-up con seis tareas (ver Figura 1). El enfoque Singh-rtdp redujo el tiempo de planificación al utilizar un límite inferior y superior para podar el espacio de acciones. mr-rtdp redujo aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-rtdp necesitó 231 segundos en promedio para resolver un problema con seis tareas y mr-rtdp requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con lrtdp-up, lo que demuestra la eficiencia de utilizar límites para podar el espacio de acciones. Además, implementamos mr-rtdp con el límite de SinghU, y esto fue ligeramente menos eficiente que con el límite de maxU. También implementamos mr-rtdp con el límite de SinghL, y esto fue ligeramente más eficiente que Singh-rtdp. A partir de estos resultados, concluimos que la diferencia de eficiencia entre mr-rtdp y Singh-rtdp se debe más al límite inferior de ingresos marginales que al límite superior de maxU. De hecho, cuando el número de tareas a ejecutar es alto, los límites inferiores por Singh-rtdp toman los valores de una sola tarea. Por otro lado, el límite inferior de mr-rtdp tiene en cuenta el valor de los 1218 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Tiempo en segundos Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-descomposición LRTDP y LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Tiempo en segundos Número de tareas LRTDP LRTDP-up Singh-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP comparado con SINGH-RTDP. tarea mediante el uso de una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera para todas las tareas, y nuestro límite inferior lo hace heurísticamente. 4. CONCLUSIÓN Los experimentos han demostrado que la descomposición Q parece ser un enfoque muy eficiente cuando un grupo de agentes debe asignar recursos que solo están disponibles para ellos mismos, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando los recursos disponibles se comparten, no es posible realizar una descomposición Q y proponemos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de bounded-rtdp, que poda el espacio de acciones, es significativamente menor que el de lrtdp. Además, el <br>límite de ingreso marginal</br> propuesto en este documento se compara favorablemente con el enfoque de Singh y Cohn [10]. La limitación de boundedrtdp con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posea recursos limitados consumibles y/o no consumibles. Una interesante línea de investigación sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, frtdp [11] y brtdp [6] son ambos algoritmos de búsqueda heurística eficientes. En particular, ambos enfoques propusieron actualizaciones eficientes de la trayectoria del estado, cuando se proporcionaron límites superiores e inferiores. Nuestros límites ajustados permitirían, tanto para frtdp como para brtdp, reducir el número de respaldos a realizar antes de la convergencia. Finalmente, la función bounded-rtdp poda el espacio de acciones cuando QU (a, s) ≤ L(s), como sugirieron Singh y Cohn [10]. frtdp y brtdp también podrían podar el espacio de acciones en estas circunstancias para reducir aún más su tiempo de planificación. 5. REFERENCIAS [1] A. Barto, S. Bradtke y S. Singh. Aprendiendo a actuar utilizando programación dinámica en tiempo real. Inteligencia Artificial, 72(1):81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión de Markov descentralizados con restricciones. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurística más rápidos para la planificación con incertidumbre y retroalimentación completa. En Actas de la Decimoctava Conferencia Internacional Conjunta de Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque lrtdp etiquetado: Mejorando la convergencia de la programación dinámica en tiempo real. En Actas de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automáticas (ICAPS-03), páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein. lao: Un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real acotada: rtdp con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Automático, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press. [7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Aprendizaje Q en línea utilizando sistemas conexionistas. Informe técnico CUED/FINFENG/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Descomposición Q para agentes de aprendizaje por refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press. [11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para procesos de decisión de Markov: sacando más provecho de una heurística. En Actas de la Vigésimo-Primera Conferencia Nacional de Inteligencia Artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricciones flexibles. Informe técnico: wucs-2002-13, Universidad de Washington, San Luis, Misuri, 2002. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1219 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "marginal revenue": {
            "translated_key": "ingreso marginal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Q-decomposition and Bounded RTDP Approach to Resource Allocation Pierrick Plamondon and Brahim Chaib-draa Computer Science & Software Engineering Dept Laval University Québec, Canada {plamon, chaib}@damas.ift.ulaval.ca Abder Rezak Benaskeur Decision Support Systems Section Defence R&D Canada - Valcartier Québec, Canada abderrezak.benaskeur@drdc-rddc.gc.ca ABSTRACT This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete.",
                "To address this complex resource management problem, a Qdecomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider.",
                "On the other hand, when the resources are available to all agents, no Qdecomposition is possible and we use heuristic search.",
                "In particular, the bounded Real-time Dynamic Programming (bounded rtdp) is used.",
                "Bounded rtdp concentrates the planning on significant states only and prunes the action space.",
                "The pruning is accomplished by proposing tight upper and lower bounds on the value function.",
                "Categories and Subject Descriptors I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods, and Search; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence.",
                "General Terms Algorithms, Performance, Experimentation. 1.",
                "INTRODUCTION This paper aims to contribute to solve complex stochastic resource allocation problems.",
                "In general, resource allocation problems are known to be NP-Complete [12].",
                "In such problems, a scheduling process suggests the action (i.e. resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the environment.",
                "When executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.",
                "In general, the number of states is the combination of all possible specific states of each task and available resources.",
                "In this case, the number of possible actions in a state is the combination of each individual possible resource assignment to the tasks.",
                "The very high number of states and actions in this type of problem makes it very complex.",
                "There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To solve this problem efficiently, we adapt Qdecomposition proposed by Russell and Zimdars [9].",
                "In our Q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.",
                "The planning process starts with the initial state s0.",
                "In s0, each agent computes their respective Q-value.",
                "Then, the planning agents are coordinated through an arbitrator to find the highest global Q-value by adding the respective possible Q-values of each agents.",
                "When implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, Q-decomposition allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.",
                "On the other hand, when the resources are available to all agents, no Q-decomposition is possible.",
                "A common way of addressing this large stochastic problem is by using Markov Decision Processes (mdps), and in particular real-time search where many algorithms have been developed recently.",
                "For instance Real-Time Dynamic Programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.",
                "Because of its anytime quality, an interesting approach is rtdp introduced by Barto et al. [1] which updates states in trajectories from an initial state s0 to a goal state sg. rtdp is used in this paper to solve efficiently a constrained resource allocation problem. rtdp is much more effective if the action space can be pruned of sub-optimal actions.",
                "To do this, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 IFAAMAS al. [6], Smith and Simmons [11], and Singh and Cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.",
                "McMahan et al. [6] and Smith and Simmons [11] suggested, in particular, an efficient trajectory of state updates to further speed up the convergence, when given upper and lower bounds.",
                "This efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.",
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of <br>marginal revenue</br> [7] to elaborate tight bounds.",
                "This paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.",
                "Our <br>marginal revenue</br> bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "Also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.",
                "The only condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.",
                "The problem is now modelled. 2.",
                "PROBLEM FORMULATION A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.",
                "These two tasks are either in the realized state, or not realized state.",
                "To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.",
                "A computer has to compute the optimal allocation of these resources to cleaner robots to realize their tasks.",
                "In this problem, a state represents a conjunction of the particular state of each task, and the available resources.",
                "The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).",
                "Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.",
                "For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.",
                "When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining resource are determined with the resource available in s, subtracted from the resources used by action a, if the resource is consumable.",
                "Indeed, our model may consider consumable and non-consumable resource types.",
                "A consumable resource type is one where the amount of available resource is decreased when it is used.",
                "On the other hand, a nonconsumable resource type is one where the amount of available resource is unchanged when it is used.",
                "For example, a brush is a non-consumable resource, while the detergent is a consumable resource. 2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.",
                "A Markov Decision Process (mdp) framework is used to model our stochastic resource allocation problem. mdps have been widely adopted by researchers today to model a stochastic process.",
                "This is due to the fact that mdps provide a well-studied and simple, yet very expressive model of the world.",
                "An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.",
                "Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.",
                "The global constraint only applies for consumable resource types (Resc) and the local constraints always apply to consumable and nonconsumable resource types. • T a is a finite set of tasks with ta ∈ T a to be accomplished. • S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources. sta is the specific state of task ta.",
                "Also, S contains a non empty set sg ⊆ S of goal states.",
                "A goal state is a sink state where an agent stays forever. • A is a finite set of actions (or assignments).",
                "The actions a ∈ A(s) applicable in a state are the combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.",
                "The possible actions are limited by Lres and Gres. • Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s). • W = [wta] is the relative weight (criticality) of each task. • State rewards R = [rs] : ta∈T a rsta ← sta × wta.",
                "The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.",
                "For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases. • A discount (preference) factor γ, which is a real number between 0 and 1.",
                "A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).",
                "In particular, πta(s) is the action (i.e. resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal policy is one that maximizes the expected total reward for accomplishing all tasks.",
                "The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \\ res(a), where res(a) are the consumable resources used by action a.",
                "Indeed, since an action a is a resource assignment, Resc \\ res(a) is the new set of available resources after the execution of action a.",
                "Furthermore, one may compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).",
                "The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.",
                "The global constraint is defined according to all system trajectories tra ∈ T RA.",
                "A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.",
                "For example, state s is entered, which may transit to s or to s , according to action a.",
                "The two possible system trajectories are (s, a), (s ) and (s, a), (s ) .",
                "The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.",
                "Since the available consumable resources are represented in the state space, this condition is verified by itself.",
                "In other words, the model is Markovian as the history has not to be considered in the state space.",
                "Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.",
                "Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.",
                "Q-decomposition which decomposes a planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced. 2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.",
                "Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.",
                "A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "For instance, a group of agents which manages the oil consummated by a country falls in this group.",
                "These agents desire to maximize their specific reward by consuming the right amount of oil.",
                "However, all the agents are penalized when an agent consumes oil because of the pollution it generates.",
                "Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.",
                "This can happen depending on the type of missiles, their range, and so on.",
                "In this case, two agents can plan for both set of tasks to determine the policy.",
                "However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.",
                "IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.",
                "To result these conflicts, we use Q-decomposition proposed by Russell and Zimdars [9] in the context of reinforcement learning.",
                "The primary assumption underlying Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each distinct agent i ∈ Ag, where |Ag| is the number of agents.",
                "That is, R = i∈Ag Ri.",
                "It requires each agent to compute a value, from its perspective, for every action.",
                "To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning iteration.",
                "The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.",
                "That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).",
                "The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.",
                "Russell and Zimdars called this approach local Sarsa.",
                "In this way, an ideal compromise can be found for the agents to reach a global optimum.",
                "Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \\ resi(ai) for a resource allocation problem.",
                "Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.",
                "Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.",
                "For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.",
                "Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.",
                "In Line 5 of the Qdec-backup function, each agent managing a task computes its respective Q-value.",
                "Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .",
                "An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.",
                "In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.",
                "Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.",
                "An agent considers as a possible state transition only the possible states of the set of tasks it manages.",
                "Since the number of states is exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.",
                "Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a standard action space, which is the combination of all possible resource allocation in a state for all agents.",
                "Then, the arbitrator functionalities are in Lines 8 to 20.",
                "The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11, considering the global action a.",
                "In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).",
                "Line 14 simply allocate the current value with respect to the highest global Q-value, as in a standard Bellman backup.",
                "Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup. 1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \\ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint actions.",
                "On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.",
                "Since |SAg| is combinatorial with the number of tasks, so |Si| |S|.",
                "Also, |A| is combinatorial with the number of resource types.",
                "If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all available resource types for all agents.",
                "In these circumstances, |Ai| |A|.",
                "In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.",
                "Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.",
                "Furthermore, the communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.",
                "However, when the resources are available to all agents, no Q-decomposition is possible.",
                "In this case, Bounded RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states. bounded-rtdp is now introduced. 2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1]. lrtdp is a simple dynamic programming algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.",
                "Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited. h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).",
                "For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.",
                "Indeed, since the problem is stochastic, the optimal value is lower than for the deterministic version.",
                "It has been proven that lrtdp, given an admissible initial heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].",
                "The convergence is accomplished by means of a labeling procedure called checkSolved(s, ).",
                "This procedure tries to label as solved each traversed state in the current trajectory.",
                "When the initial state is labelled as solved, the algorithm has converged.",
                "In this section, a bounded version of rtdp (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.",
                "This pruning enables to speed up the convergence of lrtdp. bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).",
                "Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.",
                "On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).",
                "For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).",
                "The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.",
                "Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.",
                "Only the values of the state transitions change.",
                "Thus, having to compute two Q-values instead of one does not augment the complexity of the approach.",
                "In fact, Smith and Simmons [11] state that the additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.",
                "In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly reducing the action space A.",
                "Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the difference between the lower and upper bounds is lower than .",
                "When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.",
                "In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).",
                "Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.",
                "This strategy has been proven as efficient [11] [6].",
                "As discussed by Singh and Cohn [10], this type of algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.",
                "Since the upper bound for state s is known, it may be estimated The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.",
                "Adapted from [4] and [10]. 1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup. 1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \\ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \\ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.",
                "If the difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of ones choice, which outputs a fast and near optimal solution.",
                "Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and upper bounds which exist at the time of its arrival.",
                "Singh and Cohn [10] proved that an algorithm that uses admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.",
                "The next sections describe two separate methods to define hL(s) and hU (s).",
                "First of all, the method of Singh and Cohn [10] is briefly described.",
                "Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space. 2.4 Singh and Cohns Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.",
                "Their approach is pretty straightforward.",
                "First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.",
                "Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.",
                "In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).",
                "For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.",
                "The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the optimal value of each state.",
                "To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).",
                "The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping criteria.",
                "Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter . bounded-rtdp labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.",
                "This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al. brtdp [6].",
                "In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.",
                "The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space. 2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.",
                "To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.",
                "Theorem 2.1.",
                "The upper bound defined by Equation 4 is admissible.",
                "Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible actions a.",
                "However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.",
                "Indeed, each task may use all consumable resources for its own purpose.",
                "Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.",
                "Computing the maxU bound in a state has a complexity of O(|A| × |T a|), and O(|T a|) for SinghU.",
                "A standard Bellman backup has a complexity of O(|A| × |S|).",
                "Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.",
                "Thus, the computation time of the upper bound is negligible. 1216 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.",
                "When each task has its own set of resources, each task may be solved independently.",
                "The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.",
                "The allocation a priori of the resources is made using <br>marginal revenue</br>, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, <br>marginal revenue</br> is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the <br>marginal revenue</br> of a resource is the additional expected value it involves.",
                "The <br>marginal revenue</br> of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of <br>marginal revenue</br> of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.",
                "In Line 4 of the algorithm, a value function is computed for all tasks in the environment using a standard lrtdp [4] approach.",
                "These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.",
                "The Line 5 initializes the valueta variable.",
                "This variable is the estimated value of each task ta ∈ T a.",
                "In the beginning of the algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.",
                "Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.",
                "Here, a domain expert may separate all available resources in many types or parts to be allocated.",
                "The resources are allocated in the order of its specialization.",
                "In other words, the more a resource is efficient on a small group of tasks, the more it is allocated early.",
                "Allocating the resources in this order improves the quality of the resulting lower bound.",
                "The Line 12 computes the <br>marginal revenue</br> of a consumable resource res for each task ta ∈ T a.",
                "For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.",
                "The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the policy of task ta.",
                "This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo simulation.",
                "In Line 21, the <br>marginal revenue</br> is updated in function of the resources already allocated to each task.",
                "R(sgta ) is the reward to realize task ta.",
                "Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.",
                "The <br>marginal revenue</br> is multiplied by this term to indicate that, the more a task has a high residual value, the more its <br>marginal revenue</br> is going to be high.",
                "Then, a task ta is selected in Line 23 with the highest <br>marginal revenue</br>, adjusted with residual value.",
                "In Line 24, the resource type res is allocated to the group of resources Resta of task ta.",
                "Afterwards, Line 29 recomAlgorithm 4 The <br>marginal revenue</br> lower bound algorithm. 1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \\ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) + Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.",
                "The first part of the equation to compute valueta represents the expected residual value for task ta.",
                "This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.",
                "In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).",
                "For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.",
                "All resource types are allocated in this manner until Res is empty.",
                "All consumable and non-consumable resource types are allocated to each task.",
                "When all resources are allocated, the lower bound components Lowta of each task are computed in Line 32.",
                "When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus marginalrevenue ≥ SinghL.",
                "In particular, the SinghL bound may The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.",
                "As the components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.",
                "The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.",
                "However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.",
                "Theorem 2.2.",
                "The lower bound of Equation 6 is admissible.",
                "Proof: Lowta(sta) is computed with the resource being shared.",
                "Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource constraints.",
                "Indeed, as the resources are shared, the tasks cannot overuse them.",
                "Thus, hL(s) is a realizable policy, and an admissible lower bound. 3.",
                "DISCUSSION AND EXPERIMENTS The domain of the experiments is a naval platform which must counter incoming missiles (i.e. tasks) by using its resources (i.e. weapons, movements).",
                "For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.",
                "In our problem, |Sta| = 4, thus each task can be in four distinct states.",
                "There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.",
                "The state transitions are all stochastic because when a missile is in a given state, it may always transit in many possible states.",
                "In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.",
                "When a missile is not countered, it transits to another state, which may be preferred or not to the current state, where the most preferred state for a task is when it is countered.",
                "The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.",
                "There are also local and global resource constraints on the amount that may be used.",
                "For the local constraints, at most 1 resource of each type can be allocated to execute tasks in a specific state.",
                "This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.",
                "Furthermore, for consumable resources, the total amount of available consumable resource is between 1 and 2 for each type.",
                "The global constraint is generated randomly at the start of a scenario for each consumable resource type.",
                "The number of resource type has been fixed to 5, where there are 3 consumable resource types and 2 non-consumable resources types.",
                "For this problem a standard lrtdp approach has been implemented.",
                "A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.",
                "This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.",
                "Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.",
                "Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.",
                "The approaches described in this paper are compared in Figures 1 and 2.",
                "Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.",
                "In particular the updates made in the checkSolved function are also made using the the Qdecbackup function. • lrtdp-up: The upper bound of maxU is used for lrtdp. • Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp. • mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.",
                "To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.",
                "The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .",
                "Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.",
                "When the number of tasks is odd, one more task was assigned to T ai .",
                "There are constraint between the group of resource Resi and Resi such that some assignments are not possible.",
                "These constraints are managed by the arbitrator as described in Section 2.2.",
                "Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.",
                "For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.",
                "In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.",
                "For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).",
                "The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space. mr-rtdp further reduce the planning time by providing very tight initial bounds.",
                "In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.",
                "Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.",
                "Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.",
                "We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.",
                "From these results, we conclude that the difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.",
                "Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.",
                "On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP. 0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP. task by using a heuristic to distribute the resources.",
                "Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 4.",
                "CONCLUSION The experiments have shown that Q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.",
                "On the other hand, when the available resource are shared, no Q-decomposition is possible and we proposed tight bounds for heuristic search.",
                "In this case, the planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.",
                "Furthermore, The <br>marginal revenue</br> bound proposed in this paper compares favorably to the Singh and Cohn [10] approach. boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.",
                "The only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.",
                "An interesting research avenue would be to experiment our bounds with other heuristic search algorithms.",
                "For instance, frtdp [11], and brtdp [6] are both efficient heuristic search algorithms.",
                "In particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.",
                "Our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.",
                "Finally, the bounded-rtdp function prunes the action space when QU (a, s) ≤ L(s), as Singh and Cohn [10] suggested. frtdp and brtdp could also prune the action space in these circumstances to further reduce their planning time. 5.",
                "REFERENCES [1] A. Barto, S. Bradtke, and S. Singh.",
                "Learning to act using real-time dynamic programming.",
                "Artificial Intelligence, 72(1):81-138, 1995. [2] A. Beynier and A. I. Mouaddib.",
                "An iterative algorithm for solving constrained decentralized markov decision processes.",
                "In Proceeding of the Twenty-First National Conference on Artificial Intelligence (AAAI-06), 2006. [3] B. Bonet and H. Geffner.",
                "Faster heuristic search algorithms for planning with uncertainty and full feedback.",
                "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), August 2003. [4] B. Bonet and H. Geffner.",
                "Labeled lrtdp approach: Improving the convergence of real-time dynamic programming.",
                "In Proceeding of the Thirteenth International Conference on Automated Planning & Scheduling (ICAPS-03), pages 12-21, Trento, Italy, 2003. [5] E. A. Hansen and S. Zilberstein. lao : A heuristic search algorithm that finds solutions with loops.",
                "Artificial Intelligence, 129(1-2):35-62, 2001. [6] H. B. McMahan, M. Likhachev, and G. J. Gordon.",
                "Bounded real-time dynamic programming: rtdp with monotone upper bounds and performance guarantees.",
                "In ICML 05: Proceedings of the Twenty-Second International Conference on Machine learning, pages 569-576, New York, NY, USA, 2005.",
                "ACM Press. [7] R. S. Pindyck and D. L. Rubinfeld.",
                "Microeconomics.",
                "Prentice Hall, 2000. [8] G. A. Rummery and M. Niranjan.",
                "On-line Q-learning using connectionist systems.",
                "Technical report CUED/FINFENG/TR 166, Cambridge University Engineering Department, 1994. [9] S. J. Russell and A. Zimdars.",
                "Q-decomposition for reinforcement learning agents.",
                "In ICML, pages 656-663, 2003. [10] S. Singh and D. Cohn.",
                "How to dynamically merge markov decision processes.",
                "In Advances in Neural Information Processing Systems, volume 10, pages 1057-1063, Cambridge, MA, USA, 1998.",
                "MIT Press. [11] T. Smith and R. Simmons.",
                "Focused real-time dynamic programming for mdps: Squeezing more out of a heuristic.",
                "In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI), Boston, USA, 2006. [12] W. Zhang.",
                "Modeling and solving a resource allocation problem with soft constraint techniques.",
                "Technical report: wucs-2002-13, Washington University, Saint-Louis, Missouri, 2002.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1219"
            ],
            "original_annotated_samples": [
                "On the other hand, the approach by Singh and Cohn is suitable to our case, and extended in this paper using, in particular, the concept of <br>marginal revenue</br> [7] to elaborate tight bounds.",
                "Our <br>marginal revenue</br> bounds are compared theoretically and empirically to the bounds proposed by Singh and Cohn.",
                "The allocation a priori of the resources is made using <br>marginal revenue</br>, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].",
                "In brief, <br>marginal revenue</br> is the extra revenue that an additional unit of product will bring to a firm.",
                "Thus, for a stochastic resource allocation problem, the <br>marginal revenue</br> of a resource is the additional expected value it involves."
            ],
            "translated_annotated_samples": [
                "Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de <br>ingreso marginal</br> [7] para elaborar límites precisos.",
                "Nuestros límites de <br>ingreso marginal</br> se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn.",
                "La asignación a priori de los recursos se realiza utilizando el <br>ingreso marginal</br>, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2].",
                "En resumen, el <br>ingreso marginal</br> es el ingreso adicional que una unidad adicional de producto traerá a una empresa.",
                "Por lo tanto, para un problema de asignación de recursos estocásticos, el <br>ingreso marginal</br> de un recurso es el valor esperado adicional que implica."
            ],
            "translated_text": "Este artículo contribuye a resolver de manera efectiva problemas de asignación de recursos estocásticos conocidos por ser NP-Completo mediante un enfoque de descomposición Q y RTDP acotado. Para abordar este complejo problema de gestión de recursos, se propone un enfoque de descomposición Q cuando los recursos ya son compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La descomposición Q permite coordinar a estos agentes con recompensas separadas y, por lo tanto, reduce el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible realizar una descomposición Q y utilizamos la búsqueda heurística. En particular, se utiliza la Programación Dinámica en Tiempo Real Acotada (bounded rtdp). El algoritmo Bounded RTDP concentra la planificación solo en estados significativos y poda el espacio de acciones. La poda se logra proponiendo límites superiores e inferiores ajustados en la función de valor. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida. Términos generales Algoritmos, Rendimiento, Experimentación. 1. INTRODUCCIÓN Este documento tiene como objetivo contribuir a resolver problemas complejos de asignación de recursos estocásticos. En general, se sabe que los problemas de asignación de recursos son NP-Completos [12]. En tales problemas, un proceso de programación sugiere la acción (es decir, los recursos a asignar) a emprender para llevar a cabo ciertas tareas, de acuerdo con el estado perfectamente observable del entorno. Al ejecutar una acción para llevar a cabo un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el siguiente estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada asignación de recursos posible a las tareas individuales. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos la Qdescomposición propuesta por Russell y Zimdars [9]. En nuestro enfoque de descomposición Q, un agente de planificación gestiona cada tarea y todos los agentes deben compartir los recursos limitados. El proceso de planificación comienza con el estado inicial s0. En s0, cada agente calcula su valor Q respectivo. Entonces, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto al sumar los respectivos valores Q posibles de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, la descomposición Q permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de Procesos de Decisión de Markov (MDPs), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la Programación Dinámica en Tiempo Real (RTDP) [1], LRDP [4], HDP [3] y LAO [5] son enfoques de búsqueda heurística de vanguardia en un entorno estocástico. Debido a su calidad en cualquier momento, un enfoque interesante es el RTDP introducido por Barto et al. [1], que actualiza estados en trayectorias desde un estado inicial s0 hasta un estado objetivo sg. En este artículo, se utiliza el RTDP para resolver eficientemente un problema de asignación de recursos restringidos. El RTDP es mucho más efectivo si el espacio de acciones puede ser podado de acciones subóptimas. Para hacer esto, McMahan et al. [6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo rtdp con límites superiores e inferiores en el valor de los estados. McMahan et al. [6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se proporcionan límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado puede combinarse con el enfoque propuesto aquí, ya que este artículo se centra en la definición de límites ajustados y actualización eficiente de estado para un problema de asignación de recursos restringidos. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de <br>ingreso marginal</br> [7] para elaborar límites precisos. Este documento propone nuevos algoritmos para definir límites superiores e inferiores en el contexto de un enfoque de búsqueda heurística rtdp. Nuestros límites de <br>ingreso marginal</br> se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites pueden ser utilizados con cualquier otro algoritmo para resolver un MDP. La única condición para el uso de nuestros límites es que esté en el contexto de asignación de recursos estocástica con restricciones. El problema ahora está modelado. 2. FORMULACIÓN DEL PROBLEMA Un problema simple de asignación de recursos es aquel en el que hay dos tareas a realizar: ta1 = {lavar los platos} y ta2 = {limpiar el suelo}. Estas dos tareas están en el estado realizado o no realizado. Para realizar las tareas, se asumen dos tipos de recursos: res1 = {cepillo} y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos a los robots limpiadores para llevar a cabo sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que se puede utilizar simultáneamente (restricción local) y en total (restricción global). Además, cuanto mayor sea el número de recursos asignados para realizar una tarea, mayor será la expectativa de realizar la tarea. Por esta razón, cuando los estados específicos de las tareas cambian, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción a en el estado s, los estados específicos de las tareas cambian estocásticamente, y los recursos restantes se determinan con los recursos disponibles en s, restados de los recursos utilizados por la acción a, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumibles. Un tipo de recurso consumible es aquel en el que la cantidad de recurso disponible disminuye cuando se utiliza. Por otro lado, un tipo de recurso no consumible es aquel en el que la cantidad de recurso disponible no cambia cuando se utiliza. Por ejemplo, un cepillo es un recurso no consumible, mientras que el detergente es un recurso consumible. 2.1 Asignación de recursos como MDPs En nuestro problema, la función de transición y la función de recompensa son ambas conocidas. Un marco de Proceso de Decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos. Los MDPs han sido ampliamente adoptados por los investigadores hoy en día para modelar un proceso estocástico. Esto se debe a que los MDPS proporcionan un modelo del mundo bien estudiado y simple, pero muy expresivo. Un mdp en el contexto de un problema de asignación de recursos con recursos limitados se define como una tupla Res, T, a, S, A, P, W, R, , donde: • Res = res1, ..., res|Res| es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recurso local Lres en la cantidad que se puede utilizar en un solo paso, y una restricción de recurso global Gres en la cantidad que se puede utilizar en total. La restricción global solo se aplica a los tipos de recursos consumibles (Resc) y las restricciones locales siempre se aplican a los tipos de recursos consumibles y no consumibles. • T a es un conjunto finito de tareas con ta ∈ T a que deben ser realizadas. • S es un conjunto finito de estados con s ∈ S. Un estado s es una tupla T a, res1, ..., res|Resc|, que es la característica de cada tarea no realizada ta ∈ T a en el entorno, y los recursos consumibles disponibles. sta es el estado específico de la tarea ta. Además, S contiene un conjunto no vacío sg ⊆ S de estados objetivo. Un estado objetivo es un estado de absorción donde un agente permanece para siempre. • A es un conjunto finito de acciones (o asignaciones). Las acciones a ∈ A(s) aplicables en un estado son la combinación de todas las asignaciones de recursos que pueden ser ejecutadas, de acuerdo al estado s. En particular, a es simplemente una asignación de recursos a las tareas actuales, y ata es la asignación de recursos a la tarea ta. Las posibles acciones están limitadas por Lres y Gres. • Probabilidades de transición Pa(s |s) para s ∈ S y a ∈ A(s). • W = [wta] es el peso relativo (criticidad) de cada tarea. • Recompensas del estado R = [rs] : ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea rsta es el producto de un número real sta por el factor de peso wta. Para nuestro problema, se otorga una recompensa de 1 × wta cuando el estado de una tarea (sta) se encuentra en un estado logrado, y 0 en todos los demás casos. • Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política π que mapea estados s en acciones a ∈ A(s). En particular, πta(s) es la acción (es decir, los recursos a asignar) que se debe ejecutar en la tarea ta, considerando el estado global s. En este caso, una política óptima es aquella que maximiza la recompensa total esperada por completar todas las tareas. El valor óptimo de un estado, V (s), se da por: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) donde los recursos consumibles restantes en el estado s son Resc \\ res(a), donde res(a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción a es una asignación de recursos, Resc \\ res(a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, se pueden calcular los valores Q Q(a, s) de cada par de estado-acción utilizando el The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1213 la siguiente ecuación: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) donde el valor óptimo de un estado es V (s) = max a∈A(s) Q(a, s). La política está sujeta a las restricciones de recursos locales res(π(s)) ≤ Lres para todo s ∈ S, y para todo res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T RA. Una trayectoria del sistema tra es una secuencia posible de pares estado-acción, hasta que se alcanza un estado objetivo bajo la política óptima π. Por ejemplo, se introduce el estado s, que puede transitar a s o a s, según la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción de recursos globales es res(tra) ≤ Gres∀ tra ∈ T RA , y ∀ res ∈ Resc donde res(tra) es una función que devuelve los recursos utilizados por la trayectoria tra. Dado que los recursos consumibles disponibles están representados en el espacio de estado, esta condición se verifica por sí misma. En otras palabras, el modelo es markoviano ya que la historia no tiene que ser considerada en el espacio de estados. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un mdp de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es NP-Completo, se deben emplear heurísticas. La Q-descomposición, que descompone un problema de planificación en muchos agentes para reducir la complejidad computacional asociada a los espacios de estado y/o acción, se introduce ahora. 2.2 Q-descomposición para la asignación de recursos. Pueden existir muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya están compartidos entre los agentes y las acciones realizadas por un agente no influyen en el estado de otro agente, la política óptima global se puede calcular planificando de forma separada para cada agente. Un segundo tipo de problema de asignación de recursos es cuando los recursos ya están compartidos entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que gestiona el consumo de petróleo de un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad adecuada de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar misiles entrantes (es decir, tareas) utilizando sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se clasifiquen en dos tipos: aquellos que requieren un conjunto de recursos Res1 y aquellos que requieren un conjunto de recursos Res2. Esto puede ocurrir dependiendo del tipo de misiles, su alcance, y así sucesivamente. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, existe interacción entre el recurso Res1 y Res2, de modo que ciertas combinaciones de recursos no pueden asignarse. En particular, si un agente i asigna recursos Resi al primer conjunto de tareas Tai, y el agente i asigna recursos Resi al segundo conjunto de tareas Tai, la política resultante puede incluir acciones que no pueden ejecutarse juntas. Para resolver estos conflictos, utilizamos la descomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje por refuerzo. La suposición principal subyacente en la descomposición Q es que la función de recompensa global R puede descomponerse de forma aditiva en recompensas separadas Ri para cada agente distinto i ∈ Ag, donde |Ag| es el número de agentes. Es decir, R = i∈Ag Ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinarse entre sí, cada agente i informa sus valores de acción Qi(ai, si) para cada estado si ∈ Si a un árbitro en cada iteración de aprendizaje. El árbitro luego elige una acción que maximiza la suma de los valores Q del agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente i considera el valor como su contribución respectiva, o valor Q, al valor Q global máximo. Es decir, Qi(ai, si) es el valor de un estado tal que maximiza maxa∈A(s) i∈Ag Qi(ai, si). El hecho de que los agentes utilicen un valor Q determinado como el valor de un estado es una extensión del algoritmo en política de Sarsa [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque Sarsa local. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente i utiliza la acción ai ejecutada por el árbitro en el estado sucesor si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) donde los recursos consumibles restantes en el estado si son Resci \\ resi(ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge hacia el óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen mediante un espacio de estados y acciones mucho más reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, se puede aplicar la descomposición Q para generar una solución óptima. De hecho, una copia de seguridad de Bellman óptima se puede aplicar en un estado como en el Algoritmo 1. En la línea 5 de la función Qdec-backup, cada agente que gestiona una tarea calcula su respectivo valor Q. Aquí, Qi (ai, s) determina el valor óptimo de Q del agente i en el estado s. Un agente i utiliza como valor de una posible transición de estado s el valor Q para este agente, que determina el valor Q global máximo para el estado s, como en el enfoque original de descomposición Q. En resumen, para cada estado visitado s ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s. Por lo tanto, el espacio de estados es el espacio de estados conjunto de todos los agentes. Parte de la ganancia en complejidad al usar la descomposición Q reside en la parte si∈Si Pai (si|s) de la ecuación. Un agente considera como una posible transición de estado solo los estados posibles del conjunto de tareas que gestiona. Dado que el número de estados es exponencial con el número de tareas, el uso de la descomposición Q debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes solo tiene en cuenta sus recursos disponibles, lo cual es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignaciones de recursos en un estado para todos los agentes. Entonces, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que gestiona cada tarea, como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente i no puede ser ejecutada simultáneamente con una acción de otro agente i, la acción global simplemente se descarta del espacio de acciones A(s). La línea 14 simplemente asigna el valor actual con respecto al valor Q global más alto, como en una copia de seguridad estándar de Bellman. Entonces, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones ai y los valores Q específicos Qi(ai, s) de cada agente 1214 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para la acción a. Algoritmo 1 Copia de seguridad de Bellman de descomposición Q. 1: Función Qdec-backup(s) 2: V (s) ← 0 3: para todo i ∈ Ag hacer 4: para todo ai ∈ Ai(s) hacer 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {donde Qi (ai, s ) = hi(s ) cuando s aún no ha sido visitado, y s tiene recursos consumibles restantes Resci \\ resi(ai) para cada agente i} 6: fin para 7: fin para 8: para todo a ∈ A(s) hacer 9: Q(a, s) ← 0 10: para todo i ∈ Ag hacer 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: fin para 13: si Q(a, s) > V (s) entonces 14: V (s) ← Q(a, s) 15: para todo i ∈ Ag hacer 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: fin para 19: fin si 20: fin para Un respaldo de Bellman estándar tiene una complejidad de O(|A| × |SAg|), donde |SAg| es el número de estados conjuntos para todos los agentes excluyendo los recursos, y |A| es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman de la descomposición Q tiene una complejidad de O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), donde |Si| es el número de estados para un agente i, excluyendo los recursos y |Ai| es el número de acciones para un agente i. Dado que |SAg| es combinatorio con el número de tareas, entonces |Si| |S|. Además, |A| es combinatorio con el número de tipos de recursos. Si los recursos ya están compartidos entre los agentes, el número de tipos de recursos para cada agente suele ser menor que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, |Ai| |A|. En una copia de seguridad estándar de Bellman, |A| se multiplica por |SAg|, lo cual es mucho más complejo que multiplicar |A| por |Ag| con la copia de seguridad de Bellman de descomposición Q. Por lo tanto, la copia de seguridad de Bellman con descomposición Q es mucho menos compleja que una copia de seguridad de Bellman estándar. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema geográficamente separado. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la Programación Dinámica en Tiempo Real Acotada (bounded-rtdp) permite enfocar la búsqueda en estados relevantes y podar el espacio de acciones A utilizando límites inferiores y superiores en el valor de los estados. bounded-rtdp se introduce ahora. 2.3 Bounded-RTDP Bonet y Geffner [4] propusieron lrtdp como una mejora de rtdp [1]. lrtdp es un algoritmo simple de programación dinámica que implica una secuencia de ejecuciones de prueba, cada una comenzando en el estado inicial s0 y terminando en un estado objetivo o resuelto. Cada prueba de lrtdp es el resultado de simular la política π mientras se actualizan los valores V(s) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados s que son visitados. h(s) es una heurística que define un valor inicial para el estado s. Esta heurística tiene que ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V(s) cuando la función objetivo se maximiza (o minimiza). Por ejemplo, una heurística admisible para un problema de camino más corto estocástico es la solución de un problema determinista de camino más corto. De hecho, dado que el problema es estocástico, el valor óptimo es menor que en la versión determinista. Se ha demostrado que lrtdp, dado un heurístico inicial admisible sobre el valor de los estados, no puede quedar atrapado en bucles y eventualmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado checkSolved(s, ). Este procedimiento intenta etiquetar como resuelto cada estado recorrido en la trayectoria actual. Cuando el estado inicial se etiqueta como resuelto, el algoritmo ha convergido. En esta sección, se presenta una versión acotada de rtdp (boundedrtdp) en el Algoritmo 2 para podar el espacio de acciones de acciones subóptimas. Este podado permite acelerar la convergencia de lrtdp. bounded-rtdp es similar a rtdp excepto que hay dos heurísticas iniciales distintas para los estados no visitados s ∈ S; hL(s) y hU(s). Además, el procedimiento checkSolved(s, ) se puede omitir porque los límites pueden proporcionar la etiqueta de un estado como resuelto. Por un lado, hL(s) define un límite inferior en el valor de s tal que el valor óptimo de s es mayor que hL(s). Por su parte, hU(s) define un límite superior en el valor de s tal que el valor óptimo de s es menor que hU(s). Los valores de los límites se calculan en las líneas 3 y 4 de la función de copia de seguridad limitada. Calcular estos dos valores Q se realiza simultáneamente ya que las transiciones de estado son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que es también lo que obtuvimos. En particular, L(s) es el límite inferior del estado s, mientras que U(s) es el límite superior del estado s. De manera similar, QL(a, s) es el valor Q del límite inferior de la acción a en el estado s, mientras que QU(a, s) es el valor Q del límite superior de la acción a en el estado s. El uso de estos dos límites permite reducir significativamente el espacio de acciones A. De hecho, en las líneas 5 y 6 de la función de copia de seguridad acotada, si QU (a, s) ≤ L(s) entonces la acción a puede ser eliminada del espacio de acciones de s. En la línea 13 de esta función, un estado puede ser etiquetado como resuelto si la diferencia entre los límites inferior y superior es menor que . Cuando la ejecución regresa a la función bounded-rtdp, el siguiente estado en la Línea 10 tiene un número fijo de recursos consumibles disponibles Resc, determinado en la Línea 9. En resumen, pickNextState(res) selecciona un estado s no resuelto alcanzable bajo la política actual que tiene el mayor error de Bellman (|U(s) − L(s)|). Finalmente, en las líneas 12 a 15, se realiza una copia de seguridad de manera retroactiva en todos los estados visitados de una trayectoria, una vez que esta trayectoria ha sido completada. Esta estrategia ha demostrado ser eficiente [11] [6]. Como se discute por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado s antes de que el algoritmo haya convergido (mientras permanecen múltiples acciones competitivas), se elige la acción con el límite inferior más alto. Dado que se conoce el límite superior para el estado s, se puede estimar The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1215 Algoritmo 2 El algoritmo bounded-rtdp. Adaptado de [4] y [10]. 1: Función bounded-rtdp(S) 2: devuelve una función de valor V 3: repetir 4: s ← s0 5: visitado ← nulo 6: repetir 7: visitado.push(s) 8: bounded-backup(s) 9: Resc ← Resc \\ {π(s)} 10: s ← s.pickNextState(Resc) 11: hasta que s sea una meta 12: mientras visitado = nulo hacer 13: s ← visitado.pop() 14: bounded-backup(s) 15: fin mientras 16: hasta que s0 esté resuelto o |A(s)| = 1 ∀ s ∈ S alcanzable desde s0 17: devolver V Algoritmo 3 La copia de seguridad de Bellman acotada. 1: Función bounded-backup(s) 2: para todo a ∈ A(s) hacer 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {donde L(s ) ← hL(s ) y U(s ) ← hU (s ) cuando s aún no ha sido visitado y s tiene Resc \\ res(a) recursos consumibles restantes} 5: si QU (a, s) ≤ L(s) entonces 6: A(s) ← A(s) \\ res(a) 7: fin si 8: fin para 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: si |U(s) − L(s)| < entonces 13: s ← resuelto 14: fin si qué tan lejos está el límite inferior de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, se puede optar por utilizar otro algoritmo voraz de elección propia, que genere una solución rápida y cercana a la óptima. Además, si una nueva tarea llega dinámicamente al entorno, puede ser acomodada redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acciones está garantizado de converger hacia una solución óptima. Las siguientes secciones describen dos métodos separados para definir hL(s) y hU(s). En primer lugar, se describe brevemente el método de Singh y Cohn [10]. Entonces, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción. 2.4 Límites de Singh y Cohn Singh y Cohn [10] definieron límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante directo. En primer lugar, se calcula una función de valor para todas las tareas a realizar, utilizando un enfoque estándar de RTDP. Entonces, utilizando estas funciones de valor de tarea, se pueden definir un límite inferior hL y un límite superior hU. En particular, hL(s) = max ta∈T a Vta(sta), y hU (s) = ta∈T a Vta(sta). Para mayor claridad, el límite superior propuesto por Singh y Cohn se denomina SinghU, y el límite inferior se denomina SinghL. La admisibilidad de estos límites ha sido demostrada por Singh y Cohn, de tal manera que, el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a bounded-rtdp, que utiliza los límites para inicializar L(s) y U(s). La única diferencia entre bounded-rtdp y la versión rtdp de Singh y Cohn está en los criterios de parada. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia. bounded-rtdp etiqueta los estados para los cuales |U(s) − L(s)| < a como resueltos y la convergencia se alcanza cuando s0 está resuelto o cuando solo queda una acción competitiva para cada estado. Este criterio de parada es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al. brtdp [6]. En este documento, los límites definidos por Singh y Cohn e implementados utilizando bounded-rtdp definen el enfoque Singh-rtdp. Las siguientes secciones proponen ajustar los límites de Singh-rtdp para permitir una poda más efectiva del espacio de acciones. 2.5 Reducir el Límite Superior SinghU incluye acciones que pueden no ser posibles de ejecutar debido a restricciones de recursos, lo que sobreestima el límite superior. Para considerar solo las acciones posibles, se introduce nuestro límite superior, llamado maxU: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) donde Qta(ata, sta) es el valor Q de la tarea ta para el estado sta, y la acción ata se calcula utilizando un enfoque estándar de lrtdp. Teorema 2.1. El límite superior definido por la Ecuación 4 es admisible. Prueba: Las restricciones locales de recursos se cumplen porque el límite superior se calcula utilizando todas las posibles acciones globales a. Sin embargo, hU (s) sigue sobreestimando V (s) porque la restricción de recursos global no se cumple. De hecho, cada tarea puede utilizar todos los recursos consumibles para su propio propósito. Realizar esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas globalmente con los recursos limitados compartidos. Calcular el límite maxU en un estado tiene una complejidad de O(|A| × |T a|), y O(|T a|) para SinghU. Una copia de seguridad estándar de Bellman tiene una complejidad de O(|A| × |S|). Dado que |A|×|T a| |A|×|S|, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez por cada estado visitado, es mucho menor que el tiempo de cálculo requerido para realizar una copia de seguridad estándar de Bellman para un estado, que generalmente se realiza muchas veces por cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante. 1216 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 2.6 Aumentando el Límite Inferior La idea para aumentar SinghL es asignar los recursos de antemano entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hL(s) = ta∈T a Lowta(sta), donde Lowta(sta) es una función de valor para cada tarea ta ∈ T a, de modo que los recursos han sido asignados de antemano. La asignación a priori de los recursos se realiza utilizando el <br>ingreso marginal</br>, que es un concepto muy utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un mdp descentralizado [2]. En resumen, el <br>ingreso marginal</br> es el ingreso adicional que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el <br>ingreso marginal</br> de un recurso es el valor esperado adicional que implica. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}