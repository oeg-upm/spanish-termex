{
    "id": "I-61",
    "original_text": "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars. Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management. An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix. Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion. Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation). Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1. INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today. On a typical day, more than 40,000 commercial flights operate within the US airspace [14]. In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours. As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions. In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays. The total cost of these delays was estimated to exceed three billion dollars by industry [7]. Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes. The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths. Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem. There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem. An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan. Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system. As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers). In this paper, we focus on agent based system that can be implemented readily. In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D. Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 . In this approach, the agents actions are to set the separation that approaching aircraft are required to keep. This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow. Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15]. In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system. In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search). The first explored reward consisted of the system reward. The second reward was a personalized agent reward based on collectives [3, 17, 18]. The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation. All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions. Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12]. Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11]. The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET. In Section 2, we describe the air traffic flow problem and the simulation tool, FACET. In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures. In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance. Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2. AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem. Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management). In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1. Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2. Regional flow (20 minutes to 2 hours); 3. National flow (1-8 hours); and 4. Dynamic airspace configuration (6 hours to 1 year). Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper. Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either. Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours. The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers. The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights). The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns. Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities. Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function. Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4]. It is based on propagating the trajectories of proposed flights forward in time. FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11]. FACET is extensively used by The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11]. FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1). FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes. The user can then observe the effects of these changes to congestion. In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents. The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities. Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay. The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z. More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty. The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4. The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise. Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late. In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness. This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late. In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths. Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA. Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity. Each sector capacity is computed using various metrics which include the number of air traffic controllers available. The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3. AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above. To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent. That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy. However, there are several problems with that approach. First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system. Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow. As an alternative, we assign agents to individual ground locations throughout the airspace called fixes. Each agent is then responsible for any aircraft going through its fix. Fixes offer many advantages as agents: 1. Their number can vary depending on need. The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2. Because fixes are stationary, collecting data and matching behavior to reward is easier. 3. Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4. They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them. Figure 2 shows a schematic of this agent based system. Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents. Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans. Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process). Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix. This is known as setting the Miles in Trail or MIT. When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d). When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix. By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream. Figure 2: Schematic of agent architecture. The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]). For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used. However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards. As a consequence, simple table-based immediate reward reinforcement learning is used. Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15]. At every episode an agent takes an action and then receives a reward evaluating that action. After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate. At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability . In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25. The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents. The first and most direct approach is to let each agent receive the system performance as its reward. However, in many domains such a reward structure leads to slow learning. We will therefore also set up a second set of reward structures based on agent-specific rewards. Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance. In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i. All the components of z that are affected by agent i are replaced with the fixed constant ci 2 . In many situations it is possible to use a ci that is equivalent to taking agent i out of the system. Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance. There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17]. Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost. Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known. Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function. We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors. The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation. This form of G matches our system evaluation in the air traffic domain. When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents. These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf . Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds. To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) .(11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown. However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci. We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) . Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)). These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen. This estimate should improve as the number of samples increases. To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4. SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator. In all experiments we test the performance of five different methods. The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen. The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1. The system reward, G(z), as define in Equation 1. 2. The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3. Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4. Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)). These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation. Agents are responsible for reducing congestion at this single point, while trying to minimize delay. The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion. In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes. These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed. Except where noted, the trade-off between congestion and lateness, α is set to 0.5. In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results. All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see. Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents. This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA. The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations. In both cases, the agent based methods significantly outperform the Monte Carlo method. This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly. Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75. Among the agent based methods, agents using difference rewards perform better than agents using the system reward. Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward. Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor. In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward. This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed. While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward. Note, however, that the benefit of the estimated difference rewards are only present later in learning. Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion. On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country. The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing. Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5. Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5. The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case. Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward. To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents. The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50. Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents. Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents. This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness. This evaluation function The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance. Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α. With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness. To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75. Figure 8 shows that qualitatively the relative performance of the algorithms remain the same. Next, we perform a series of experiments where α ranges from 0.0 to 1.0 . Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution. This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays. All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points. Therefore, unless D is far from being optimal, the two penalties are not independent. Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives. For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes. Those results show that D is significantly superior to the other algorithms. One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms. The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75. Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents. Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET. Except when D is used, the values of k are computed once per episode. However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode. While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box. Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5. All the algorithms except the fully computed D reach 2100 k computations at time step 2100. D however computes k once for the system, and then once for each agent, leading to 21 computations per time step. It therefore reaches 2100 computations at time step 100. We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100). Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost. Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2. This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5. DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year. The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry. Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix. It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable. The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions. We are currently extending this work in three directions. First, we are exploring new methods of estimating agent rewards, to further speed up the simulations. Second we are investigating deployment strategies and looking for modifications that would have larger impact. One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion. Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper. Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6. REFERENCES [1] A. Agogino and K. Tumer. Efficient evaluation functions for multi-rover systems. In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer. Multi agent reward analysis for learning in noisy domains. In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer. Handling communiction restrictions and team formation in congestion games. Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe. FACET: Future ATM concepts evaluation tool. Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria. A geometric optimization approach to aircraft conflict resolution. In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III. Free flight separation assurance using distributed algorithms. In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005. US Department of Transportation website. [8] S. Grabbe and B. Sridhar. Central east pacific flight routing. In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling. A cooperative multi-agent approach to free flight. In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar. Optimal strategies for free flight air traffic conflict resolution. Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination. FACET: Future ATM concepts evaluation tool. Case no. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller. Autonomous agents for air-traffic deconfliction. In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe. Benefits of direct-to in national airspace system. In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji. Aggregate flow model for air-traffic management. Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry. Conflict resolution for air traffic management. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors. Collectives and the Design of Complex Systems. Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer. Optimal payoff functions for members of collectives. Advances in Complex Systems, 4(2/3):265-279, 2001. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349",
    "original_translation": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples. Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción. Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje. En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad . En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25. Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje. El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema. En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2. En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema. De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema. Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17]. Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G. Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida. Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales. El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes. El Sexto Internacional. Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra. Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo. Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes. Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf. Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos. Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos. Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci. Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci). Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)). Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)). RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de optimización del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política. Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1. La recompensa del sistema, G(z), como se define en la Ecuación 1.2. La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3. Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4. Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)). Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso. Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión. Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados. Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes. Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA. Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes. En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria. Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema. Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente. En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular. Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema. Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje. Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando. Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5. Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5. Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual. Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema. Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50. La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes. Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes. Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso. Esta función de evaluación The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α. Con un α alto, la optimización se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión. Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos. El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75. Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes. Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET. Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio. Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100. Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo. Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K.  Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100). Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable. De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2. Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5. DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable. Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones. Actualmente estamos ampliando este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones. Segundo, estamos investigando estrategias de implementación y buscando modificaciones que tendrían un impacto mayor. Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión. Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6. REFERENCIAS [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples rovers. En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación de equipos en juegos de congestión. Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Control de Tráfico Aéreo Trimestral, 9(1), 2001. [5] Karl D. Bilimoria. Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos. En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005. Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar. Enrutamiento de vuelos en el Pacífico central este. En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre. Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Caso número. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la descongestión del tráfico aéreo. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios de vuelo directo en el sistema nacional de espacio aéreo. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje por refuerzo: Una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el Diseño de Sistemas Complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para los miembros de colectivos. Avances en Sistemas Complejos, 4(2/3):265-279, 2001. El Sexto Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349",
    "original_sentences": [
        "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
        "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
        "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
        "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
        "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
        "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
        "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
        "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
        "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
        "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
        "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
        "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
        "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
        "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
        "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
        "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
        "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
        "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
        "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
        "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
        "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
        "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
        "In this paper, we focus on agent based system that can be implemented readily.",
        "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
        "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
        "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
        "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
        "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
        "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
        "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
        "The first explored reward consisted of the system reward.",
        "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
        "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
        "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
        "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
        "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
        "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
        "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
        "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
        "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
        "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
        "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
        "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
        "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
        "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
        "Regional flow (20 minutes to 2 hours); 3.",
        "National flow (1-8 hours); and 4.",
        "Dynamic airspace configuration (6 hours to 1 year).",
        "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
        "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
        "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
        "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
        "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
        "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
        "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
        "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
        "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
        "It is based on propagating the trajectories of proposed flights forward in time.",
        "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
        "FACET is extensively used by The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
        "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
        "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
        "The user can then observe the effects of these changes to congestion.",
        "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
        "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
        "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
        "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
        "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
        "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
        "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
        "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
        "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
        "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
        "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
        "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
        "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
        "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
        "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
        "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
        "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
        "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
        "However, there are several problems with that approach.",
        "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
        "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
        "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
        "Each agent is then responsible for any aircraft going through its fix.",
        "Fixes offer many advantages as agents: 1.",
        "Their number can vary depending on need.",
        "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
        "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
        "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
        "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
        "Figure 2 shows a schematic of this agent based system.",
        "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
        "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
        "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
        "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
        "This is known as setting the Miles in Trail or MIT.",
        "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
        "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
        "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
        "Figure 2: Schematic of agent architecture.",
        "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
        "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
        "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
        "As a consequence, simple table-based immediate reward reinforcement learning is used.",
        "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
        "At every episode an agent takes an action and then receives a reward evaluating that action.",
        "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
        "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
        "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
        "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
        "The first and most direct approach is to let each agent receive the system performance as its reward.",
        "However, in many domains such a reward structure leads to slow learning.",
        "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
        "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
        "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
        "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
        "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
        "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
        "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
        "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
        "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
        "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
        "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
        "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
        "This form of G matches our system evaluation in the air traffic domain.",
        "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
        "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
        "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
        "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
        "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
        "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
        "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
        "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
        "This estimate should improve as the number of samples increases.",
        "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
        "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
        "In all experiments we test the performance of five different methods.",
        "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
        "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
        "The system reward, G(z), as define in Equation 1. 2.",
        "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
        "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
        "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
        "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
        "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
        "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
        "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
        "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
        "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
        "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
        "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
        "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
        "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
        "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
        "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
        "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
        "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
        "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
        "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
        "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
        "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
        "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
        "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
        "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
        "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
        "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
        "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
        "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
        "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
        "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
        "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
        "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
        "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
        "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
        "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
        "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
        "This evaluation function The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
        "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
        "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
        "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
        "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
        "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
        "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
        "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
        "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
        "Therefore, unless D is far from being optimal, the two penalties are not independent.",
        "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
        "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
        "Those results show that D is significantly superior to the other algorithms.",
        "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
        "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
        "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
        "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
        "Except when D is used, the values of k are computed once per episode.",
        "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
        "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
        "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
        "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
        "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
        "It therefore reaches 2100 computations at time step 100.",
        "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
        "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
        "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
        "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
        "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
        "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
        "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
        "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
        "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
        "We are currently extending this work in three directions.",
        "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
        "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
        "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
        "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
        "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
        "REFERENCES [1] A. Agogino and K. Tumer.",
        "Efficient evaluation functions for multi-rover systems.",
        "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
        "Multi agent reward analysis for learning in noisy domains.",
        "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
        "Handling communiction restrictions and team formation in congestion games.",
        "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
        "FACET: Future ATM concepts evaluation tool.",
        "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
        "A geometric optimization approach to aircraft conflict resolution.",
        "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
        "Free flight separation assurance using distributed algorithms.",
        "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
        "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
        "Central east pacific flight routing.",
        "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
        "A cooperative multi-agent approach to free flight.",
        "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
        "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
        "Optimal strategies for free flight air traffic conflict resolution.",
        "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
        "FACET: Future ATM concepts evaluation tool.",
        "Case no.",
        "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
        "Autonomous agents for air-traffic deconfliction.",
        "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
        "Benefits of direct-to in national airspace system.",
        "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
        "Aggregate flow model for air-traffic management.",
        "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
        "Reinforcement Learning: An Introduction.",
        "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
        "Conflict resolution for air traffic management.",
        "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
        "Collectives and the Design of Complex Systems.",
        "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
        "Optimal payoff functions for members of collectives.",
        "Advances in Complex Systems, 4(2/3):265-279, 2001.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
    ],
    "translated_text_sentences": [
        "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad.",
        "La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares.",
        "Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual.",
        "Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos).",
        "En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico.",
        "Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo.",
        "Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión.",
        "Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo).",
        "Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1.",
        "INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad.",
        "En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14].",
        "Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas.",
        "Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales.",
        "En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso.",
        "El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7].",
        "Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores.",
        "La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo.",
        "A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo.",
        "Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire.",
        "Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado.",
        "Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual.",
        "Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo).",
        "En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente.",
        "En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D.",
        "Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo.",
        "En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener.",
        "Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo.",
        "Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15].",
        "En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema.",
        "En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo).",
        "La primera recompensa explorada consistió en la recompensa del sistema.",
        "La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18].",
        "Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas.",
        "Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes.",
        "El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12].",
        "Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11].",
        "La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET.",
        "En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET.",
        "En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa.",
        "En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento.",
        "Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces.",
        "GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente.",
        "No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria).",
        "Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1.",
        "Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2.",
        "Flujo regional (20 minutos a 2 horas); 3.",
        "Flujo nacional (1-8 horas); y 4.",
        "Configuración dinámica del espacio aéreo (6 horas a 1 año).",
        "Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento.",
        "Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo.",
        "En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas.",
        "El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo.",
        "El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos).",
        "El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos.",
        "Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales.",
        "A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema.",
        "Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4].",
        "Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos.",
        "FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11].",
        "FACET es ampliamente utilizado por The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11].",
        "FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1).",
        "FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos.",
        "El usuario puede entonces observar los efectos de estos cambios en la congestión.",
        "En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes.",
        "Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales.",
        "En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo.",
        "La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z.",
        "Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión.",
        "La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4.",
        "La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario.",
        "De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde.",
        "De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza.",
        "Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías.",
        "En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas.",
        "De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA.",
        "De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA.",
        "La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles.",
        "La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA.",
        "El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente.",
        "Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente.",
        "Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política.",
        "Sin embargo, hay varios problemas con ese enfoque.",
        "Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande.",
        "Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento.",
        "Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos.",
        "Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia.",
        "Las reparaciones ofrecen muchas ventajas como agentes: 1.",
        "Su número puede variar dependiendo de la necesidad.",
        "El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2.",
        "Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3.",
        "Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4.",
        "Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos.",
        "La Figura 2 muestra un esquema de este sistema basado en agentes.",
        "Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes.",
        "Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo.",
        "Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente).",
        "En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar.",
        "Esto se conoce como establecer las Millas en Rastro o MIT.",
        "Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d).",
        "Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo.",
        "Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba.",
        "Figura 2: Esquema de la arquitectura del agente.",
        "Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]).",
        "Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal.",
        "Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas.",
        "Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples.",
        "Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15].",
        "En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción.",
        "Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje.",
        "En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad .",
        "En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25.",
        "Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje.",
        "El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa.",
        "Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento.",
        "Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente.",
        "Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema.",
        "En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i.",
        "Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2.",
        "En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema.",
        "De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema.",
        "Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17].",
        "Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional.",
        "Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G.",
        "Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida.",
        "Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales.",
        "El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes.",
        "El Sexto Internacional.",
        "Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra.",
        "Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo.",
        "Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes.",
        "Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf.",
        "Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos.",
        "Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos.",
        "Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci.",
        "Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci).",
        "Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)).",
        "Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto.",
        "Esta estimación debería mejorar a medida que aumenta el número de muestras.",
        "Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)).",
        "RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de optimización del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET.",
        "En todos los experimentos probamos el rendimiento de cinco métodos diferentes.",
        "El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política.",
        "Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1.",
        "La recompensa del sistema, G(z), como se define en la Ecuación 1.2.",
        "La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3.",
        "Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4.",
        "Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)).",
        "Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas.",
        "Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso.",
        "Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión.",
        "En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos.",
        "Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión.",
        "Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5.",
        "En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados.",
        "Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver.",
        "Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes.",
        "Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA.",
        "Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes.",
        "En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo.",
        "Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria.",
        "Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75.",
        "Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema.",
        "Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa.",
        "Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente.",
        "En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa.",
        "Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular.",
        "Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema.",
        "Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje.",
        "Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión.",
        "En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país.",
        "La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando.",
        "Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5.",
        "Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5.",
        "Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual.",
        "Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema.",
        "Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes.",
        "Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50.",
        "La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes.",
        "Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes.",
        "Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso.",
        "Esta función de evaluación The Sixth Intl.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema.",
        "Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α.",
        "Con un α alto, la optimización se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza.",
        "Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75.",
        "La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual.",
        "A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0.",
        "La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión.",
        "Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos.",
        "Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales.",
        "Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes.",
        "Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos.",
        "Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios.",
        "Esos resultados muestran que D es significativamente superior a los otros algoritmos.",
        "Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos.",
        "El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75.",
        "Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes.",
        "Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET.",
        "Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio.",
        "Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio.",
        "Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra.",
        "La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5.",
        "Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100.",
        "Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo.",
        "Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100.",
        "También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K. ",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100).",
        "Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable.",
        "De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2.",
        "Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5.",
        "DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año.",
        "La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria.",
        "Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo.",
        "Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable.",
        "Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones.",
        "Actualmente estamos ampliando este trabajo en tres direcciones.",
        "Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones.",
        "Segundo, estamos investigando estrategias de implementación y buscando modificaciones que tendrían un impacto mayor.",
        "Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión.",
        "Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento.",
        "Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6.",
        "REFERENCIAS [1] A. Agogino y K. Tumer.",
        "Funciones de evaluación eficientes para sistemas de múltiples rovers.",
        "En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer.",
        "Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos.",
        "En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer.",
        "Manejo de restricciones de comunicación y formación de equipos en juegos de congestión.",
        "Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe.",
        "FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos.",
        "Control de Tráfico Aéreo Trimestral, 9(1), 2001. [5] Karl D. Bilimoria.",
        "Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves.",
        "En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III.",
        "Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos.",
        "En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005.",
        "Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar.",
        "Enrutamiento de vuelos en el Pacífico central este.",
        "En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling.",
        "Un enfoque cooperativo de múltiples agentes para el vuelo libre.",
        "En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 1083-1090, Nueva York, NY, EE. UU., 2005.",
        "ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar.",
        "Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre.",
        "Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006.",
        "FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos.",
        "Caso número.",
        "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller.",
        "Agentes autónomos para la descongestión del tráfico aéreo.",
        "En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe.",
        "Beneficios de vuelo directo en el sistema nacional de espacio aéreo.",
        "En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji.",
        "Modelo de flujo agregado para la gestión del tráfico aéreo.",
        "Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto.",
        "Aprendizaje por refuerzo: Una introducción.",
        "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry.",
        "Resolución de conflictos para la gestión del tráfico aéreo.",
        "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores.",
        "Colectivos y el Diseño de Sistemas Complejos.",
        "Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer.",
        "Funciones de pago óptimas para los miembros de colectivos.",
        "Avances en Sistemas Complejos, 4(2/3):265-279, 2001.",
        "El Sexto Congreso Internacional.",
        "Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349"
    ],
    "error_count": 2,
    "keys": {
        "multiagent system": {
            "translated_key": "sistemas multiagente",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and <br>multiagent system</br>s, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and <br>multiagent system</br>s, pages 1083-1090, New York, NY, USA, 2005."
            ],
            "translated_annotated_samples": [
                "En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y <br>sistemas multiagente</br>, páginas 1083-1090, Nueva York, NY, EE. UU., 2005."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples. Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción. Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje. En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad . En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25. Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje. El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema. En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2. En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema. De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema. Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17]. Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G. Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida. Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales. El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes. El Sexto Internacional. Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra. Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo. Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes. Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf. Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos. Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos. Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci. Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci). Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)). Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)). RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de optimización del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política. Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1. La recompensa del sistema, G(z), como se define en la Ecuación 1.2. La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3. Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4. Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)). Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso. Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión. Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados. Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes. Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA. Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes. En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria. Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema. Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente. En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular. Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema. Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje. Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando. Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5. Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5. Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual. Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema. Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50. La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes. Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes. Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso. Esta función de evaluación The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α. Con un α alto, la optimización se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión. Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos. El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75. Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes. Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET. Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio. Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100. Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo. Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K.  Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100). Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable. De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2. Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5. DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable. Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones. Actualmente estamos ampliando este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones. Segundo, estamos investigando estrategias de implementación y buscando modificaciones que tendrían un impacto mayor. Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión. Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6. REFERENCIAS [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples rovers. En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación de equipos en juegos de congestión. Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Control de Tráfico Aéreo Trimestral, 9(1), 2001. [5] Karl D. Bilimoria. Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos. En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005. Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar. Enrutamiento de vuelos en el Pacífico central este. En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y <br>sistemas multiagente</br>, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre. Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Caso número. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la descongestión del tráfico aéreo. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios de vuelo directo en el sistema nacional de espacio aéreo. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje por refuerzo: Una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el Diseño de Sistemas Complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para los miembros de colectivos. Avances en Sistemas Complejos, 4(2/3):265-279, 2001. El Sexto Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reinforcement learning": {
            "translated_key": "aprendizaje por refuerzo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use <br>reinforcement learning</br> to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a <br>reinforcement learning</br> (RL) algorithm [15].",
                "In a <br>reinforcement learning</br> approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated <br>reinforcement learning</br> systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward <br>reinforcement learning</br> is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use <br>reinforcement learning</br> to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "<br>reinforcement learning</br>: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "Agents use <br>reinforcement learning</br> to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Agents learn the most appropriate separation for their location using a <br>reinforcement learning</br> (RL) algorithm [15].",
                "In a <br>reinforcement learning</br> approach, the selection of the agent reward has a large impact on the performance of the system.",
                "For complex delayed-reward problems, relatively sophisticated <br>reinforcement learning</br> systems such as temporal difference may have to be used.",
                "As a consequence, simple table-based immediate reward <br>reinforcement learning</br> is used."
            ],
            "translated_annotated_samples": [
                "Los agentes utilizan el <br>aprendizaje por refuerzo</br> para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión.",
                "Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de <br>aprendizaje por refuerzo</br> (RL) [15].",
                "En un enfoque de <br>aprendizaje por refuerzo</br>, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema.",
                "Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de <br>aprendizaje por refuerzo</br> relativamente sofisticados como la diferencia temporal.",
                "Como consecuencia, se utiliza el <br>aprendizaje por refuerzo</br> de recompensa inmediata basado en tablas simples."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el <br>aprendizaje por refuerzo</br> para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de <br>aprendizaje por refuerzo</br> (RL) [15]. En un enfoque de <br>aprendizaje por refuerzo</br>, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de <br>aprendizaje por refuerzo</br> relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el <br>aprendizaje por refuerzo</br> de recompensa inmediata basado en tablas simples. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "optimization": {
            "translated_key": "optimización",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic <br>optimization</br> method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the <br>optimization</br> focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric <br>optimization</br> approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic <br>optimization</br> method on a series of simulations using the FACET air traffic simulator.",
                "With high α the <br>optimization</br> focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "A geometric <br>optimization</br> approach to aircraft conflict resolution."
            ],
            "translated_annotated_samples": [
                "RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de <br>optimización</br> del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET.",
                "Con un α alto, la <br>optimización</br> se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza.",
                "Un enfoque de <br>optimización</br> geométrica para la resolución de conflictos de aeronaves."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples. Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción. Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje. En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad . En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25. Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje. El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema. En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2. En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema. De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema. Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17]. Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G. Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida. Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales. El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes. El Sexto Internacional. Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra. Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo. Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes. Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf. Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos. Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos. Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci. Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci). Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)). Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)). RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de <br>optimización</br> del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política. Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1. La recompensa del sistema, G(z), como se define en la Ecuación 1.2. La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3. Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4. Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)). Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso. Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión. Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados. Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes. Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA. Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes. En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria. Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema. Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente. En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular. Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema. Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje. Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando. Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5. Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5. Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual. Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema. Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50. La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes. Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes. Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso. Esta función de evaluación The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α. Con un α alto, la <br>optimización</br> se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión. Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos. El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75. Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes. Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET. Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio. Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100. Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo. Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K.  Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100). Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable. De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2. Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5. DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable. Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones. Actualmente estamos ampliando este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones. Segundo, estamos investigando estrategias de implementación y buscando modificaciones que tendrían un impacto mayor. Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión. Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6. REFERENCIAS [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples rovers. En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación de equipos en juegos de congestión. Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Control de Tráfico Aéreo Trimestral, 9(1), 2001. [5] Karl D. Bilimoria. Un enfoque de <br>optimización</br> geométrica para la resolución de conflictos de aeronaves. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos. En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005. Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar. Enrutamiento de vuelos en el Pacífico central este. En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre. Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Caso número. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la descongestión del tráfico aéreo. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios de vuelo directo en el sistema nacional de espacio aéreo. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje por refuerzo: Una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el Diseño de Sistemas Complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para los miembros de colectivos. Avances en Sistemas Complejos, 4(2/3):265-279, 2001. El Sexto Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "future atm concept evaluation tool": {
            "translated_key": "herramienta de evaluación de conceptos de cajeros automáticos del futuro",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "new method of estimating agent reward": {
            "translated_key": "nuevo método de estimación de la recompensa del agente",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "deployment strategies": {
            "translated_key": "estrategias de implementación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating <br>deployment strategies</br> and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "Second we are investigating <br>deployment strategies</br> and looking for modifications that would have larger impact."
            ],
            "translated_annotated_samples": [
                "Segundo, estamos investigando <br>estrategias de implementación</br> y buscando modificaciones que tendrían un impacto mayor."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples. Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción. Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje. En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad . En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25. Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje. El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema. En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2. En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema. De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema. Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17]. Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G. Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida. Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales. El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes. El Sexto Internacional. Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra. Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo. Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes. Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf. Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos. Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos. Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci. Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci). Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)). Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)). RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de optimización del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política. Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1. La recompensa del sistema, G(z), como se define en la Ecuación 1.2. La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3. Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4. Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)). Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso. Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión. Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados. Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes. Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA. Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes. En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria. Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema. Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente. En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular. Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema. Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje. Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando. Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5. Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5. Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual. Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema. Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50. La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes. Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes. Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso. Esta función de evaluación The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α. Con un α alto, la optimización se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión. Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos. El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75. Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes. Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET. Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio. Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100. Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo. Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K.  Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100). Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable. De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2. Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5. DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable. Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones. Actualmente estamos ampliando este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones. Segundo, estamos investigando <br>estrategias de implementación</br> y buscando modificaciones que tendrían un impacto mayor. Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión. Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6. REFERENCIAS [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples rovers. En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación de equipos en juegos de congestión. Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Control de Tráfico Aéreo Trimestral, 9(1), 2001. [5] Karl D. Bilimoria. Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos. En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005. Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar. Enrutamiento de vuelos en el Pacífico central este. En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre. Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Caso número. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la descongestión del tráfico aéreo. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios de vuelo directo en el sistema nacional de espacio aéreo. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje por refuerzo: Una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el Diseño de Sistemas Complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para los miembros de colectivos. Avances en Sistemas Complejos, 4(2/3):265-279, 2001. El Sexto Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "traffic flow": {
            "translated_key": "flujo de tráfico",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air <br>traffic flow</br> Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air <br>traffic flow</br> management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air <br>traffic flow</br> simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for <br>traffic flow</br> management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current <br>traffic flow</br> control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the <br>traffic flow</br> increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air <br>traffic flow</br>.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air <br>traffic flow</br> management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air <br>traffic flow</br> problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR <br>traffic flow</br> MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of <br>traffic flow</br> is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this <br>traffic flow</br> occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air <br>traffic flow</br> problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR <br>traffic flow</br> The multi agent approach to air <br>traffic flow</br> management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect <br>traffic flow</br> patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air <br>traffic flow</br> is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air <br>traffic flow</br> management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the <br>traffic flow</br>, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air <br>traffic flow</br> management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "Distributed Agent-Based Air <br>traffic flow</br> Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air <br>traffic flow</br> management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "In this paper we use FACET - an air <br>traffic flow</br> simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for <br>traffic flow</br> management.",
                "In order to efficiently and safely route this air traffic, current <br>traffic flow</br> control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "Furthermore, as the <br>traffic flow</br> increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air <br>traffic flow</br>."
            ],
            "translated_annotated_samples": [
                "La gestión distribuida del <br>flujo de tráfico</br> aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad.",
                "En este artículo utilizamos FACET, un simulador de <br>flujo de tráfico</br> aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del <br>flujo de tráfico</br>.",
                "Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del <br>flujo de tráfico</br> se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas.",
                "Además, a medida que aumenta el <br>flujo de tráfico</br>, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores.",
                "Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el <br>flujo general del tráfico aéreo</br>."
            ],
            "translated_text": "La gestión distribuida del <br>flujo de tráfico</br> aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de <br>flujo de tráfico</br> aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del <br>flujo de tráfico</br>. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del <br>flujo de tráfico</br> se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el <br>flujo de tráfico</br>, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el <br>flujo general del tráfico aéreo</br>. ",
            "candidates": [],
            "error": [
                [
                    "flujo de tráfico",
                    "flujo de tráfico",
                    "flujo de tráfico",
                    "flujo de tráfico",
                    "flujo de tráfico",
                    "flujo general del tráfico aéreo"
                ]
            ]
        },
        "congestion": {
            "translated_key": "congestión",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage <br>congestion</br>.",
                "Our FACET based results show that agents receiving personalized rewards reduce <br>congestion</br> by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze <br>congestion</br> patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to <br>congestion</br>.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and <br>congestion</br> but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of <br>congestion</br> in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total <br>congestion</br> penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total <br>congestion</br> penalty is a sum over the <br>congestion</br> penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a <br>congestion</br> or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing <br>congestion</br> at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible <br>congestion</br> become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic <br>congestion</br> domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of <br>congestion</br> over a four hour simulation.",
                "Agents are responsible for reducing <br>congestion</br> at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of <br>congestion</br> is added with the 100 remaining aircraft going through this second point of <br>congestion</br>.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no <br>congestion</br> control is being performed.",
                "Except where noted, the trade-off between <br>congestion</br> and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single <br>congestion</br> problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single <br>congestion</br> In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of <br>congestion</br> is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single <br>congestion</br> problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces <br>congestion</br> and lateness, other agents at the same time may take actions that increase <br>congestion</br> and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of <br>congestion</br>.",
                "On this problem the first region of <br>congestion</br> is the same as in the previous problem, and the second region of <br>congestion</br> is added in a different part of the country.",
                "The second <br>congestion</br> is less severe than the first one, so agents have to form different policies depending which point of <br>congestion</br> they are influencing.",
                "Figure 5: Performance on two <br>congestion</br> problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two <br>congestion</br> problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single <br>congestion</br> case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both <br>congestion</br> and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two <br>congestion</br> problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing <br>congestion</br>, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero <br>congestion</br> penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two <br>congestion</br> problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two <br>congestion</br> problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating <br>congestion</br>.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and <br>congestion</br> dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in <br>congestion</br> games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage <br>congestion</br>.",
                "Our FACET based results show that agents receiving personalized rewards reduce <br>congestion</br> by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze <br>congestion</br> patterns of different sectors and centers (Figure 1).",
                "The user can then observe the effects of these changes to <br>congestion</br>.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and <br>congestion</br> but does not account for fairness impact on different commercial entities."
            ],
            "translated_annotated_samples": [
                "Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la <br>congestión</br>.",
                "Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la <br>congestión</br> hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo).",
                "FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de <br>congestión</br> de diferentes sectores y centros (Figura 1).",
                "El usuario puede entonces observar los efectos de estos cambios en la <br>congestión</br>.",
                "Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la <br>congestión</br>, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la <br>congestión</br>. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la <br>congestión</br> hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de <br>congestión</br> de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la <br>congestión</br>. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la <br>congestión</br>, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "air traffic control": {
            "translated_key": "Control de Tráfico Aéreo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].",
                "In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated reinforcement learning systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward reinforcement learning is used.",
                "Our reinforcement learner is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use reinforcement learning to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "<br>air traffic control</br> Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "<br>air traffic control</br> Quarterly, 9(1), 2001. [5] Karl D. Bilimoria."
            ],
            "translated_annotated_samples": [
                "<br>Control de Tráfico Aéreo</br> Trimestral, 9(1), 2001. [5] Karl D. Bilimoria."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples. Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción. Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje. En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad . En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25. Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje. El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema. En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2. En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema. De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema. Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17]. Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G. Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida. Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales. El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes. El Sexto Internacional. Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra. Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo. Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes. Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf. Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos. Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos. Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci. Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci). Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)). Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)). RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de optimización del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política. Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1. La recompensa del sistema, G(z), como se define en la Ecuación 1.2. La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3. Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4. Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)). Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso. Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión. Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados. Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes. Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA. Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes. En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria. Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema. Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente. En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular. Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema. Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje. Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando. Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5. Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5. Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual. Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema. Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50. La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes. Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes. Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso. Esta función de evaluación The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α. Con un α alto, la optimización se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión. Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos. El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75. Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes. Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET. Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio. Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100. Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo. Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K.  Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100). Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable. De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2. Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5. DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable. Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones. Actualmente estamos ampliando este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones. Segundo, estamos investigando estrategias de implementación y buscando modificaciones que tendrían un impacto mayor. Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión. Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6. REFERENCIAS [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples rovers. En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación de equipos en juegos de congestión. Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. <br>Control de Tráfico Aéreo</br> Trimestral, 9(1), 2001. [5] Karl D. Bilimoria. Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos. En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005. Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar. Enrutamiento de vuelos en el Pacífico central este. En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre. Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Caso número. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la descongestión del tráfico aéreo. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios de vuelo directo en el sistema nacional de espacio aéreo. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje por refuerzo: Una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el Diseño de Sistemas Complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para los miembros de colectivos. Avances en Sistemas Complejos, 4(2/3):265-279, 2001. El Sexto Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349 ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "reinforcement learn": {
            "translated_key": "aprendizaje por refuerzo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Distributed Agent-Based Air Traffic Flow Management Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, USA kagan.tumer@oregonstate.edu Adrian Agogino UCSC, NASA Ames Research Center Mailstop 269-3 Moffett Field, CA 94035, USA adrian@email.arc.nasa.gov ABSTRACT Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today.",
                "The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars.",
                "Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume.",
                "This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace).",
                "In this paper we use FACET - an air traffic flow simulator developed at NASA and used extensively by the FAA and industry - to test a multi-agent algorithm for traffic flow management.",
                "An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix.",
                "Agents use <br>reinforcement learn</br>ing to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).",
                "Categories and Subject Descriptors I.2.11 [Computing Methodologies]: Artificial IntelligenceMultiagent systems General Terms Algorithms, Performance 1.",
                "INTRODUCTION The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.",
                "On a typical day, more than 40,000 commercial flights operate within the US airspace [14].",
                "In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized, hierarchical routing strategy that performs flow projections ranging from one to six hours.",
                "As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.",
                "In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.",
                "The total cost of these delays was estimated to exceed three billion dollars by industry [7].",
                "Furthermore, as the traffic flow increases, the current procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region) without providing any of them with means to shape the traffic patterns beyond minor reroutes.",
                "The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.",
                "Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.",
                "There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.",
                "An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.",
                "Though a truly distributed and adaptive solution (e.g., free flight where aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most radical departure from the current system.",
                "As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport capacity) and political fallout (e.g., impact on air traffic controllers).",
                "In this paper, we focus on agent based system that can be implemented readily.",
                "In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.",
                "Because aircraft flight plans consist of a sequence of fixes, this representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .",
                "In this approach, the agents actions are to set the separation that approaching aircraft are required to keep.",
                "This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.",
                "Agents learn the most appropriate separation for their location using a <br>reinforcement learn</br>ing (RL) algorithm [15].",
                "In a <br>reinforcement learn</br>ing approach, the selection of the agent reward has a large impact on the performance of the system.",
                "In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g, equivalent to a Monte-Carlo search).",
                "The first explored reward consisted of the system reward.",
                "The second reward was a personalized agent reward based on collectives [3, 17, 18].",
                "The last two rewards were personalized rewards based on estimations to lower the computational burden of the reward computation.",
                "All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents actions.",
                "Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents community [6, 9, 12].",
                "Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.",
                "In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.",
                "In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents learning algorithms and reward structures.",
                "In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and discuss the computational cost of achieving certain levels of performance.",
                "Finally, in Section 5, we discuss the implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 2.",
                "AIR TRAFFIC FLOW MANAGEMENT With over 40,000 flights operating within the United States airspace on an average day, the management of traffic flow is a complex and demanding problem.",
                "Not only are there concerns for the efficiency of the system, but also for fairness (e.g., different airlines), adaptability (e.g., developing weather patterns), reliability and safety (e.g., airport management).",
                "In order to address such issues, the management of this traffic flow occurs over four hierarchical levels: 1.",
                "Separation assurance (2-30 minute decisions); 1 We discuss how flight plans with few fixes can be handled in more detail in Section 2. 2.",
                "Regional flow (20 minutes to 2 hours); 3.",
                "National flow (1-8 hours); and 4.",
                "Dynamic airspace configuration (6 hours to 1 year).",
                "Because of the strict guidelines and safety concerns surrounding aircraft separation, we will not address that control level in this paper.",
                "Similarly, because of the business and political impact of dynamic airspace configuration, we will not address the outermost flow control level either.",
                "Instead, we will focus on the regional and national flow management problems, restricting our impact to decisions with time horizons between twenty minutes and eight hours.",
                "The proposed algorithm will fit between long term planning by the FAA and the very short term decisions by air traffic controllers.",
                "The continental US airspace consists of 20 regional centers (handling 200-300 flights on a given day) and 830 sectors (handling 10-40 flights).",
                "The flow control problem has to address the integration of policies across these sectors and centers, account for the complexity of the system (e.g., over 5200 public use airports and 16,000 air traffic controllers) and handle changes to the policies caused by weather patterns.",
                "Two of the fundamental problems in addressing the flow problem are: (i) modeling and simulating such a large complex system as the fidelity required to provide reliable results is difficult to achieve; and (ii) establishing the method by which the flow management is evaluated, as directly minimizing the total delay may lead to inequities towards particular regions or commercial entities.",
                "Below, we discuss how we addressed both issues, namely, we present FACET a widely used simulation tool and discuss our system evaluation function.",
                "Figure 1: FACET screenshot displaying traffic routes and air flow statistics. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), a physics based model of the US airspace was developed to accurately model the complex air traffic flow problem [4].",
                "It is based on propagating the trajectories of proposed flights forward in time.",
                "FACET can be used to either simulate and display air traffic (a 24 hour slice with 60,000 flights takes 15 minutes to simulate on a 3 GHz, 1 GB RAM computer) or provide rapid statistics on recorded data (4D trajectories for 10,000 flights including sectors, airports, and fix statistics in 10 seconds on the same computer) [11].",
                "FACET is extensively used by The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 343 the FAA, NASA and industry (over 40 organizations and 5000 users) [11].",
                "FACET simulates air traffic based on flight plans and through a graphical user interface allows the user to analyze congestion patterns of different sectors and centers (Figure 1).",
                "FACET also allows the user to change the flow patterns of the aircraft through a number of mechanisms, including metering aircraft through fixes.",
                "The user can then observe the effects of these changes to congestion.",
                "In this paper, agents use FACET directly through batch mode, where agents send scripts to FACET asking it to simulate air traffic based on metering orders imposed by the agents.",
                "The agents then produce their rewards based on receive feedback from FACET about the impact of these meterings. 2.2 System Evaluation The system performance evaluation function we select focuses on delay and congestion but does not account for fairness impact on different commercial entities.",
                "Instead it focuses on the amount of congestion in a particular sector and on the amount of measured air traffic delay.",
                "The linear combination of these two terms gives the full system evaluation function, G(z) as a function of the full system state z.",
                "More precisely, we have: G(z) = −((1 − α)B(z) + αC(z)) , (1) where B(z) is the total delay penalty for all aircraft in the system, and C(z) is the total congestion penalty.",
                "The relative importance of these two penalties is determined by the value of α, and we explore various trade-offs based on α in Section 4.",
                "The total delay, B, is a sum of delays over a set of sectors S and is given by: B(z) = X s∈S Bs(z) (2) where Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) where ks,t is the number of aircraft in sector s at a particular time, τs is a predetermined time, and Θ(·) is the step function that equals 1 when its argument is greater or equal to zero, and has a value of zero otherwise.",
                "Intuitively, Bs(z) provides the total number of aircraft that remain in a sector s past a predetermined time τs, and scales their contribution to count by the amount by which they are late.",
                "In this manner Bs(z) provides a delay factor that not only accounts for all aircraft that are late, but also provides a scale to measure their lateness.",
                "This definition is based on the assumption that most aircraft should have reached the sector by time τs and that aircraft arriving after this time are late.",
                "In this paper the value of τs is determined by assessing aircraft counts in the sector in the absence of any intervention or any deviation from predicted paths.",
                "Similarly, the total congestion penalty is a sum over the congestion penalties over the sectors of observation, S: C(z) = X s∈S Cs(z) (4) where Cs(z) = a X t Θ(ks,t − cs)eb(ks,t−cs) , (5) where a and b are normalizing constants, and cs is the capacity of sector s as defined by the FAA.",
                "Intuitively, Cs(z) penalizes a system state where the number of aircraft in a sector exceeds the FAAs official sector capacity.",
                "Each sector capacity is computed using various metrics which include the number of air traffic controllers available.",
                "The exponential penalty is intended to provide strong feedback to return the number of aircraft in a sector to below the FAA mandated capacities. 3.",
                "AGENT BASED AIR TRAFFIC FLOW The multi agent approach to air traffic flow management we present is predicated on adaptive agents taking independent actions that maximize the system evaluation function discussed above.",
                "To that end, there are four critical decisions that need to be made: agent selection, agent action set selection, agent learning algorithm selection and agent reward structure selection. 3.1 Agent Selection Selecting the aircraft as agents is perhaps the most obvious choice for defining an agent.",
                "That selection has the advantage that agent actions can be intuitive (e.g., change of flight plan, increase or decrease speed and altitude) and offer a high level of granularity, in that each agent can have its own policy.",
                "However, there are several problems with that approach.",
                "First, there are in excess of 40,000 aircraft in a given day, leading to a massively large multi-agent system.",
                "Second, as the agents would not be able to sample their state space sufficiently, learning would be prohibitively slow.",
                "As an alternative, we assign agents to individual ground locations throughout the airspace called fixes.",
                "Each agent is then responsible for any aircraft going through its fix.",
                "Fixes offer many advantages as agents: 1.",
                "Their number can vary depending on need.",
                "The system can have as many agents as required for a given situation (e.g., agents coming live around an area with developing weather conditions). 2.",
                "Because fixes are stationary, collecting data and matching behavior to reward is easier. 3.",
                "Because aircraft flight plans consist of fixes, agent will have the ability to affect traffic flow patterns. 4.",
                "They can be deployed within the current air traffic routing procedures, and can be used as tools to help air traffic controllers rather than compete with or replace them.",
                "Figure 2 shows a schematic of this agent based system.",
                "Agents surrounding a congestion or weather condition affect the flow of traffic to reduce the burden on particular regions. 3.2 Agent Actions The second issue that needs to be addressed, is determining the action set of the agents.",
                "Again, an obvious choice may be for fixes to bid on aircraft, affecting their flight plans.",
                "Though appealing from a free flight perspective, that approach makes the flight plans too unreliable and significantly complicates the scheduling problem (e.g., arrival at airports and the subsequent gate assignment process).",
                "Instead, we set the actions of an agent to determining the separation (distance between aircraft) that aircraft have 344 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) to maintain, when going through the agents fix.",
                "This is known as setting the Miles in Trail or MIT.",
                "When an agent sets the MIT value to d, aircraft going towards its fix are instructed to line up and keep d miles of separation (though aircraft will always keep a safe distance from each other regardless of the value of d).",
                "When there are many aircraft going through a fix, the effect of issuing higher MIT values is to slow down the rate of aircraft that go through the fix.",
                "By increasing the value of d, an agent can limit the amount of air traffic downstream of its fix, reducing congestion at the expense of increasing the delays upstream.",
                "Figure 2: Schematic of agent architecture.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own <br>reinforcement learn</br>er [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated <br>reinforcement learn</br>ing systems such as temporal difference may have to be used.",
                "However, due to our agent selection and agent action set, the air traffic congestion domain modeled in this paper only needs to utilize immediate rewards.",
                "As a consequence, simple table-based immediate reward <br>reinforcement learn</br>ing is used.",
                "Our <br>reinforcement learn</br>er is equivalent to an -greedy Q-learner with a discount rate of 0 [15].",
                "At every episode an agent takes an action and then receives a reward evaluating that action.",
                "After taking action a and receiving reward R an agent updates its Q table (which contains its estimate of the value for taking that action [15]) as follows: Q (a) = (1 − l)Q(a) + l(R), (6) where l is the learning rate.",
                "At every time step the agent chooses the action with the highest table value with probability 1 − and chooses a random action with probability .",
                "In the experiments described in this paper, α is equal to 0.5 and is equal to 0.25.",
                "The parameters were chosen experimentally, though system performance was not overly sensitive to these parameters. 3.4 Agent Reward Structure The final issue that needs to be addressed is selecting the reward structure for the learning agents.",
                "The first and most direct approach is to let each agent receive the system performance as its reward.",
                "However, in many domains such a reward structure leads to slow learning.",
                "We will therefore also set up a second set of reward structures based on agent-specific rewards.",
                "Given that agents aim to maximize their own rewards, a critical task is to create good agent rewards, or rewards that when pursued by the agents lead to good overall system performance.",
                "In this work we focus on difference rewards which aim to provide a reward that is both sensitive to that agents actions and aligned with the overall system reward [2, 17, 18]. 3.4.1 Difference Rewards Consider difference rewards of the form [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci) , (7) where zi is the action of agent i.",
                "All the components of z that are affected by agent i are replaced with the fixed constant ci 2 .",
                "In many situations it is possible to use a ci that is equivalent to taking agent i out of the system.",
                "Intuitively this causes the second term of the difference reward to evaluate the performance of the system without i and therefore D evaluates the agents contribution to the system performance.",
                "There are two advantages to using D: First, because the second term removes a significant portion of the impact of other agents in the system, it provides an agent with a cleaner signal than G. This benefit has been dubbed learnability (agents have an easier time learning) in previous work [2, 17].",
                "Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17]. 3.4.2 Estimates of Difference Rewards Though providing a good compromise between aiming for system performance and removing the impact of other agents from an agents reward, one issue that may plague D is computational cost.",
                "Because it relies on the computation of the counterfactual term G(z − zi + ci) (i.e., the system performance without agent i) it may be difficult or impossible to compute, particularly when the exact mathematical form of G is not known.",
                "Let us focus on G functions in the following form: G(z) = Gf (f(z)), (8) where Gf () is non-linear with a known functional form and, f(z) = X i fi(zi) , (9) where each fi is an unknown non-linear function.",
                "We assume that we can sample values from f(z), enabling us to compute G, but that we cannot sample from each fi(zi). 2 This notation uses zero padding and vector addition rather than concatenation to form full state vectors from partial state vectors.",
                "The vector zi in our notation would be ziei in standard vector notation, where ei is a vector with a value of 1 in the ith component and is zero everywhere else.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345 In addition, we assume that Gf is much easier to compute than f(z), or that we may not be able to even compute f(z) directly and must sample it from a black box computation.",
                "This form of G matches our system evaluation in the air traffic domain.",
                "When we arrange agents so that each aircraft is typically only affected by a single agent, each agents impact of the counts of the number of aircraft in a sector, kt,s, will be mostly independent of the other agents.",
                "These values of kt,s are the f(z)s in our formulation and the penalty functions form Gf .",
                "Note that given aircraft counts, the penalty functions (Gf ) can be easily computed in microseconds, while aircraft counts (f) can only be computed by running FACET taking on the order of seconds.",
                "To compute our counterfactual G(z − zi + ci) we need to compute: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Unfortunately, we cannot compute this directly as the values of fi(zi) are unknown.",
                "However, if agents take actions independently (it does not observe how other agents act before taking its own action) we can take advantage of the linear form of f(z) in the fis with the following equality: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) where E(f−i(z−i)|zi) is the expected value of all of the fs other than fi given the value of zi and E(f−i(z−i)|ci) is the expected value of all of the fs other than fi given the value of zi is changed to ci.",
                "We can then estimate f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci) .",
                "Therefore we can evaluate Di = G(z) − G(z − zi + ci) as: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13) leaving us with the task of estimating the values of E(f(z)|zi) and E(f(z)|ci)).",
                "These estimates can be computed by keeping a table of averages where we average the values of the observed f(z) for each value of zi that we have seen.",
                "This estimate should improve as the number of samples increases.",
                "To improve our estimates, we can set ci = E(z) and if we make the mean squared approximation of f(E(z)) ≈ E(f(z)) then we can estimate G(z) − G(z − zi + ci) as: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) This formulation has the advantage in that we have more samples at our disposal to estimate E(f(z)) than we do to estimate E(f(z)|ci)). 4.",
                "SIMULATION RESULTS In this paper we test the performance of our agent based air traffic optimization method on a series of simulations using the FACET air traffic simulator.",
                "In all experiments we test the performance of five different methods.",
                "The first method is Monte Carlo estimation, where random policies are created, with the best policy being chosen.",
                "The other four methods are agent based methods where the agents are maximizing one of the following rewards: 1.",
                "The system reward, G(z), as define in Equation 1. 2.",
                "The difference reward, Di(z), assuming that agents can calculate counterfactuals. 3.",
                "Estimation to the difference reward, Dest1 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)|ci). 4.",
                "Estimation to the difference reward, Dest2 i (z), where agents estimate the counterfactual using E(f(z)|zi) and E(f(z)).",
                "These methods are first tested on an air traffic domain with 300 aircraft, where 200 of the aircraft are going through a single point of congestion over a four hour simulation.",
                "Agents are responsible for reducing congestion at this single point, while trying to minimize delay.",
                "The methods are then tested on a more difficult problem, where a second point of congestion is added with the 100 remaining aircraft going through this second point of congestion.",
                "In all experiments the goal of the system is to maximize the system performance given by G(z) with the parameters, a = 50, b = 0.3, τs1 equal to 200 minutes and τs1 equal to 175 minutes.",
                "These values of τ are obtained by examining the time at which most of the aircraft leave the sectors, when no congestion control is being performed.",
                "Except where noted, the trade-off between congestion and lateness, α is set to 0.5.",
                "In all experiments to make the agent results comparable to the Monte Carlo estimation, the best policies chosen by the agents are used in the results.",
                "All results are an average of thirty independent trials with the differences in the mean (σ/ √ n) shown as error bars, though in most cases the error bars are too small to see.",
                "Figure 3: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .5. 4.1 Single Congestion In the first experiment we test the performance of the five methods when there is a single point of congestion, with twenty agents.",
                "This point of congestion is created by setting up a series of flight plans that cause the number of aircraft in 346 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the sector of interest to be significantly more than the number allowed by the FAA.",
                "The results displayed in Figures 3 and 4 show the performance of all five algorithms on two different system evaluations.",
                "In both cases, the agent based methods significantly outperform the Monte Carlo method.",
                "This result is not surprising since the agent based methods intelligently explore their space, where as the Monte Carlo method explores the space randomly.",
                "Figure 4: Performance on single congestion problem, with 300 Aircraft , 20 Agents and α = .75.",
                "Among the agent based methods, agents using difference rewards perform better than agents using the system reward.",
                "Again this is not surprising, since with twenty agents, an agent directly trying to maximize the system reward has difficulty determining the effect of its actions on its own reward.",
                "Even if an agent takes an action that reduces congestion and lateness, other agents at the same time may take actions that increase congestion and lateness, causing the agent to wrongly believe that its action was poor.",
                "In contrast agents using the difference reward have more influence over the value of their own reward, therefore when an agent takes a good action, the value of this action is more likely to be reflected in its reward.",
                "This experiment also shows that estimating the difference reward is not only possible, but also quite effective, when the true value of the difference reward cannot be computed.",
                "While agents using the estimates do not achieve as high of results as agents using the true difference reward, they still perform significantly better than agents using the system reward.",
                "Note, however, that the benefit of the estimated difference rewards are only present later in learning.",
                "Earlier in learning, the estimates are poor, and agents using the estimated difference rewards perform no better then agents using the system reward. 4.2 Two Congestions In the second experiment we test the performance of the five methods on a more difficult problem with two points of congestion.",
                "On this problem the first region of congestion is the same as in the previous problem, and the second region of congestion is added in a different part of the country.",
                "The second congestion is less severe than the first one, so agents have to form different policies depending which point of congestion they are influencing.",
                "Figure 5: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .5.",
                "Figure 6: Performance on two congestion problem, with 300 Aircraft, 50 Agents and α = .5.",
                "The results displayed in Figure 5 show that the relative performance of the five methods is similar to the single congestion case.",
                "Again agent based methods perform better than the Monte Carlo method and the agents using difference rewards perform better than agents using the system reward.",
                "To verify that the performance improvement of our methods is maintained when there are a different number of agents, we perform additional experiments with 50 agents.",
                "The results displayed in Figure 6 show that indeed the relative performances of the methods are comparable when the number of agents is increased to 50.",
                "Figure 7 shows scaling results and demonstrates that the conclusions hold over a wide range of number of agents.",
                "Agents using Dest2 perform slightly better than agents using Dest1 in all cases but for 50 agents.",
                "This slight advantage stems from Dest2 providing the agents with a cleaner signal, since its estimate uses more data points. 4.3 Penalty Tradeoffs The system evaluation function used in the experiments is G(z) = −((1−α)D(z)+αC(z)), which comprises of penalties for both congestion and lateness.",
                "This evaluation function The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 347 Figure 7: Impact of number of agents on system performance.",
                "Two congestion problem, with 300 Aircraft and α = .5. forces the agents to tradeoff these relative penalties depending on the value of α.",
                "With high α the optimization focuses on reducing congestion, while with low α the system focuses on reducing lateness.",
                "To verify that the results obtained above are not specific to a particular value of α, we repeat the experiment with 20 agents for α = .75.",
                "Figure 8 shows that qualitatively the relative performance of the algorithms remain the same.",
                "Next, we perform a series of experiments where α ranges from 0.0 to 1.0 .",
                "Figure 9 shows the results which lead to three interesting observations: • First, there is a zero congestion penalty solution.",
                "This solution has agents enforce large MIT values to block all air traffic, which appears viable when the system evaluation does not account for delays.",
                "All algorithms find this solution, though it is of little interest in practice due to the large delays it would cause. • Second, if the two penalties were independent, an optimal solution would be a line from the two end points.",
                "Therefore, unless D is far from being optimal, the two penalties are not independent.",
                "Note that for α = 0.5 the difference between D and this hypothetical line is as large as it is anywhere else, making α = 0.5 a reasonable choice for testing the algorithms in a difficult setting. • Third, Monte Carlo and G are particularly poor at handling multiple objectives.",
                "For both algorithms, the performance degrades significantly for mid-ranges of α. 4.4 Computational Cost The results in the previous section show the performance of the different algorithms after a specific number of episodes.",
                "Those results show that D is significantly superior to the other algorithms.",
                "One question that arises, though, is what computational overhead D puts on the system, and what results would be obtained if the additional computational expense of D is made available to the other algorithms.",
                "The computation cost of the system evaluation, G (Equation 1) is almost entirely dependent on the computation of Figure 8: Performance on two congestion problem, with 300 Aircraft, 20 Agents and α = .75.",
                "Figure 9: Tradeoff Between Objectives on two congestion problem, with 300 Aircraft and 20 Agents.",
                "Note that Monte Carlo and G are particularly bad at handling multiple objectives. the airplane counts for the sectors kt,s, which need to be computed using FACET.",
                "Except when D is used, the values of k are computed once per episode.",
                "However, to compute the counterfactual term in D, if FACET is treated as a black box, each agent would have to compute their own values of k for their counterfactual resulting in n + 1 computations of k per episode.",
                "While it may be possible to streamline the computation of D with some knowledge of the internals of FACET, given the complexity of the FACET simulation, it is not unreasonable in this case to treat it as a black box.",
                "Table 1 shows the performance of the algorithms after 2100 G computations for each of the algorithms for the simulations presented in Figure 5 where there were 20 agents, 2 congestions and α = .5.",
                "All the algorithms except the fully computed D reach 2100 k computations at time step 2100.",
                "D however computes k once for the system, and then once for each agent, leading to 21 computations per time step.",
                "It therefore reaches 2100 computations at time step 100.",
                "We also show the results of the full D computation at t=2100, which needs 44100 computations of k as D44K . 348 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: System Performance for 20 Agents, 2 congestions and α = .5, after 2100 G evaluations (except for D44K which has 44100 G evaluations at t=2100).",
                "Reward G σ/ √ n time Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Although D44K provides the best result by a slight margin, it is achieved at a considerable computational cost.",
                "Indeed, the performance of the two D estimates is remarkable in this case as they were obtained with about twenty times fewer computations of k. Furthermore, the two D estimates, significantly outperform the full D computation for a given number of computations of k and validate the assumptions made in Section 3.4.2.",
                "This shows that for this domain, in practice it is more fruitful to perform more learning steps and approximate D, than few learning steps with full D computation when we treat FACET as a black box. 5.",
                "DISCUSSION The efficient, safe and reliable management of air traffic flow is a complex problem, requiring solutions that integrate control policies with time horizons ranging from minutes up to a year.",
                "The main contribution of this paper is to present a distributed adaptive air traffic flow management algorithm that can be readily implemented and to test that algorithm using FACET, a simulation tool widely used by the FAA, NASA and the industry.",
                "Our method is based on agents representing fixes and having each agent determine the separation between aircraft approaching its fix.",
                "It offers the significant benefit of not requiring radical changes to the current air flow management structure and is therefore readily deployable.",
                "The agents use <br>reinforcement learn</br>ing to learn control policies and we explore different agent reward functions and different ways of estimating those functions.",
                "We are currently extending this work in three directions.",
                "First, we are exploring new methods of estimating agent rewards, to further speed up the simulations.",
                "Second we are investigating deployment strategies and looking for modifications that would have larger impact.",
                "One such modification is to extend the definition of agents from fixes to sectors, giving agents more opportunity to control the traffic flow, and allow them to be more efficient in eliminating congestion.",
                "Finally, in cooperation with domain experts, we are investigating different system evaluation functions, above and beyond the delay and congestion dependent G presented in this paper.",
                "Acknowledgments: The authors thank Banavar Sridhar for his invaluable help in describing both current air traffic flow management and NGATS, and Shon Grabbe for his detailed tutorials on FACET. 6.",
                "REFERENCES [1] A. Agogino and K. Tumer.",
                "Efficient evaluation functions for multi-rover systems.",
                "In The Genetic and Evolutionary Computation Conference, pages 1-12, Seatle, WA, June 2004. [2] A. Agogino and K. Tumer.",
                "Multi agent reward analysis for learning in noisy domains.",
                "In Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multi-Agent Systems, Utrecht, Netherlands, July 2005. [3] A. K. Agogino and K. Tumer.",
                "Handling communiction restrictions and team formation in congestion games.",
                "Journal of Autonous Agents and Multi Agent Systems, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, and S. R. Grabbe.",
                "FACET: Future ATM concepts evaluation tool.",
                "Air Traffic Control Quarterly, 9(1), 2001. [5] Karl D. Bilimoria.",
                "A geometric optimization approach to aircraft conflict resolution.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [6] Martin S. Eby and Wallace E. Kelly III.",
                "Free flight separation assurance using distributed algorithms.",
                "In Proc of Aerospace Conf, 1999, Aspen, CO, 1999. [7] FAA OPSNET data Jan-Dec 2005.",
                "US Department of Transportation website. [8] S. Grabbe and B. Sridhar.",
                "Central east pacific flight routing.",
                "In AIAA Guidance, Navigation, and Control Conference and Exhibit, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost, and Wynn C. Stirling.",
                "A cooperative multi-agent approach to free flight.",
                "In AAMAS 05: Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 1083-1090, New York, NY, USA, 2005.",
                "ACM Press. [10] P. K. Menon, G. D. Sweriduk, and B. Sridhar.",
                "Optimal strategies for free flight air traffic conflict resolution.",
                "Journal of Guidance, Control, and Dynamics, 22(2):202-211, 1999. [11] 2006 NASA Software of the Year Award Nomination.",
                "FACET: Future ATM concepts evaluation tool.",
                "Case no.",
                "ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek, and M. Uller.",
                "Autonomous agents for air-traffic deconfliction.",
                "In Proc of the Fifth Int Jt Conf on Autonomous Agents and Multi-Agent Systems, Hakodate, Japan, May 2006. [13] B. Sridhar and S. Grabbe.",
                "Benefits of direct-to in national airspace system.",
                "In AIAA Guidance, Navigation, and Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth, and G. B. Chatterji.",
                "Aggregate flow model for air-traffic management.",
                "Journal of Guidance, Control, and Dynamics, 29(4):992-997, 2006. [15] R. S. Sutton and A. G. Barto.",
                "Reinforcement Learning: An Introduction.",
                "MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, and S. Sastry.",
                "Conflict resolution for air traffic management.",
                "IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer and D. Wolpert, editors.",
                "Collectives and the Design of Complex Systems.",
                "Springer, New York, 2004. [18] D. H. Wolpert and K. Tumer.",
                "Optimal payoff functions for members of collectives.",
                "Advances in Complex Systems, 4(2/3):265-279, 2001.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 349"
            ],
            "original_annotated_samples": [
                "Agents use <br>reinforcement learn</br>ing to set this separation and their actions speed up or slow down traffic to manage congestion.",
                "Agents learn the most appropriate separation for their location using a <br>reinforcement learn</br>ing (RL) algorithm [15].",
                "In a <br>reinforcement learn</br>ing approach, the selection of the agent reward has a large impact on the performance of the system.",
                "The agents corresponding to fixes surrounding a possible congestion become live and start setting new separation times. 3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance, G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own <br>reinforcement learn</br>er [15] (though alternatives such as evolving neuro-controllers are also effective [1]).",
                "For complex delayed-reward problems, relatively sophisticated <br>reinforcement learn</br>ing systems such as temporal difference may have to be used."
            ],
            "translated_annotated_samples": [
                "Los agentes utilizan el <br>aprendizaje por refuerzo</br> para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión.",
                "Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de <br>aprendizaje por refuerzo</br> (RL) [15].",
                "En un enfoque de <br>aprendizaje por refuerzo</br>, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema.",
                "Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio <br>aprendiz reforzado</br> [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]).",
                "Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de <br>aprendizaje por refuerzo</br> relativamente sofisticados como la diferencia temporal."
            ],
            "translated_text": "La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el <br>aprendizaje por refuerzo</br> para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de <br>aprendizaje por refuerzo</br> (RL) [15]. En un enfoque de <br>aprendizaje por refuerzo</br>, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio <br>aprendiz reforzado</br> [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de <br>aprendizaje por refuerzo</br> relativamente sofisticados como la diferencia temporal. ",
            "candidates": [],
            "error": [
                [
                    "aprendizaje por refuerzo",
                    "aprendizaje por refuerzo",
                    "aprendizaje por refuerzo",
                    "aprendiz reforzado",
                    "aprendizaje por refuerzo"
                ]
            ]
        }
    }
}