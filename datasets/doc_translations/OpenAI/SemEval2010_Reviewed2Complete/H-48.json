{
    "id": "H-48",
    "original_text": "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents. Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems. Query Expansion (QE) is one method for dealing with term mismatch. IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets. While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch. In this paper, we propose a new approach for evaluating query expansion techniques. The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1. INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue. Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding. Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics. Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents. During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning. Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs. This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched. IR evaluations are comparative in nature (cf. TREC). Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics. Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets. While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch. If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation. An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting. In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents. Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not. If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2. RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47]. Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs. Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37]. Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2]. Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents). In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others. QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27]. Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift. As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5]. The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14]. In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41]. In addition, the IR research community has given attention to differences between the performance of individual queries. Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance. In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9]. There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP. GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33]. However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms. By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query. The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time. In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms. Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3. METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner. Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes. We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query. Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner. This removal process affects only the relevant documents in the search collection. The queries themselves remain unaltered. Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms. In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents. Notice that, for a given query, only relevant documents are modified. Non-relevant documents are left unchanged, even in the case that they contain query terms. Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents. Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap. Removing a query term from relevant documents simply masks the presence of that query term in those documents. It does not in any way change the conceptual relevancy of the documents. The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments. The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance. Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection. In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection. In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric. In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query. It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch. The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time. Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents. Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection. Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection. Once an order for removal has been decided, a manner for term removal/masking must be decided. It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on). The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch. Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4. EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections. Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search). TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source. This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched. Translation probabilities for QE [2] are calculated from these large external corpora. Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines. Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms. We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities. As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE. The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7. When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms. The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections. The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection. The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries. The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length. In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection. After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection. Terms with high IDF values tend to influence document ranking more than those with lower IDF values. Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first. For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents. The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents. For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents. Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5. RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection. As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection. The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures. TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms). Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search. Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection. However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries. The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS. On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6). This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries. MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries. P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed. Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents. At two query terms removed, TCS starts outperforming Okapi FB. If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher. It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed. This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries. As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation. As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed. On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed. Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6. DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents. In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection. Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents. A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection. The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be. This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked. For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis. Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents. This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents. Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance. The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents. However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted. Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience. As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users. Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection. This could lead to insight as to when QE should and should not be applied. This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch. In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known. By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems. Such robustness makes a system more user-friendly, especially to non-expert users. This paper presents a novel framework for IR system evaluation, the applications of which are numerous. The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied. To be sure, there is much future work that could be done using this framework. In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch. Indeed, this framework would also benefit from further testing on a larger collection. 7. CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents. Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval. By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents. Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection. The evaluation framework proposed in this paper is attractive for several reasons. Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch. In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections. Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way. It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries. An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8. REFERENCES [1] Amati, G., C. Carpineto, and G. Romano. Query difficulty, robustness and selective application of query expansion. In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D. Lafferty. 1999. Information retrieval as statistical translation. In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003. Query expansion using associated queries. In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003. When Query Expansion Fails. In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004. Questioning Query Expansion: An Examination of Behaviour and Parameters. In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005. Document Expansion versus Query Expansion for ad-hoc Retrieval. In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006. What Makes A Query Difficult? In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998. Informative Term Selection for Automatic Query Expansion. In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005. Incremental Test Collections. In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006. Minimal Test Collections for Retrieval Evaluation. In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L. Clarke. 1998. Efficient Construction of Large Test Collections. In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R. Lynam. 2006. Statistical Precision of Information Retrieval Evaluation. In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B. Croft. 2004. A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N. Query Expansion. 1996. In Martha E. Williams (ed.), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995. CLARIT-TREC Experiments. Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X. Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004. Dependence language model for information retrieval. In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992. Relevance feedback revisited. In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993. The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994. The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995. The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998. Towards Interactive Query Expansion. In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B. Croft. 1994. The Association Thesaurus for Information Retrieval. In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B. Keefer. Query expansion/reduction and its impact on retrieval effectiveness. In: D.K. Harman, ed. The Third Text REtrieval Conference (TREC-3). Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002. Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources. In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998. Improving Automatic Query Expansion. In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991. The limitations of term co-occurrence data for query expansion in document retrieval systems. Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B. Croft. 1998. A language modeling approach to information retrieval. In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993. Concept based query expansion. In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006. On GMAP - and other transformations. In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976. Relevance Weighting of Search Terms. Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M. Hancock-Beaulieu, and M. Gatford. 1994. Okapi at TREC-2. In D.K. Harman (ed). 1994. The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M. Hancock-Beaulieu, and M. Gatford. 1995. Okapi at TREC-3. In D.K. Harman (ed). 1995. The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971. Relevance feedback in information retrieval. In G. Salton (Ed.), The SMART Retrieval System. Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968. Automatic Information Organization and Retrieval. McGraw-Hill. [39] Salton, G. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980. Automatic term class construction using relevance-a summary of work in automatic pseudoclassification. Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988. On the Use of Spreading Activation Methods in Automatic Information Retrieval. In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994. Word sense disambiguation and information retrieval. In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004. Forming test collections with no system pooling. In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J. Van Rijsbergen. 1983. The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System. Computer Journal. 26(3):239-246. [46] Song, F. and W.B. Croft. 1999. A general language model for information retrieval. In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971. Automatic Keyword Classification for Information Retrieval. London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004. Scoring missing terms in information retrieval tasks. In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994. Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance. In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a. On Expanding Query Vectors with Lexically Related Words. In Harman, D. K., ed. Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b. Query Expansion Using Lexical-Semantic Relations. In Proceedings of SIGIR 1994, pp. 61-69.",
    "original_translation": "Un nuevo enfoque para evaluar la expansión de la consulta: consulta Documento Término Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN Tonya.custis@thomson.com Khalid al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, Mn Khalid.al--kofahi@thomson.com Resumen La efectividad de los sistemas de recuperación de información (IR) está influenciado por el grado de superposición de términos entre consultas de usuario y documentos relevantes. La falta de coincidencia del término del documento de consulta, ya sea parcial o total, es un hecho que los sistemas IR deben tratarse. La expansión de la consulta (QE) es un método para lidiar con la falta de coincidencia de términos. Los sistemas IR que implementan la expansión de consultas se evalúan típicamente ejecutando cada consulta dos veces, con y sin expansión de la consulta, y luego comparando los dos conjuntos de resultados. Si bien esto mide un cambio general en el rendimiento, no mide directamente la efectividad de los sistemas IR para superar el problema inherente del malhechor de términos entre la consulta y los documentos relevantes, ni proporciona ninguna idea de cómo tales sistemas se comportarían en presencia deCONSIDAD DEL Documento MISMACH. En este documento, proponemos un nuevo enfoque para evaluar las técnicas de expansión de consultas. El enfoque propuesto es atractivo porque proporciona una estimación del rendimiento del sistema bajo diversos grados de desajuste de término de documento de consulta, utiliza colecciones de pruebas fácilmente disponibles y no requiere ningún juicio de relevancia adicional o ninguna forma de procesamiento manual. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información Términos generales Medición, Experimentación 1. Introducción en nuestro dominio, 1 y, a diferencia de la búsqueda web, es muy importante que los abogados encuentren todos los documentos (por ejemplo, casos) que sean relevantes para un problema. Los documentos relevantes faltantes pueden tener consecuencias no triviales en el resultado de un procedimiento judicial. Los abogados están especialmente preocupados por la falta de documentos relevantes al investigar un tema legal que es nuevo para ellos, ya que pueden no ser conscientes de todas las variaciones de idiomas en tales temas. Por lo tanto, es importante desarrollar sistemas de recuperación de información que sean robustos con respecto a las variaciones de lenguaje o el malhechado de término entre consultas y documentos relevantes. Durante nuestro trabajo sobre el desarrollo de dichos sistemas, concluimos que los métodos de evaluación actuales no son suficientes para este propósito.{Tos ferina, tos ferina}, {ataque cardíaco, infarto de miocardio}, {lavado de autos, limpieza de automóviles}, {abogado, asesor legal, abogado} son ejemplos de cosas que comparten el mismo significado. A menudo, los términos elegidos por los usuarios en sus consultas son diferentes a los que aparecen en los documentos relevantes para sus necesidades de información. Este término del término de consulta del documento surge de dos fuentes: (1) La sinonimia que se encuentra en el lenguaje natural, tanto en el término como en el nivel de la frase, y (2) el grado en que el usuario es un experto en la búsqueda y/o tiene expertoConocimiento en el dominio de la colección que se busca. Las evaluaciones IR son de naturaleza comparativa (cf. Trec). En general, las evaluaciones IR muestran cómo el sistema A lo hizo en relación con el sistema B en la misma colección de pruebas basada en varias métricas basadas en precisión y retiro. Del mismo modo, los sistemas IR con capacidades QE se evalúan típicamente ejecutando cada búsqueda dos veces, una y una vez sin expansión de la consulta, y luego comparando los dos conjuntos de resultados. Si bien este enfoque muestra qué sistema puede haber tenido un mejor desempeño en general con respecto a una colección de pruebas particular, no mide directa o sistemáticamente la efectividad de los sistemas IR para superar la falta de coincidencia del término de documento de consulta. Si el objetivo de QE es aumentar el rendimiento de la búsqueda mediante la mitigación de los efectos del desajuste del término del documento de consulta, entonces el grado en que un sistema lo hace debe ser medible en la evaluación. Un método de evaluación efectivo debe medir el rendimiento de los sistemas IR bajo diversos grados de desajuste de término de documento de consulta, no solo en términos de rendimiento general en una colección en relación con otro sistema.1 Thomson Corporation desarrolla soluciones basadas en información a los mercados profesionales, incluidos los legales, financieros, de atención médica, científica e fiscal y contabilidad. Para medir que un sistema IR en particular puede superar la falta de coincidencia del término del documento de consulta al recuperar documentos que son relevantes para una consulta de usuarios, pero que no necesariamente contienen los términos de consulta en sí mismos, introducimos sistemáticamente el desajuste de términos en la recopilación de pruebas deEliminar términos de consulta de documentos relevantes conocidos. Debido a que estamos induciendo a propósito el malhechor de términos entre las consultas y los documentos relevantes conocidos en nuestras colecciones de pruebas, el marco de evaluación propuesto puede medir la efectividad de QE de una manera que las pruebas en toda la colección no lo son. Si un método de búsqueda QE encuentra un documento que se sabe que es relevante, pero que no obstante se pierde los términos de consulta, muestra que la técnica QE es realmente robusta con respecto a la desajuste del término de consulta.2. Trabajo relacionado que contabiliza a término desajuste entre los términos en consultas de usuarios y los documentos relevantes para las necesidades de información de los usuarios ha sido un problema fundamental en la investigación de IR durante casi 40 años [38, 37, 47]. La expansión de la consulta (QE) es una técnica utilizada en IR para mejorar el rendimiento de la búsqueda al aumentar la probabilidad de superposición de términos (ya sea explícita o implícitamente) entre consultas y documentos que son relevantes para las necesidades de información de los usuarios. La expansión de consultas explícitas se produce en tiempo de ejecución, en función de los resultados de búsqueda iniciales, como es el caso con la retroalimentación de relevancia y la retroalimentación de pseudo relevancia [34, 37]. La expansión de consultas implícitas puede basarse en propiedades estadísticas de la recopilación de documentos, o puede confiar en fuentes de conocimiento externas como un tesauro o una ontología [32, 17, 26, 50, 51, 2]. Independientemente del método, los algoritmos QE que son capaces de recuperar documentos relevantes a pesar de la falta de coincidencia parcial o total entre consultas y documentos relevantes deberían aumentar el retiro de los sistemas IR (recuperando documentos que se habrían perdido anteriormente), así como su precisión (al recuperardocumentos más relevantes). En la práctica, QE tiende a mejorar el rendimiento promedio de la recuperación general, haciéndolo mejorando el rendimiento en algunas consultas al tiempo que lo empeora en otras. Las técnicas QE se consideran efectivas en el caso de que ayudan más de lo que duelen en general en una colección particular [47, 45, 41, 27]. A menudo, los términos de expansión agregados a una consulta en la fase de expansión de la consulta terminan perjudicando el rendimiento general de la recuperación porque introducen ruido semántico, lo que hace que el significado de la consulta se deriva. Como tal, se ha realizado mucho trabajo con respecto a diferentes estrategias para elegir términos QE semánticamente relevantes para incluir para evitar la deriva de la consulta [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5]. La evaluación de los sistemas IR ha recibido mucha atención en la comunidad de investigación, tanto en términos de desarrollo de colecciones de pruebas para la evaluación de diferentes sistemas [11, 12, 13, 43] como en términos de la utilidad de las métricas de evaluación como el retiro, la precisión, la precisión, precisión promedio media, precisión en rango, bpref, etc. [7, 8, 44, 14]. Además, ha habido evaluaciones comparativas de diferentes técnicas de QE en varias colecciones de prueba [47, 45, 41]. Además, la comunidad de investigación IR ha prestado atención a las diferencias entre el desempeño de las consultas individuales. Se han realizado esfuerzos de investigación para predecir qué consultas serán mejoradas con QE y luego aplicarlo selectivamente solo a esas consultas [1, 5, 27, 29, 15, 48], para lograr un rendimiento general óptimo. Además, se ha realizado el trabajo relacionado para predecir la dificultad de consultas, o qué consultas probablemente se realizan mal [1, 4, 5, 9]. Existe un interés general en la comunidad de investigación para mejorar la robustez de los sistemas IR al mejorar el rendimiento de la recuperación en consultas difíciles, como lo demuestra la pista robusta en las competiciones TREC y las nuevas medidas de evaluación como GMAP. GMAP (precisión promedio media geométrica) da más peso al extremo inferior de la precisión promedio (en lugar de MAP), enfatizando así el grado en que las consultas difíciles o de bajo rendimiento contribuyen a la puntuación [33]. Sin embargo, no se presta atención a la evaluación de la robustez de los sistemas IR que implementan QE con respecto al desajuste del término de consulta en términos cuantificables. Al inducir deliberadamente la falta de coincidencia entre los términos en consultas y documentos relevantes, nuestro marco de evaluación nos permite una manera controlada para degradar la calidad de las consultas con respecto a sus documentos relevantes, y luego medir tanto el grado de dificultad (inducida)de la consulta y el grado en que QE mejora el rendimiento de recuperación de la consulta degradada. El trabajo más similar al nuestro en la literatura consiste en un trabajo en el que las colecciones o consultas de documentos se alteran de manera sistemática para medir las diferencias de rendimiento de la consulta.[42] se introduce en la recopilación de documentos Pseudowords que son ambiguos con respecto al sentido de las palabras, para medir el grado en que la desambiguación del sentido de las palabras es útil en IR.[6] Experimentos con la alteración de la recopilación de documentos agregando términos de expansión semánticamente relacionados a los documentos en el tiempo de indexación. En el IR cruzado, [28] explora diferentes técnicas de expansión de consultas al tiempo que degrada deliberadamente sus recursos de traducción, en lo que equivale a expandir una consulta con solo un porcentaje controlado de sus términos de traducción. Aunque similar en la introducción de una cantidad controlada de varianza en sus colecciones de pruebas, estos trabajos difieren del trabajo que se presenta en este documento, ya que el trabajo que se presenta aquí se mide explícita y sistemáticamente la efectividad de la consulta en presencia de la falta de coincidencia de términos de consulta.3. Metodología Para medir con precisión el rendimiento del sistema IR en presencia de desajuste a término de consultas, necesitamos poder ajustar el grado de desajuste a término en un corpus de prueba de manera principalmente. Nuestro enfoque es introducir la falta de coincidencia del término deldocumento en un corpus de manera controlada y luego medir el rendimiento de los sistemas IR a medida que cambia el grado de desajuste del término. Eliminamos sistemáticamente los términos de consulta de documentos relevantes conocidos, creando versiones alternativas de una colección de pruebas que difieren solo en cuántos o qué términos de consulta se han eliminado de los documentos relevantes para una consulta en particular. La introducción de la falta de desajuste del término del documento de consulta en la colección de pruebas de esta manera nos permite manipular el grado de desajuste del término entre documentos y consultas relevantes de manera controlada. Este proceso de eliminación afecta solo a los documentos relevantes en la colección de búsqueda. Las consultas mismas permanecen inalteradas. Los términos de consulta se eliminan de los documentos uno por uno, por lo que las diferencias en el rendimiento del sistema IR se pueden medir con respecto a los términos faltantes. En el caso más extremo (es decir, cuando la longitud de la consulta es menor o igual al número de términos de consulta eliminados de los documentos relevantes), no habrá superposición de término entre una consulta y sus documentos relevantes. Observe que, para una consulta dada, solo se modifican documentos relevantes. Los documentos no relevantes se dejan sin cambios, incluso en el caso de que contengan términos de consulta. Aunque, en la superficie, estamos cambiando la distribución de los términos entre los conjuntos de documentos relevantes y no relevantes al eliminar los términos de consulta de los documentos relevantes, ya que no cambia la relevancia conceptual de estos documentos. Eliminar sistemáticamente los términos de consulta de documentos relevantes conocidos introduce una cantidad controlada de desajuste de término de documento de consulta por el cual podemos evaluar el grado en que las técnicas QE particulares pueden recuperar documentos conceptualmente relevantes, a pesar de la falta de superposición de término real. Eliminar un término de consulta de documentos relevantes simplemente enmascara la presencia de ese término de consulta en esos documentos. De ninguna manera cambia la relevancia conceptual de los documentos. El marco de evaluación presentado en este documento consta de tres elementos: una colección de pruebas, C;una estrategia para seleccionar qué términos de consulta eliminar de los documentos relevantes en esa colección, s;y una métrica para comparar el rendimiento de los sistemas IR, m.La recopilación de pruebas, C, consiste en una recopilación de documentos, consultas y juicios de relevancia. La estrategia, s, determina el orden y la forma en que los términos de la consulta se eliminan de los documentos relevantes en C. Este marco de evaluación no es específico de la métrica;Cualquier métrica (mapa, p@10, retirada, etc.) se puede utilizar para medir el rendimiento del sistema IR. Aunque las colecciones de pruebas son difíciles de encontrar, debe tenerse en cuenta que este marco de evaluación se puede usar en cualquier colección de pruebas disponible. De hecho, el uso de este marco estira el valor de las colecciones de pruebas existentes en que una colección se convierte en varios cuando los términos de consulta se eliminan de los documentos relevantes, lo que aumenta la cantidad de información que se puede obtener al evaluar una colección en particular. En otras evaluaciones de efectividad de QE, la variable controlada es simplemente si las consultas se han expandido o no, comparado en términos de alguna métrica. Por el contrario, la variable controlada en este marco es el término de consulta que se ha eliminado de los documentos relevantes para esa consulta, según lo determinado por la estrategia de eliminación, los términos de consulta de S. se eliminan uno por uno, de una manera y orden determinado por S, de modo que las colecciones difieren solo con respecto al un término que se ha eliminado (o enmascarado) en los documentos relevantes para esa consulta. Es de esta manera que podemos medir explícitamente el grado en que un sistema IR supera el desajuste del término del documento de consulta. La elección de una estrategia de eliminación de términos de consulta es relativamente flexible;La única restricción para elegir una estrategia s es que los términos de consulta deben eliminarse uno a la vez. Se deben tomar dos decisiones al elegir una estrategia de eliminación S. El primero es el orden en el que S elimina los términos de los documentos relevantes. Los posibles pedidos de eliminación podrían basarse en métricas como IDF o la probabilidad global de un término en una recopilación de documentos. Según el propósito de la evaluación y el algoritmo de recuperación que se está utilizando, podría tener más sentido elegir una orden de eliminación para S basada en el término de consulta IDF o tal vez basado en una medida de la probabilidad de término de consulta en la recopilación de documentos. Una vez que se ha decidido un pedido de eliminación, se debe decidir una forma de extracción/enmascaramiento a plazo. Debe determinarse si S eliminará los términos individualmente (es decir, eliminar solo un término diferente cada vez) o aditivamente (es decir, eliminar un término primero, entonces ese término además de otro, y así sucesivamente). La eliminación aditiva incremental de los términos de consulta de los documentos relevantes permite que la evaluación muestre el grado en que el rendimiento del sistema IR se degrada a medida que faltan más y más términos de consulta, lo que aumenta el grado de desajuste de término de documento de consulta. Eliminar términos individualmente permite una comparación clara de la contribución de QE en ausencia de cada término de consulta individual.4. Configuración experimental 4.1 Sistemas IR Utilizamos el marco de evaluación propuesto para evaluar cuatro sistemas IR en dos colecciones de pruebas. De los cuatro sistemas utilizados en la evaluación, dos técnicas de expansión de consultas implementan: OKAPI (con pseudo-retroalimentación para QE) y un motor de búsqueda conceptual patentado (bien, llame TCS, para Thomson Concept Search). TCS es un motor de recuperación basado en modelos de idiomas que utiliza un corpus externo apropiado para el tema (es decir, legal o noticias) como fuente de conocimiento. Esta fuente de conocimiento externo es un corpus separado, pero temáticamente relacionado con la recopilación de documentos a buscar. Las probabilidades de traducción para QE [2] se calculan a partir de estos grandes corpus externos. OKAPI (sin retroalimentación) y un modelo de probabilidad de consulta del modelo de idioma (QL) (implementado con Indri) se incluyen como líneas de base de solo palabras clave. Okapi sin retroalimentación se pretende como una línea de base análoga para OKAPI con comentarios, y el modelo QL está destinado a ser una línea de base apropiada para TCS, ya que ambos implementan algoritmos de recuperación basados en modelos de lenguaje. Elegimos esto como líneas de base porque solo dependen de las palabras que aparecen en las consultas y no tienen capacidades QE. Como resultado, esperamos que cuando los términos de la consulta se eliminen de los documentos relevantes, el rendimiento de estos sistemas debería degradarse más dramáticamente que sus contrapartes que implementan QE. Los resultados del modelo OKAPI y QL se obtuvieron utilizando el kit de herramientas Lemur.2 OKAPI se ejecutó con los parámetros K1 = 1.2, B = 0.75 y K3 = 7. Cuando se ejecutan con comentarios, los parámetros de retroalimentación utilizados en Okapi se establecieron en 10 documentos y 25 términos. El modelo QL utilizó el suavizado Jelinek-Mercer, con λ = 0.6.4.2 COLECCIONES DE PRUEBA Evaluamos el rendimiento de los cuatro sistemas IR descritos anteriormente en dos colecciones de prueba diferentes. Las dos colecciones de prueba utilizadas fueron la colección TREC AP89 (disco de Tipster 1) y la colección FSupp. La colección FSUPP es una colección patentada de 11,953 documentos de jurisprudencia para los cuales tenemos 44 consultas (que van de cuatro a veintidós palabras después de la eliminación de palabras de parada) con juicios de relevancia total.3 La longitud promedio de los documentos en la colección FSUPP es 3444 palabras 3444 palabras.2 www.lemuRproject.org 3 Cada uno de los 11,953 documentos fue evaluado por expertos en dominios con respecto a cada una de las 44 consultas. La colección de pruebas TREC AP89 contiene 84,678 documentos, con un promedio de 252 palabras de longitud. En nuestra evaluación, utilizamos tanto el título como los campos de descripción de los temas 151200 como consultas, por lo que tenemos dos conjuntos de resultados para la colección AP89. Después de la eliminación de palabras de parada, las consultas de título varían de dos a once palabras y las consultas de descripción varían de cuatro a veintiséis términos.4.3 Estrategia de eliminación de términos de consulta En nuestros experimentos, elegimos eliminar secuencial y aditivamente los términos de consulta de la frecuencia de documentos inversa (IDF) más alta a la más baja con respecto a toda la recopilación de documentos. Los términos con altos valores de IDF tienden a influir en la clasificación de documentos más que aquellos con valores de IDF más bajos. Además, los términos de IDF altos tienden a ser términos específicos de dominio que son menos propensos a ser conocidos por un usuario no experto, por lo tanto, comenzamos eliminándolos primero. Para la colección FSupp, las consultas se evaluaron de manera incremental con uno, dos, tres, cinco y siete términos eliminados de sus documentos relevantes correspondientes. Las consultas de descripción más largas de TREC TEMICS 151-200 también se evaluaron en la colección AP89 con uno, dos, tres, cinco y siete términos de consulta eliminados de sus documentos relevantes. Para las consultas de título de TREC más cortas, eliminamos uno, dos, tres y cinco términos de los documentos relevantes.4.4 Métricas En esta implementación del marco de evaluación, elegimos tres métricas para comparar el rendimiento del sistema IR: precisión promedio media (MAP), precisión en 10 documentos (P10) y recuperación en 1000 documentos. Aunque estas son las métricas que elegimos para demostrar este marco, cualquier métrica IR apropiada podría usarse dentro del marco.5. Los resultados 5.1 FSUPP COLECCIÓN Las Figuras 1, 2 y 3 muestran el rendimiento (en términos de MAP, P10 y retiro, respectivamente) para los cuatro motores de búsqueda en la colección FSUPP. Como se esperaba, el rendimiento de los sistemas IR de solo palabra clave, QL y OKAPI, cae rápidamente a medida que los términos de consulta se eliminan de los documentos relevantes en la colección. El rendimiento de Okapi con comentarios (Okapi FB) es algo sorprendente en el que en la colección original (es decir, antes de la eliminación del término de consulta), su rendimiento es peor que el de Okapi sin comentarios sobre las tres medidas. TCS supera la línea de base de palabras clave QL en cada medida, excepto el mapa en la colección original (es decir, antes de eliminar cualquier término de consulta). Debido a que TCS emplea la expansión de consultas implícitas utilizando una base de conocimiento específica de dominio externo, es menos sensible a la eliminación de términos (es decir, desajuste) que el OKAPI FB, que se basa en términos de los documentos mejor clasificados recuperados mediante una búsqueda de palabras clave inicial. Debido a que el rendimiento general del motor de búsqueda se mide con frecuencia en términos de MAP, y debido a que otras evaluaciones de QE a menudo solo consideran el rendimiento en toda la colección (es decir, no consideran la falta de coincidencia de término), el QE implementado en TCS se consideraría (en AN0 12 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MediaPrecision (MAP) OKAPI FB OKAPI TCS QL FSUPP: Precisión promedio media con términos de consulta Figura 1: el rendimiento de los cuatroSistemas de recuperación en la colección FSupp en términos de precisión promedio media (MAP) y en función del número de términos de consulta eliminados (el eje horizontal). 0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de documentos relevantes 0 0.050.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 PrecisionAt10Documentos (P10) OKAPI FB OKAPI TCS QL FSUPP: P10 Con términos de consulta eliminado Figura 2: El rendimiento de los cuatro sistemas de recuperación en la colección FSUPP en términos de precisión en 10 y como unfunción del número de términos de consulta eliminados (el eje horizontal).Otra evaluación) para dañar el rendimiento en la colección FSupp. Sin embargo, cuando observamos la comparación de TCS con QL cuando los términos de consulta se eliminan de los documentos relevantes, podemos ver que el QE en TCS está contribuyendo positivamente a la búsqueda.5.2 La colección AP89: utilizando la descripción consultas Figuras 4, 5 y 6 muestran el rendimiento de los cuatro sistemas IR en la colección AP89, utilizando las descripciones de temas de TREC como consultas. La diferencia más interesante entre el rendimiento en la colección FSupp y la colección AP89 es la reversión de Okapi FB y TCS. En FSUPP, TCS superó a los otros motores de manera consistente (ver Figuras 1, 2 y 3);En la colección AP89, Okapi FB es claramente el mejor artista (ver Figuras 4, 5 y 6). Esto es aún más interesante, según el hecho de que QE en Okapi FB tiene lugar después de la primera iteración de búsqueda, que 0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de documentos relevantes 0 0.1 0.2 0.3 0.4 0.6 0.7 0.8 0.80.9 1 Recuerde el Okapi FB Okapi TCS Indri Fsupp: Recuerde a 1000 documentos con términos de consulta eliminado Figura 3: El retiro (a 1000) de los cuatro sistemas de recuperación en la colección FSupp en su función del número de términos de consulta eliminado (el eje horizontal).0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Media VerageRecision (MAP) OKAPI FB OKAPI TCS QL AP89: Precisión promedio media con términos de consulta (Queras Descripción) Figura 4: MAPde los cuatro sistemas IR en la colección AP89, utilizando consultas de descripción TREC. El mapa se mide en función del número de términos de consulta eliminados.0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 PrecisionAt10Documentos (P10) OKAPI FB OKAPI TCS QL AP89: P10 Con Términos de consulta eliminados (Descripción Consultas) Figura 5: Precisión: Precisióna 10 de los cuatro sistemas IR en la colección AP89, utilizando consultas de descripción de TREC. P en 10 se mide en función del número de términos de consulta eliminados.0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (a 1000) de los cuatro sistemas IR en la colección AP89, utilizando consultas de descripción TREC y en función del número de términos de consulta eliminados.Esperamos ser discapacitados cuando se eliminen los términos de la consulta. Mirando P10 en la Figura 5, podemos ver que TCS y Okapi FB obtienen de manera similar en P10, comenzando en el punto donde se elimina un término de consulta de los documentos relevantes. En dos términos de consulta eliminados, TCS comienza a superar a Okapi FB. Si modela esto en términos de usuarios expertos versus no expertos, podríamos concluir que TCS podría ser un mejor motor de búsqueda para que los no expertos se utilicen en la colección AP89, mientras que Okapi FB sería mejor para un buscador de expertos. Es interesante observar que en cada métrica para las consultas de descripción AP89, TCS funciona más mal que todos los demás sistemas en la colección original, pero supera rápidamente los sistemas de referencia y se acerca al rendimiento de OKAPI FBS a medida que se eliminan los términos. Este es nuevamente un caso en el que el rendimiento de un sistema en toda la colección no es necesariamente indicativo de cómo maneja el desajuste del término del documento de consulta.5.3 La colección AP89: Uso de las consultas de título Figuras 7, 8 y 9 muestran el rendimiento de los cuatro sistemas IR en la colección AP89, utilizando los títulos de temas de TREC como consultas. Al igual que con las consultas de descripción AP89, Okapi FB es nuevamente el mejor desempeño de los cuatro sistemas en la evaluación. Como antes, el rendimiento de los sistemas OKAPI y QL, los sistemas de referencia que no son de QE, se degrada bruscamente a medida que se eliminan los términos de consulta. En las consultas más cortas, TCS parece tener más dificultades para ponerse al día con el rendimiento de Okapi FB a medida que se eliminan los términos. Quizás el resultado más interesante de nuestra evaluación es que, aunque las líneas de base de solo palabras clave funcionaban de manera consistente y como se esperaba en ambas colecciones con respecto a la eliminación de términos de consulta de documentos relevantes, el rendimiento de los motores que implementan técnicas QE diferían dramáticamente entre las colecciones.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MediaSaverageRecision (MAP) OKAPI FB OKAPI TCS QL AP89: Precisión promedio media con términos de consulta (Título Queries) Figura 7: Mapa deLos cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.4 0.45 0.5 Precisión10Documentos (P10) OKAPI FB OKAPI TCS QL AP89: P10 Con Términos de consulta eliminados (Consultas de título) Figura 8: Precisión AT10 de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recuerde Okapi FB Okapi TCS QL AP89: Recuerde a 1000 documentos con términos de consulta eliminados (consultas de título) Figura 9: Recuerdos (a 1000 a 1000) de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.6. Discusión La intuición detrás de este marco de evaluación es medir el grado en que varias técnicas de QE superan el desajuste de términos entre consultas y documentos relevantes. En general, es fácil evaluar el rendimiento general de diferentes técnicas para QE en comparación entre sí o contra una variante que no es de QE en cualquier colección de prueba completa. Tal enfoque nos dice qué sistemas funcionan mejor en una colección de prueba completa, pero no mide la capacidad de una técnica QE particular para recuperar documentos relevantes a pesar de la falta de coincidencia de término parcial o completo entre consultas y documentos relevantes. Una evaluación sistemática de los sistemas IR como se describe en este documento es útil no solo con respecto a la medición del éxito general o el fracaso de técnicas de QE particulares en presencia de desajuste del término de documento de consulta, sino que también proporciona información sobre cómo será un sistema IR particularRealice cuando los usuarios expertos versus no expertos en una colección en particular. Cuanto menos conoce el usuario sobre el dominio de la recopilación de documentos en la que están buscando, es probable que sea más prevaleciente desajuste del término del documento de consulta. Esta distinción es especialmente relevante en el caso de que la colección de pruebas es específica del dominio (es decir, médica o legal, en oposición a un dominio más general, como las noticias), donde la distinción entre expertos y no expertos puede ser más marcada. Por ejemplo, un no experto en el dominio médico podría buscar tos ferina, pero los documentos relevantes podrían contener el término médico. Dado que los términos de consulta están enmascarados solo en los documentos relevantes, este marco de evaluación está realmente sesgado contra la recuperación de documentos relevantes. Esto se debe a que los documentos no relevantes también pueden contener términos de consulta, lo que puede hacer que un sistema de recuperación clasifique dichos documentos más altos de lo que hubiera sido antes de que los términos se enmascararon en documentos relevantes. Aún así, creemos que este es un escenario más realista que eliminar los términos de todos los documentos, independientemente de la relevancia. El grado en que una técnica QE se adapta bien a una colección particular se puede evaluar en términos de su capacidad para aún encontrar los documentos relevantes, incluso cuando faltan términos de consulta, a pesar del sesgo de este enfoque contra los documentos relevantes. Sin embargo, dado que Okapi FB y TCS superaron entre sí en dos conjuntos de recolección diferentes, probablemente esté justificada una mayor investigación sobre el grado de compatibilidad entre el enfoque de expansión de QE y la recolección de objetivos. Además, la investigación de otras estrategias de eliminación de términos podría proporcionar información sobre el comportamiento de las diferentes técnicas QE y su impacto general en la experiencia del usuario. Como se mencionó anteriormente, nuestra elección de la estrategia de eliminación del término fue motivada por (1) nuestro deseo de ver el mayor impacto en el rendimiento del sistema a medida que se eliminan los términos y (2) porque los altos términos de las FDI, en nuestro contexto de dominio, tienen más probabilidades de serDominio específico, que nos permite comprender mejor el rendimiento de un sistema IR experimentado por usuarios expertos y no expertos. Aunque no se intenta en nuestros experimentos, otra aplicación de este marco de evaluación sería eliminar los términos de consulta individualmente, en lugar de incrementalmente, para analizar qué términos (o posiblemente qué tipos de términos) están siendo ayudados más por una técnica QE en una colección de pruebas particular.. Esto podría conducir a una idea de cuándo deberíamos y no debemos aplicarse. Este marco de evaluación nos permite ver cómo funcionan los sistemas IR en presencia de desajuste del término del documento de consulta. En otras evaluaciones, el rendimiento de un sistema se mide solo en toda la colección, en la que no se conoce el grado de desajuste de documentos de consulta a plazo. Al introducir sistemáticamente este desajuste, podemos ver que incluso si un sistema IR no es el mejor desempeño en toda la colección, su rendimiento puede ser más robusto para el desajuste de términos de documentos de consulta que otros sistemas. Tal robustez hace que un sistema sea más fácil de usar, especialmente para usuarios no expertos. Este artículo presenta un marco novedoso para la evaluación del sistema IR, cuyas aplicaciones son numerosas. Los resultados presentados en este documento no están destinados a ser exhaustivos o completamente representativos de las formas en que se podría aplicar esta evaluación. Sin duda, hay mucho trabajo futuro que podría hacerse utilizando este marco. Además de observar el rendimiento promedio de los sistemas IR, los resultados de las consultas individuales podrían examinarse y compararse más de cerca, tal vez dando más información sobre la clasificación y la predicción de consultas difíciles, o tal vez mostrando qué técnicas QE mejoran (o degradan) la consulta individualrendimiento bajo diferentes grados de coincidencia de término de consultas. De hecho, este marco también se beneficiaría de más pruebas en una colección más grande.7. Conclusión El marco de evaluación propuesto nos permite medir el grado en que los diferentes sistemas de IR superan (o no superan) el malhechor del término entre consultas y documentos relevantes. Las evaluaciones de los sistemas IR que emplean QE realizados solo en toda la colección no tienen en cuenta que el propósito de QE es mitigar los efectos del desajuste a término en la recuperación. Al eliminar sistemáticamente los términos de consulta de los documentos relevantes, podemos medir el grado en que QE contribuye a una búsqueda al mostrar la diferencia entre el rendimiento de un sistema QE y su línea de base de palabras clave cuando los términos de consulta se han eliminado de documentos relevantes conocidos. Además, podemos modelar el comportamiento de usuarios expertos versus no expertos manipulando la cantidad de desajuste de término de documento de consulta introducido en la colección. El marco de evaluación propuesto en este documento es atractivo por varias razones. Lo más importante es que proporciona una manera controlada para medir el rendimiento de QE con respecto al desajuste del término del documento de consulta. Además, este marco aprovecha y estira la cantidad de información que podemos obtener de las colecciones de pruebas existentes. Además, este marco de evaluación no es específico de la métrica: se puede obtener información en términos de cualquier métrica (mapa, p@10, etc.) para evaluar un sistema IR de esta manera. También se debe tener en cuenta que este marco es generalizable para cualquier sistema IR, ya que evalúa qué tan bien los sistemas IR evalúan las necesidades de información de los usuarios como lo representan sus consultas. Un sistema IR que es fácil de usar debe ser bueno para recuperar documentos que sean relevantes para las necesidades de información de los usuarios, incluso si las consultas proporcionadas por los usuarios no contienen las mismas palabras clave que los documentos relevantes.8. Referencias [1] Amati, G., C. Carpineto y G. Romano. Dificultad de consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), pp. 127-137.[2] Berger, A. y J.D. Lafferty.1999. Recuperación de información como traducción estadística. En investigación y desarrollo en recuperación de información, páginas 222-229.[3] Billerbeck, B., F. Scholer, H. E. Williams y J. Zobel.2003. Expansión de consulta utilizando consultas asociadas. En Actas de CIKM 2003, pp. 2-9.[4] Billerbeck, B. y J. Zobel.2003. Cuando la expansión de la consulta falla. En Actas de Sigir 2003, pp. 387-388.[5] Billerbeck, B. y J. Zobel.2004. Expansión de consultas de consulta: un examen del comportamiento y los parámetros. En Actas de la 15ª Conferencia de la Base de datos de Australasia (ADC2004), pp. 69-76.[6] Billerbeck, B. y J. Zobel.2005. Expansión del documento versus expansión de consulta para la recuperación ad-hoc. En Actas del Décimo Simposio de Computación de Documentos de Australia.[7] Buckley, C. y E.M. Voorhees.2000. Evaluación de la estabilidad de la medida de evaluación. En Actas de Sigir 2000, pp. 33-40.[8] Buckley, C. y E.M. Voorhees.2004. Evaluación de recuperación con información incompleta. En Actas de Sigir 2004, pp. 25-32.[9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg.2006. ¿Qué hace que una consulta sea difícil? En Actas de Sigir 2006, pp. 390-397.[10] Carpineto, C., R. Mori y G. Romano.1998. Selección de término informativo para la expansión automática de consultas. En la séptima conferencia de recuperación de texto, pp.363: 369.[11] Carterette, B. y J. Allan.2005. Colecciones de pruebas incrementales. En Actas de CIKM 2005, pp. 680-687.[12] Carterette, B., J. Allan y R. Sitaraman.2006. Colecciones de pruebas mínimas para la evaluación de recuperación. En Actas de Sigir 2006, pp. 268-275.[13] Cormack, G.V., C. R. Palmer y C.L. Clarke.1998. Construcción eficiente de grandes colecciones de pruebas. En Actas de Sigir 1998, pp. 282-289.[14] Cormack, G. y T.R. Lynam.2006. Precisión estadística de la evaluación de recuperación de información. En Actas de Sigir 2006, pp. 533-540.[15] Cronen-Townsend, S., Y. Zhou y W.B. Granja pequeña.2004. Un marco de modelado de idiomas para la expansión de consultas selectivas, informe técnico de CIIR.[16] Efthimiadis, E.N. Expansión de consulta.1996. En Martha E. Williams (ed.), Revisión anual de Sistemas de Información y Tecnología (ARIST), V31, pp 121-187. [17] Evans, D.A.y Lefferts, R.G.1995. Experimentos de Clarit-TREC. Procesamiento y gestión de la información.31 (3): 385-295.[18] Fang, H. y C.X. Zhai.2006. Matriota de término semántico en enfoques axiomáticos para la recuperación de información. En Actas de Sigir 2006, pp. 115-122.[19] Gao, J., J. Nie, G. Wu y G. Cao.2004. Modelo de lenguaje de dependencia para la recuperación de información. En Actas de Sigir 2004, pp. 170-177.[20] Harman, D.K.1992. Comentarios de relevancia revisitado. En Actas de ACM Sigir 1992, pp. 1-10.[21] Harman, D.K., ed.1993. La primera conferencia de recuperación de texto (TREC-1): 1992. [22] Harman, D.K., ed.1994. La segunda conferencia de recuperación de texto (TREC-2): 1993. [23] Harman, D.K., ed.1995. La tercera conferencia de recuperación de texto (TREC-3): 1994. [24] Harman, D.K., 1998. Hacia la expansión de consultas interactivas. En Actas de Sigir 1998, pp. 321-331.[25] Hofmann, T. 1999. Indexación semántica latente probabilística. En Actas de Sigir 1999, pp 50-57.[26] Jing, Y. y W.B. Granja pequeña.1994. El Tesauro de la Asociación para la recuperación de información. En Actas de Riao 1994, pp. 146-160 [27] Lu, X.A.y R.B. Keefer. Expansión/reducción de consulta y su impacto en la efectividad de la recuperación. En: D.K. Harman, ed. La tercera conferencia de recuperación de texto (TREC-3). Gaithersburg, MD: Instituto Nacional de Normas y Tecnología, 1995,231-239.[28] McNamee, P. y J. Mayfield.2002. Comparación de técnicas de expansión de consultas de consulta cruzada mediante la degradación de los recursos de traducción. En Actas de Sigir 2002, pp. 159-166.[29] Mitra, M., A. Singhal y C. Buckley.1998. Mejora de la expansión automática de consultas. En Actas de Sigir 1998, pp. 206-214.[30] Peat, H. J. y P. Willett.1991. Las limitaciones de los datos de concurrencia de término para la expansión de la consulta en los sistemas de recuperación de documentos. Journal of the American Society for Information Science, 42 (5): 378-383.[31] Ponte, J.M. y W.B. Granja pequeña.1998. Un enfoque de modelado de idiomas para la recuperación de información. En Actas de Sigir 1998, pp.275-281.[32] Qiu Y. y Frei H. 1993. Expansión de consultas basada en el concepto. En Actas de Sigir 1993, pp. 160-169.[33] Robertson, S. 2006. En GMAP y otras transformaciones. En Actas de CIKM 2006, pp. 78-83.[34] Robertson, S.E.y K. Sparck Jones.1976. Ponderación de relevancia de los términos de búsqueda. Journal of the American Society for Information Science, 27 (3): 129-146.[35] Robertson, S.E., S. Walker, S. Jones, M.M. Hancock-Beaulieu y M. Gatford.1994. Okapi en TREC-2. En D.K. Harman (ed).1994. La segunda conferencia de recuperación de texto (TREC-2): 1993, pp. 21-34.[36] Robertson, S.E., S. Walker, S. Jones, M.M. Hancock-Beaulieu y M. Gatford.1995. Okapi en TREC-3. En D.K. Harman (ed).1995. La tercera conferencia de recuperación de texto (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J.1971. Comentarios de relevancia en la recuperación de información. En G. Salton (ed.), El sistema de recuperación inteligente. Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323.[38] Salton, G. 1968. Organización y recuperación de información automática. McGraw-Hill.[39] Salton, G. 1971. El sistema de recuperación inteligente: experimentos en el procesamiento automático de documentos. Englewood Cliffs NJ;Prentice Hall.[40] Salton, g.1980. Constructor de clase de término automático utilizando Relevance-A Resumen del trabajo en la clasificación de pseudo automático. Procesamiento y gestión de la información.16 (1): 1-15.[41] Salton, G. y C. Buckley.1988. Sobre el uso de métodos de activación de propagación en la recuperación de información automática. En Actas de Sigir 1998, pp. 147-160.[42] Sanderson, M. 1994. La desambiguación del sentido de la palabra y la recuperación de la información. En Actas de Sigir 1994, pp. 161-175.[43] Sanderson, M. y H. Joho.2004. Formando colecciones de pruebas sin agrupación de sistemas. En Actas de Sigir 2004, pp. 186-193.[44] Sanderson, M. y Zobel, J. 2005. Evaluación del sistema de recuperación de información: esfuerzo, sensibilidad y confiabilidad. En Actas de Sigir 2005, pp. 162-169.[45] Smeaton, A.F. y C.J. Van Rijsbergen.1983. Los efectos de recuperación de la expansión de la consulta en un sistema de recuperación de documentos de retroalimentación. Revista de computadora.26 (3): 239-246.[46] Song, F. y W.B. Granja pequeña.1999. Un modelo de idioma general para la recuperación de información. En Actas de la Octava Conferencia Internacional sobre Gestión de Información y Conocimiento, páginas 316-321.[47] Sparck Jones, K. 1971. Clasificación automática de palabras clave para recuperación de información. Londres: Butterworths.[48] Terra, E. y C. L. Clarke.2004. Puntuación de términos faltantes en tareas de recuperación de información. En Actas de CIKM 2004, pp. 50-58.[49] Tortuga, Howard.1994. Lenguaje natural vs. Evaluación de consultas booleanas: una comparación del rendimiento de la recuperación. En Actas de Sigir 1994, pp. 212-220.[50] Voorhees, E.M. 1994a. Al expandir los vectores de consulta con palabras relacionadas con léxicamente. En Harman, D. K., ed. Conferencia de recuperación de texto (TREC-1): 1992. [51] Voorhees, E.M. 1994b. Expansión de consulta utilizando relaciones léxicas semánticas. En Actas de Sigir 1994, pp. 61-69.",
    "original_sentences": [
        "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
        "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
        "Query Expansion (QE) is one method for dealing with term mismatch.",
        "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
        "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
        "In this paper, we propose a new approach for evaluating query expansion techniques.",
        "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
        "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
        "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
        "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
        "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
        "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
        "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
        "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
        "IR evaluations are comparative in nature (cf.",
        "TREC).",
        "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
        "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
        "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
        "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
        "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
        "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
        "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
        "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
        "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
        "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
        "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
        "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
        "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
        "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
        "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
        "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
        "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
        "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
        "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
        "In addition, the IR research community has given attention to differences between the performance of individual queries.",
        "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
        "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
        "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
        "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
        "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
        "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
        "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
        "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
        "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
        "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
        "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
        "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
        "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
        "This removal process affects only the relevant documents in the search collection.",
        "The queries themselves remain unaltered.",
        "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
        "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
        "Notice that, for a given query, only relevant documents are modified.",
        "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
        "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
        "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
        "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
        "It does not in any way change the conceptual relevancy of the documents.",
        "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
        "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
        "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
        "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
        "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
        "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
        "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
        "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
        "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
        "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
        "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
        "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
        "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
        "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
        "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
        "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
        "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
        "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
        "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
        "Translation probabilities for QE [2] are calculated from these large external corpora.",
        "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
        "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
        "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
        "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
        "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
        "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
        "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
        "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
        "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
        "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
        "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
        "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
        "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
        "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
        "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
        "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
        "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
        "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
        "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
        "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
        "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
        "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
        "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
        "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
        "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
        "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
        "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
        "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
        "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
        "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
        "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
        "At two query terms removed, TCS starts outperforming Okapi FB.",
        "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
        "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
        "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
        "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
        "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
        "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
        "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
        "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
        "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
        "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
        "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
        "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
        "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
        "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
        "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
        "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
        "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
        "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
        "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
        "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
        "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
        "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
        "This could lead to insight as to when QE should and should not be applied.",
        "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
        "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
        "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
        "Such robustness makes a system more user-friendly, especially to non-expert users.",
        "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
        "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
        "To be sure, there is much future work that could be done using this framework.",
        "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
        "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
        "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
        "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
        "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
        "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
        "The evaluation framework proposed in this paper is attractive for several reasons.",
        "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
        "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
        "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
        "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
        "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
        "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
        "Query difficulty, robustness and selective application of query expansion.",
        "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
        "Lafferty. 1999.",
        "Information retrieval as statistical translation.",
        "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
        "Query expansion using associated queries.",
        "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
        "When Query Expansion Fails.",
        "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
        "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
        "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
        "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
        "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
        "Evaluating Evaluation Measure Stability.",
        "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
        "Retrieval evaluation with incomplete information.",
        "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
        "What Makes A Query Difficult?",
        "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
        "Informative Term Selection for Automatic Query Expansion.",
        "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
        "Incremental Test Collections.",
        "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
        "Minimal Test Collections for Retrieval Evaluation.",
        "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
        "Clarke. 1998.",
        "Efficient Construction of Large Test Collections.",
        "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
        "Lynam. 2006.",
        "Statistical Precision of Information Retrieval Evaluation.",
        "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
        "Croft. 2004.",
        "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
        "Query Expansion. 1996.",
        "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
        "CLARIT-TREC Experiments.",
        "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
        "Zhai. 2006.",
        "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
        "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
        "Dependence language model for information retrieval.",
        "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
        "Relevance feedback revisited.",
        "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
        "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
        "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
        "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
        "Towards Interactive Query Expansion.",
        "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
        "Probabilistic latent semantic indexing.",
        "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
        "Croft. 1994.",
        "The Association Thesaurus for Information Retrieval.",
        "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
        "Keefer.",
        "Query expansion/reduction and its impact on retrieval effectiveness.",
        "In: D.K.",
        "Harman, ed.",
        "The Third Text REtrieval Conference (TREC-3).",
        "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
        "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
        "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
        "Improving Automatic Query Expansion.",
        "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
        "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
        "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
        "Croft. 1998.",
        "A language modeling approach to information retrieval.",
        "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
        "Concept based query expansion.",
        "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
        "On GMAP - and other transformations.",
        "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
        "Relevance Weighting of Search Terms.",
        "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
        "Hancock-Beaulieu, and M. Gatford. 1994.",
        "Okapi at TREC-2.",
        "In D.K.",
        "Harman (ed). 1994.",
        "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
        "Hancock-Beaulieu, and M. Gatford. 1995.",
        "Okapi at TREC-3.",
        "In D.K.",
        "Harman (ed). 1995.",
        "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
        "Relevance feedback in information retrieval.",
        "In G. Salton (Ed. ), The SMART Retrieval System.",
        "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
        "Automatic Information Organization and Retrieval.",
        "McGraw-Hill. [39] Salton, G. 1971.",
        "The SMART Retrieval System: Experiments in Automatic Document Processing.",
        "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
        "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
        "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
        "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
        "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
        "Word sense disambiguation and information retrieval.",
        "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
        "Forming test collections with no system pooling.",
        "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
        "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
        "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
        "Van Rijsbergen. 1983.",
        "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
        "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
        "Croft. 1999.",
        "A general language model for information retrieval.",
        "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
        "Automatic Keyword Classification for Information Retrieval.",
        "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
        "Scoring missing terms in information retrieval tasks.",
        "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
        "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
        "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
        "On Expanding Query Vectors with Lexically Related Words.",
        "In Harman, D. K., ed.",
        "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
        "Query Expansion Using Lexical-Semantic Relations.",
        "In Proceedings of SIGIR 1994, pp. 61-69."
    ],
    "error_count": 0,
    "keys": {
        "query expansion": {
            "translated_key": "expansión de la consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating <br>query expansion</br>: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "<br>query expansion</br> (QE) is one method for dealing with term mismatch.",
                "IR systems implementing <br>query expansion</br> are typically evaluated by executing each query twice, with and without <br>query expansion</br>, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating <br>query expansion</br> techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without <br>query expansion</br>, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "<br>query expansion</br> (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit <br>query expansion</br> occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit <br>query expansion</br> can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the <br>query expansion</br> phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different <br>query expansion</br> techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement <br>query expansion</br> techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit <br>query expansion</br> using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of <br>query expansion</br>.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "<br>query expansion</br> using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When <br>query expansion</br> Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning <br>query expansion</br>: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus <br>query expansion</br> for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic <br>query expansion</br>.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective <br>query expansion</br>, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "<br>query expansion</br>. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive <br>query expansion</br>.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "<br>query expansion</br>/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language <br>query expansion</br> Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic <br>query expansion</br>.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for <br>query expansion</br> in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based <br>query expansion</br>.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of <br>query expansion</br> on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "<br>query expansion</br> Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un nuevo enfoque para evaluar la \"expansión de la consulta\": el término del documento de consulta MISMATCH TONya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN Tonya.custis@thomson.com Khalid al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, Mn Khalid.al-kofahi@thomson.com Resumen La efectividad de los sistemas de recuperación de información (IR) está influenciado por el grado de superposición de términos entre consultas de usuarios y documentos relevantes.",
                "La \"expansión de la consulta\" (QE) es un método para lidiar con el desajuste de términos.",
                "Los sistemas IR que implementan la \"expansión de la consulta\" se evalúan típicamente ejecutando cada consulta dos veces, con y sin \"expansión de la consulta\", y luego comparando los dos conjuntos de resultados.",
                "En este documento, proponemos un nuevo enfoque para evaluar técnicas de \"expansión de consultas\".",
                "Del mismo modo, los sistemas IR con capacidades QE se evalúan típicamente ejecutando cada búsqueda dos veces, una y otra sin \"expansión de la consulta\", y luego comparando los dos conjuntos de resultados.",
                "La \"expansión de la consulta\" (QE) es una técnica utilizada en IR para mejorar el rendimiento de la búsqueda al aumentar la probabilidad de superposición de términos (ya sea explícita o implícitamente) entre consultas y documentos que son relevantes para las necesidades de información de los usuarios.",
                "La \"expansión de consulta\" explícita se produce en tiempo de ejecución, en función de los resultados de búsqueda iniciales, como es el caso con la retroalimentación de relevancia y la retroalimentación de pseudo relevancia [34, 37].",
                "La \"expansión de consulta\" implícita puede basarse en propiedades estadísticas de la recopilación de documentos, o puede depender de fuentes de conocimiento externos como un tesauro o una ontología [32, 17, 26, 50, 51, 2].",
                "A menudo, los términos de expansión agregados a una consulta en la fase de \"expansión de la consulta\" terminan perjudicando el rendimiento general de la recuperación porque introducen ruido semántico, lo que hace que el significado de la consulta se deriva.",
                "En IR cruzado, [28] explora diferentes técnicas de \"expansión de consultas\" al tiempo que degrada deliberadamente sus recursos de traducción, en lo que equivale a expandir una consulta con solo un porcentaje controlado de sus términos de traducción.",
                "De los cuatro sistemas utilizados en la evaluación, dos implementan técnicas de \"expansión de consultas\": OKAPI (con pseudo-retroalimentación para QE) y un motor de búsqueda conceptual patentado (bien, llame TCS, para Thomson Concept Search).",
                "Debido a que TCS emplea la \"expansión de consultas\" implícita utilizando una base de conocimiento específica de dominio externo, es menos sensible a la eliminación de términos (es decir, desajuste) que el OKAPI FB, que se basa en términos de los documentos mejor clasificados recuperados por una búsqueda inicial de palabras clave.",
                "Dificultad de consulta, robustez y aplicación selectiva de \"expansión de consultas\".",
                "\"Expansión de consulta\" utilizando consultas asociadas.",
                "Cuando falla la \"expansión de la consulta\".",
                "Cuestionando la \"expansión de la consulta\": un examen del comportamiento y los parámetros.",
                "Documento de expansión versus \"expansión de consulta\" para la recuperación ad-hoc.",
                "Selección de término informativo para la \"expansión de consultas\" automática.",
                "Un marco de modelado de idiomas para la \"expansión de consultas\" selectiva, informe técnico de CIIR.[16] Efthimiadis, E.N.",
                "\"Expansión de consulta\".1996.",
                "Hacia la \"expansión de consulta\" interactiva.",
                "\"Expansión de consulta\"/reducción y su impacto en la efectividad de la recuperación.",
                "Comparación de técnicas de \"expansión de consulta\" cruzadas mediante la degradación de los recursos de traducción.",
                "Mejora de la \"expansión de consulta\" automática.",
                "Las limitaciones de los datos de concurrencia de término para la \"expansión de la consulta\" en los sistemas de recuperación de documentos.",
                "\"Expansión de consulta\" basada en el concepto.",
                "Los efectos de recuperación de la \"expansión de la consulta\" en un sistema de recuperación de documentos de retroalimentación.",
                "\"Expansión de consulta\" utilizando relaciones léxicas semánticas."
            ],
            "translated_text": "",
            "candidates": [
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de consultas",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de consulta",
                "expansión de la consulta",
                "expansión de consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de consultas",
                "expansión de la consulta",
                "expansión de consultas",
                "expansión de la consulta",
                "expansión de consultas",
                "expansión de la consulta",
                "expansión de consultas",
                "expansión de la consulta",
                "Expansión de consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de consulta",
                "expansión de la consulta",
                "expansión de consultas",
                "expansión de la consulta",
                "expansión de consultas",
                "Expansión de consultas",
                "Expansión de consulta",
                "expansión de la consulta",
                "expansión de consulta",
                "expansión de la consulta",
                "Expansión de consulta",
                "expansión de la consulta",
                "expansión de consulta",
                "expansión de la consulta",
                "expansión de consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "Expansión de consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "expansión de la consulta",
                "Expansión de consulta"
            ],
            "error": []
        },
        "query-document term mismatch": {
            "translated_key": "disparidad del término del documento de consulta",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: <br>query-document term mismatch</br> Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "<br>query-document term mismatch</br>, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of <br>query-document term mismatch</br>.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of <br>query-document term mismatch</br>, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This <br>query-document term mismatch</br> arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming <br>query-document term mismatch</br>.",
                "If the goal of QE is to increase search performance by mitigating the effects of <br>query-document term mismatch</br>, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of <br>query-document term mismatch</br>, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome <br>query-document term mismatch</br> by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to <br>query-document term mismatch</br>. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of <br>query-document term mismatch</br>. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing <br>query-document term mismatch</br> into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of <br>query-document term mismatch</br> by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes <br>query-document term mismatch</br>.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of <br>query-document term mismatch</br>.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles <br>query-document term mismatch</br>. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of <br>query-document term mismatch</br>, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent <br>query-document term mismatch</br> is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of <br>query-document term mismatch</br>.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to <br>query-document term mismatch</br> than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of <br>query-document term mismatch</br> introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to <br>query-document term mismatch</br>.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un nuevo enfoque para evaluar la expansión de la consulta: \"Término de consulta Documento\" Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN Tonya.custis@thomson.com Khalid al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, Mn Khalid.al-kofahi@thomson.com Resumen La efectividad de los sistemas de recuperación de información (IR) está influenciado por el grado de superposición de términos entre consultas de usuarios y documentos relevantes.",
                "\"Misas de consulta del documento del documento\", ya sea parcial o total, es un hecho que debe ser tratado por los sistemas IR.",
                "Si bien esto mide un cambio general en el rendimiento, no mide directamente la efectividad de los sistemas IR para superar el problema inherente del malhechor de términos entre la consulta y los documentos relevantes, ni proporciona ninguna idea de cómo tales sistemas se comportarían en presencia de\"CONSEJO DEL Documento de consulta MISMACH\".",
                "El enfoque propuesto es atractivo porque proporciona una estimación del rendimiento del sistema bajo diversos grados de \"coincidencia de términos de documento de consulta\", utiliza colecciones de pruebas fácilmente disponibles y no requiere ningún juicio de relevancia adicional o ninguna forma de procesamiento manual.",
                "Este \"término del documento de consulta\" surge de dos fuentes: (1) la sinonimia que se encuentra en el lenguaje natural, tanto en el término como en el nivel frasal, y (2) el grado en que el usuario es un experto en la búsqueda y/otiene conocimiento experto en el dominio de la colección que se busca.",
                "Si bien este enfoque muestra qué sistema puede haber tenido un mejor desempeño en general con respecto a una colección de pruebas particular, no mide directa o sistemáticamente la efectividad de los sistemas IR para superar la \"desajuste del término de consulta\".",
                "Si el objetivo de QE es aumentar el rendimiento de la búsqueda mediante la mitigación de los efectos del \"desajuste del término del documento de consulta\", entonces el grado en que un sistema lo hace debe ser medible en la evaluación.",
                "Un método de evaluación efectivo debe medir el rendimiento de los sistemas IR bajo diversos grados de \"desajuste de término de consulta\", no solo en términos de rendimiento general en una colección en relación con otro sistema.1 Thomson Corporation desarrolla soluciones basadas en información a los mercados profesionales, incluidos los legales, financieros, de atención médica, científica e fiscal y contabilidad.",
                "Para medir que un sistema IR en particular puede superar el \"mal desagrado de término de documento de consulta\" al recuperar documentos que son relevantes para una consulta de usuarios, pero que no necesariamente contienen los términos de consulta en sí mismos, introducimos sistemáticamente el malhijo en la pruebaColección eliminando los términos de consulta de documentos relevantes conocidos.",
                "Si un método de búsqueda QE encuentra un documento que se sabe que es relevante, pero que no obstante se pierde los términos de consulta, muestra que la técnica QE es realmente robusta con respecto a la \"desajuste del término de consulta\".2.",
                "Aunque similar en la introducción de una cantidad controlada de varianza en sus colecciones de pruebas, estos trabajos difieren del trabajo que se presenta en este documento, ya que el trabajo que se presenta aquí se mide explícita y sistemáticamente la efectividad de la consulta en presencia de \"incumplimiento de término de consulta\".3.",
                "La introducción de \"desajuste del término del documento de consulta\" en la colección de pruebas de esta manera nos permite manipular el grado de desajuste del término entre documentos y consultas relevantes de manera controlada.",
                "Eliminar sistemáticamente los términos de consulta de documentos relevantes conocidos introduce una cantidad controlada de \"desajuste del término de documento de consulta\" por el cual podemos evaluar el grado en que las técnicas de QE particulares pueden recuperar documentos conceptualmente relevantes, a pesar de la falta de superposición de término real.",
                "Es de esta manera que podemos medir explícitamente el grado en que un sistema IR supera el \"desajuste del término del documento de consulta\".",
                "La eliminación de aditivos incrementales de los términos de consulta de documentos relevantes permite que la evaluación muestre el grado en que el rendimiento del sistema IR se degrada a medida que faltan más y más términos de consulta, lo que aumenta el grado de \"desajuste de término de consulta\".",
                "Este es nuevamente un caso en el que el rendimiento de un sistema en toda la colección no es necesariamente indicativo de cómo maneja el \"desajuste del término del documento de consulta\".5.3 La colección AP89: Uso de las consultas de título Figuras 7, 8 y 9 muestran el rendimiento de los cuatro sistemas IR en la colección AP89, utilizando los títulos de temas de TREC como consultas.",
                "Una evaluación sistemática de los sistemas IR como se describe en este documento es útil no solo con respecto a la medición del éxito general o el fracaso de técnicas de QE particulares en presencia de \"desajuste de término de documento de consulta\", sino que también proporciona información sobre cómo un IR particular IR IREl sistema funcionará cuando use usuarios expertos versus no expertos en una colección en particular.",
                "Cuanto menos sepa un usuario sobre el dominio de la recopilación de documentos en la que está buscando, es probable que sea más frecuente \"Misasto de término de documento de consulta\".",
                "Este marco de evaluación nos permite ver cómo funcionan los sistemas IR en presencia de \"desajuste del término de consulta\".",
                "Al introducir sistemáticamente este desajuste, podemos ver que incluso si un sistema IR no es el mejor desempeño en toda la colección, su rendimiento puede ser más robusto para \"desajuste de término de consulta\" que otros sistemas.",
                "Además, podemos modelar el comportamiento de usuarios expertos versus no expertos manipulando la cantidad de \"desajuste de término de documento de consulta\" introducido en la colección.",
                "Lo más importante es que proporciona una manera controlada para medir el rendimiento de QE con respecto a la \"desajuste del término del documento de consulta\"."
            ],
            "translated_text": "",
            "candidates": [
                "coincidencia de documento de consultas no coincidiendo",
                "Término de consulta Documento",
                "coincidencia de documento de consultas no coincidiendo",
                "Misas de consulta del documento del documento",
                "coincidencia de documento de consultas no coincidiendo",
                "CONSEJO DEL Documento de consulta MISMACH",
                "coincidencia de documento de consulta no coincidiendo",
                "coincidencia de términos de documento de consulta",
                "coincidencia de documento de consulta no coincidiendo",
                "término del documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste del término de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste del término del documento de consulta",
                "coincidencia de documento de consulta no coincidiendo",
                "desajuste de término de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "mal desagrado de término de documento de consulta",
                "CONSIGURA DEL Documento de consulta No coincide",
                "desajuste del término de consulta",
                "CONSEJO DEL Documento de consulta MISMACH",
                "incumplimiento de término de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste del término del documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste del término de documento de consulta",
                "coincidencia de documento de consulta no coincidiendo",
                "desajuste del término del documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste de término de consulta",
                "coincidencia de documento de consulta no coincidiendo",
                "desajuste del término del documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste de término de documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "Misasto de término de documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste del término de consulta",
                "coincidencia de documento de consulta no coincidiendo",
                "desajuste de término de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste de término de documento de consulta",
                "coincidencia de documento de consultas no coincidiendo",
                "desajuste del término del documento de consulta"
            ],
            "error": []
        },
        "information retrieval": {
            "translated_key": "recuperación de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of <br>information retrieval</br> (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop <br>information retrieval</br> systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on <br>information retrieval</br> (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "<br>information retrieval</br> as statistical translation.",
                "In Research and Development in <br>information retrieval</br>, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of <br>information retrieval</br> Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to <br>information retrieval</br>.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for <br>information retrieval</br>.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for <br>information retrieval</br>.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to <br>information retrieval</br>.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in <br>information retrieval</br>.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic <br>information retrieval</br>.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and <br>information retrieval</br>.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "<br>information retrieval</br> System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for <br>information retrieval</br>.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for <br>information retrieval</br>.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in <br>information retrieval</br> tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un nuevo enfoque para evaluar la expansión de la consulta: consulta Documento Término Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN Tonya.custis@thomson.com Khalid al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, Mn Khalid.al--kofahi@thomson.com Resumen La efectividad de los sistemas de \"recuperación de información\" (IR) está influenciada por el grado de superposición de términos entre las consultas de los usuarios y los documentos relevantes.",
                "Por lo tanto, es importante desarrollar sistemas de \"recuperación de información\" que sean robustos con respecto a las variaciones de lenguaje o el malhechado de término entre consultas y documentos relevantes.",
                "En Actas de la 25ª Conferencia Europea sobre \"Recuperación de información\" (ECIR 2004), pp. 127-137.[2] Berger, A. y J.D.",
                "\"Recuperación de información\" como traducción estadística.",
                "En investigación y desarrollo en \"Recuperación de información\", páginas 222-229.[3] Billerbeck, B., F. Scholer, H. E. Williams y J. Zobel.2003.",
                "Precisión estadística de la evaluación de \"recuperación de información\".",
                "Matriota de término semántico en enfoques axiomáticos para la \"recuperación de información\".",
                "Modelo de lenguaje de dependencia para \"recuperación de información\".",
                "La Asociación Tesauro para \"Recuperación de información\".",
                "Un enfoque de modelado de idiomas para la \"recuperación de información\".",
                "Comentarios de relevancia en \"Recuperación de información\".",
                "Sobre el uso de métodos de activación de propagación en la \"recuperación de información\" automática.",
                "Desambiguación del sentido de la palabra y \"recuperación de información\".",
                "Evaluación del sistema \"Recuperación de información\": esfuerzo, sensibilidad y confiabilidad.",
                "Un modelo de idioma general para \"recuperación de información\".",
                "Clasificación automática de palabras clave para \"recuperación de información\".",
                "Puntuación de términos faltantes en tareas de \"recuperación de información\"."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "Recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "Recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información",
                "recuperación de información"
            ],
            "error": []
        },
        "information search": {
            "translated_key": "Búsqueda de información",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: <br>information search</br> and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: \"Búsqueda de información\" y recuperación de Términos generales Medición, Experimentación 1."
            ],
            "translated_text": "",
            "candidates": [
                "Búsqueda de información",
                "Búsqueda de información"
            ],
            "error": []
        },
        "document expansion": {
            "translated_key": "expansión de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "<br>document expansion</br> versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Expansión de documentos\" versus expansión de consulta para la recuperación ad-hoc."
            ],
            "translated_text": "",
            "candidates": [
                "expansión de documentos",
                "Expansión de documentos"
            ],
            "error": []
        },
        "document processing": {
            "translated_key": "Procesamiento de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this evaluation framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic <br>document processing</br>.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El sistema de recuperación inteligente: experimentos en el \"procesamiento de documentos\" automático."
            ],
            "translated_text": "",
            "candidates": [
                "Procesamiento de documentos",
                "procesamiento de documentos"
            ],
            "error": []
        },
        "relevant document": {
            "translated_key": "documento relevante",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and <br>relevant document</br>s.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and <br>relevant document</br>s, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing <br>relevant document</br>s may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing <br>relevant document</br>s when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and <br>relevant document</br>s.",
                "During our work on developing such systems, we concluded that current evaluation methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in evaluation.",
                "An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known <br>relevant document</br>s.",
                "Because we are purposely inducing term mismatch between the queries and known <br>relevant document</br>s in our test collections, the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving <br>relevant document</br>s despite partial or total term mismatch between queries and <br>relevant document</br>s should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The evaluation of IR systems has received much attention in the research community, both in terms of developing test collections for the evaluation of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and <br>relevant document</br>s, our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their <br>relevant document</br>s, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known <br>relevant document</br>s, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between <br>relevant document</br>s and queries in a controlled manner.",
                "This removal process affects only the <br>relevant document</br>s in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the <br>relevant document</br>s), there will be no term overlap between a query and its <br>relevant document</br>s.",
                "Notice that, for a given query, only <br>relevant document</br>s are modified.",
                "Non-<br>relevant document</br>s are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-<br>relevant document</br>s sets by removing query terms from the <br>relevant document</br>s, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known <br>relevant document</br>s introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually <br>relevant document</br>s, despite a lack of actual term overlap.",
                "Removing a query term from <br>relevant document</br>s simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the <br>relevant document</br>s in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the <br>relevant document</br>s in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from <br>relevant document</br>s, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the <br>relevant document</br>s.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from <br>relevant document</br>s allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed evaluation framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the evaluation, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from <br>relevant document</br>s, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our evaluation, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding <br>relevant document</br>s.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their <br>relevant document</br>s.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the <br>relevant document</br>s. 4.4 Metrics In this implementation of the evaluation framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the <br>relevant document</br>s in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other evaluation) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the <br>relevant document</br>s, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from <br>relevant document</br>s.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the evaluation.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from <br>relevant document</br>s, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this evaluation framework is to measure the degree to which various QE techniques overcome term mismatch between queries and <br>relevant document</br>s.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve <br>relevant document</br>s despite partial or complete term mismatch between queries and <br>relevant document</br>s.",
                "A systematic evaluation of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but <br>relevant document</br>s might instead contain the medical term pertussis.",
                "Since query terms are masked only the in <br>relevant document</br>s, this evaluation framework is actually biased against retrieving <br>relevant document</br>s.",
                "This is because non-<br>relevant document</br>s may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in <br>relevant document</br>s.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the <br>relevant document</br>s, even when they are missing query terms, despite the bias of this approach against <br>relevant document</br>s.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this evaluation framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This evaluation framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system evaluation, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this evaluation could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed evaluation framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and <br>relevant document</br>s.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from <br>relevant document</br>s, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known <br>relevant document</br>s.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The evaluation framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this evaluation framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the <br>relevant document</br>s. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating Evaluation Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval evaluation with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval Evaluation.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un nuevo enfoque para evaluar la expansión de la consulta: consulta Documento Término Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN Tonya.custis@thomson.com Khalid al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, Mn Khalid.al--kofahi@thomson.com Resumen La efectividad de los sistemas de recuperación de información (IR) está influenciada por el grado de superposición de términos entre consultas de usuario y \"documento relevante\" s.",
                "Si bien esto mide un cambio general en el rendimiento, no mide directamente la efectividad de los sistemas IR para superar el tema inherente de la falta de coincidencia de término entre la consulta y el \"documento relevante\", ni proporciona ninguna idea de cómo se comportarían dichos sistemasLa presencia de la falta de desajuste del término del documento de consulta.",
                "La falta de \"documentos relevantes\" puede tener consecuencias no triviales en el resultado de un procedimiento judicial.",
                "Los abogados están especialmente preocupados por la falta de \"documentos relevantes\" al investigar un tema legal que es nuevo para ellos, ya que pueden no ser conscientes de todas las variaciones de lenguaje en tales temas.",
                "Por lo tanto, es importante desarrollar sistemas de recuperación de información que sean robustos con respecto a las variaciones de lenguaje o el malhijo entre consultas y \"documentos relevantes\" s.",
                "Para medir que un sistema IR en particular puede superar la falta de coincidencia del término del documento de consulta al recuperar documentos que son relevantes para una consulta de usuarios, pero que no necesariamente contienen los términos de consulta en sí mismos, introducimos sistemáticamente el desajuste de términos en la recopilación de pruebas deEliminar términos de consulta del \"documento relevante\" conocido s.",
                "Debido a que estamos induciendo a propósito un desajuste de términos entre las consultas y los \"documentos relevantes\" conocidos en nuestras colecciones de pruebas, el marco de evaluación propuesto puede medir la efectividad de QE de una manera que las pruebas en toda la colección no lo son.",
                "Independientemente del método, los algoritmos QE que son capaces de recuperar \"documentos relevantes\" a pesar de la falta de coincidencia parcial o total entre consultas y \"documentos relevantes\" s deberían aumentar el retiro de los sistemas IR (recuperando documentos que también se habrían perdido))como su precisión (recuperando documentos más relevantes).",
                "Al inducir deliberadamente la falta de coincidencia entre los términos en consultas y \"documentos relevantes\", nuestro marco de evaluación nos permite una manera controlada para degradar la calidad de las consultas con respecto a sus \"documentos relevantes\", y luego medir tanto el tantogrado de dificultad (inducida) de la consulta y el grado en que QE mejora el rendimiento de recuperación de la consulta degradada.",
                "Eliminamos sistemáticamente los términos de consulta de los \"documentos relevantes\" conocidos, creando versiones alternativas de una colección de pruebas que difieren solo en cuántos o qué términos de consulta se han eliminado de los documentos relevantes para una consulta en particular.",
                "La introducción de la falta de desajuste del término del documento de consulta en la colección de pruebas de esta manera nos permite manipular el grado de desajuste del término entre \"documentos relevantes\" y consultas de manera controlada.",
                "Este proceso de eliminación afecta solo el \"documento relevante\" s en la colección de búsqueda.",
                "En el caso más extremo (es decir, cuando la longitud de la consulta es menor o igual al número de términos de consulta eliminados del \"documento relevante\" s), no habrá superposición de término entre una consulta y su \"documento relevante\"s.",
                "Observe que, para una consulta dada, solo se modifican \"documentos relevantes\".",
                "Los no \"documentos relevantes\" se dejan sin cambios, incluso en el caso de que contengan términos de consulta.",
                "Aunque, en la superficie, estamos cambiando la distribución de los términos entre los conjuntos de documentos relevantes y no \"relevantes\" eliminando los términos de consulta del \"documento relevante\", no cambia la relevancia conceptual de estos documentos.",
                "Eliminar sistemáticamente los términos de consulta del \"documento relevante\" conocido introduce una cantidad controlada de coincidencia de término del documento de consulta por el cual podemos evaluar el grado en que las técnicas de QE particulares pueden recuperar el \"documento relevante\" conceptualmente, a pesar de la falta de término realsuperposición.",
                "Eliminar un término de consulta del \"documento relevante\" S simplemente enmascara la presencia de ese término de consulta en esos documentos.",
                "El marco de evaluación presentado en este documento consta de tres elementos: una colección de pruebas, C;Una estrategia para seleccionar qué términos de consulta eliminar del \"documento relevante\" s en esa colección, s;y una métrica para comparar el rendimiento de los sistemas IR, m.La recopilación de pruebas, C, consiste en una recopilación de documentos, consultas y juicios de relevancia.",
                "La estrategia, s, determina el orden y la forma en que los términos de la consulta se eliminan del \"documento relevante\" s en C. Este marco de evaluación no es específico de la métrica;Cualquier métrica (mapa, p@10, retirada, etc.) se puede utilizar para medir el rendimiento del sistema IR.",
                "De hecho, el uso de este marco estira el valor de las colecciones de pruebas existentes en que una colección se convierte en varios cuando los términos de consulta se eliminan de \"documento relevante\", aumentando así la cantidad de información que se puede evaluar en una evaluación en una colección particular.",
                "Se deben tomar dos decisiones al elegir una estrategia de eliminación S. El primero es el orden en el que S elimina los términos del \"documento relevante\" s.",
                "La eliminación aditiva incremental de los términos de consulta del \"documento relevante\" permite que la evaluación muestre el grado en que el rendimiento del sistema IR se degrada a medida que faltan más y más términos de consulta, aumentando así el grado de desajuste del término del documento de consultorio.",
                "Como resultado, esperamos que cuando los términos de la consulta se eliminen de los \"documentos relevantes\", el rendimiento de estos sistemas debería degradarse más dramáticamente que sus contrapartes que implementan QE.",
                "Para la colección FSupp, las consultas se evaluaron de manera incremental con uno, dos, tres, cinco y siete términos eliminados de su correspondiente \"documento relevante\" s.",
                "Las consultas de descripción más largas de TREC TEMICS 151-200 también se evaluaron en la colección AP89 con uno, dos, tres, cinco y siete términos de consulta eliminados de su \"documento relevante\" s.",
                "Para las consultas de título de TREC más cortas, eliminamos uno, dos, tres y cinco términos del \"documento relevante\" s.4.4 Métricas En esta implementación del marco de evaluación, elegimos tres métricas para comparar el rendimiento del sistema IR: precisión promedio media (MAP), precisión en 10 documentos (P10) y recuperación en 1000 documentos.",
                "Como se esperaba, el rendimiento de los sistemas IR solo por palabras clave, QL y OKAPI, cae rápidamente a medida que los términos de consulta se eliminan del \"documento relevante\" en la colección.",
                "Sin embargo, cuando observamos la comparación de TCS con QL cuando los términos de consulta se eliminan del \"documento relevante\", podemos ver que el QE en TCS está contribuyendo positivamente a la búsqueda.5.2 La colección AP89: utilizando la descripción consultas Figuras 4, 5 y 6 muestran el rendimiento de los cuatro sistemas IR en la colección AP89, utilizando las descripciones de temas de TREC como consultas.",
                "Mirando P10 en la Figura 5, podemos ver que TCS y Okapi FB obtienen de manera similar en P10, comenzando en el punto en que se elimina un término de consulta del \"documento relevante\" s.",
                "Quizás el resultado más interesante de nuestra evaluación es que, aunque las líneas de base de solo palabra clave se desempeñaron de manera consistente y como se esperaba en ambas colecciones con respecto a la eliminación de términos de consulta de \"documentos relevantes\", los rendimientos de los motores que implementan técnicas QE diferían dramáticamente entre las colecciones.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MediaSaverageRecision (MAP) OKAPI FB OKAPI TCS QL AP89: Precisión promedio media con términos de consulta (Título Queries) Figura 7: Mapa deLos cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.4 0.45 0.5 Precisión10Documentos (P10) OKAPI FB OKAPI TCS QL AP89: P10 Con Términos de consulta eliminados (Consultas de título) Figura 8: Precisión AT10 de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recuerde Okapi FB Okapi TCS QL AP89: Recuerde a 1000 documentos con términos de consulta eliminados (consultas de título) Figura 9: Recuerdos (a 1000 a 1000) de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.6.",
                "Discusión La intuición detrás de este marco de evaluación es medir el grado en que varias técnicas de QE superan el desajuste del término entre las consultas y el \"documento relevante\" s.",
                "Tal enfoque nos dice qué sistemas funcionan mejor en una colección de prueba completa, pero no mide la capacidad de una técnica QE particular para recuperar el \"documento relevante\" a pesar de la desajuste de término parcial o completo entre consultas y \"documentos relevantes\".",
                "Por ejemplo, un no experto en el dominio médico podría buscar tos ferina, pero el \"documento relevante\" podría contener el término médico.",
                "Dado que los términos de la consulta están enmascarados solo en el \"documento relevante\" s, este marco de evaluación en realidad está sesgado contra la recuperación del \"documento relevante\" s.",
                "Esto se debe a que los documentos no \"relevantes\" también pueden contener términos de consulta, lo que puede hacer que un sistema de recuperación clasifique dichos documentos más altos de lo que hubiera sido antes de que los términos se enmascararon en \"documentos relevantes\".",
                "El grado en que una técnica QE se adapta bien a una colección en particular puede evaluarse en términos de su capacidad para aún encontrar el \"documento relevante\", incluso cuando faltan términos de consulta, a pesar del sesgo de este enfoque contra \"relevantedocumentos.",
                "Conclusión El marco de evaluación propuesto nos permite medir el grado en que los diferentes sistemas IR superan (o no superan) el malhechor del término entre consultas y \"documento relevante\" s.",
                "Al eliminar sistemáticamente los términos de consulta de los \"documentos relevantes\", podemos medir el grado en que QE contribuye a una búsqueda al mostrar la diferencia entre el rendimiento de un sistema QE y su línea de base de forma clave cuando se han eliminado los términos de consulta del documento relevante conocido \"conocido\"\"s.",
                "Un sistema IR que es fácil de usar debe ser bueno para recuperar documentos que sean relevantes para las necesidades de información de los usuarios, incluso si las consultas proporcionadas por los usuarios no contienen las mismas palabras clave que el \"documento relevante\".8."
            ],
            "translated_text": "",
            "candidates": [
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "Documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documentos relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "relevantes",
                "documentos relevantes",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documento relevante",
                "documentos relevantes",
                "conocido",
                "Documento relevante",
                "documento relevante"
            ],
            "error": []
        },
        "evaluation": {
            "translated_key": "evaluación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN tonya.custis@thomson.com Khalid Al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, MN khalid.al-kofahi@thomson.com ABSTRACT The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents.",
                "Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems.",
                "Query Expansion (QE) is one method for dealing with term mismatch.",
                "IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets.",
                "While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch.",
                "In this paper, we propose a new approach for evaluating query expansion techniques.",
                "The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Measurement, Experimentation 1.",
                "INTRODUCTION In our domain,1 and unlike web search, it is very important for attorneys to find all documents (e.g., cases) that are relevant to an issue.",
                "Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding.",
                "Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them, as they may not be aware of all language variations in such topics.",
                "Therefore, it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents.",
                "During our work on developing such systems, we concluded that current <br>evaluation</br> methods are not sufficient for this purpose. {Whooping cough, pertussis}, {heart attack, myocardial infarction}, {car wash, automobile cleaning}, {attorney, legal counsel, lawyer} are all examples of things that share the same meaning.",
                "Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.",
                "This query-document term mismatch arises from two sources: (1) the synonymy found in natural language, both at the term and the phrasal level, and (2) the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched.",
                "IR evaluations are comparative in nature (cf.",
                "TREC).",
                "Generally, IR evaluations show how System A did in relation to System B on the same test collection based on various precision- and recall-based metrics.",
                "Similarly, IR systems with QE capabilities are typically evaluated by executing each search twice, once with and once without query expansion, and then comparing the two result sets.",
                "While this approach shows which system may have performed better overall with respect to a particular test collection, it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch.",
                "If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch, then the degree to which a system does so should be measurable in <br>evaluation</br>.",
                "An effective <br>evaluation</br> method should measure the performance of IR systems under varying degrees of query-document term mismatch, not just in terms of overall performance on a collection relative to another system. 1 Thomson Corporation builds information based solutions to the professional markets including legal, financial, health care, scientific, and tax and accounting.",
                "In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a users query, but that do not necessarily contain the query terms themselves, we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.",
                "Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections, the proposed <br>evaluation</br> framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not.",
                "If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms, it shows that QE technique is indeed robust with respect to query-document term mismatch. 2.",
                "RELATED WORK Accounting for term mismatch between the terms in user queries and the documents relevant to users information needs has been a fundamental issue in IR research for almost 40 years [38, 37, 47].",
                "Query expansion (QE) is one technique used in IR to improve search performance by increasing the likelihood of term overlap (either explicitly or implicitly) between queries and documents that are relevant to users information needs.",
                "Explicit query expansion occurs at run-time, based on the initial search results, as is the case with relevance feedback and pseudo relevance feedback [34, 37].",
                "Implicit query expansion can be based on statistical properties of the document collection, or it may rely on external knowledge sources such as a thesaurus or an ontology [32, 17, 26, 50, 51, 2].",
                "Regardless of method, QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems (by retrieving documents that would have previously been missed) as well as their precision (by retrieving more relevant documents).",
                "In practice, QE tends to improve the average overall retrieval performance, doing so by improving performance on some queries while making it worse on others.",
                "QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection [47, 45, 41, 27].",
                "Often, the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise, causing the meaning of the query to drift.",
                "As such, much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].",
                "The <br>evaluation</br> of IR systems has received much attention in the research community, both in terms of developing test collections for the <br>evaluation</br> of different systems [11, 12, 13, 43] and in terms of the utility of evaluation metrics such as recall, precision, mean average precision, precision at rank, Bpref, etc. [7, 8, 44, 14].",
                "In addition, there have been comparative evaluations of different QE techniques on various test collections [47, 45, 41].",
                "In addition, the IR research community has given attention to differences between the performance of individual queries.",
                "Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve optimal overall performance.",
                "In addition, related work on predicting query difficulty, or which queries are likely to perform poorly, has been done [1, 4, 5, 9].",
                "There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries, as is evidenced by the Robust track in the TREC competitions and new <br>evaluation</br> measures such as GMAP.",
                "GMAP (geometric mean average precision) gives more weight to the lower end of the average precision (as opposed to MAP), thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score [33].",
                "However, no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms.",
                "By purposely inducing mismatch between the terms in queries and relevant documents, our <br>evaluation</br> framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents, and then to measure the both the degree of (induced) difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query.",
                "The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance. [42] introduces into the document collection pseudowords that are ambiguous with respect to word sense, in order to measure the degree to which word sense disambiguation is useful in IR. [6] experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time.",
                "In cross-language IR, [28] explores different query expansion techniques while purposely degrading their translation resources, in what amounts to expanding a query with only a controlled percentage of its translation terms.",
                "Although similar in introducing a controlled amount of variance into their test collections, these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch. 3.",
                "METHODOLOGY In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.",
                "Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes.",
                "We systematically remove query terms from known relevant documents, creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.",
                "Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner.",
                "This removal process affects only the relevant documents in the search collection.",
                "The queries themselves remain unaltered.",
                "Query terms are removed from documents one by one, so the differences in IR system performance can be measured with respect to missing terms.",
                "In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.",
                "Notice that, for a given query, only relevant documents are modified.",
                "Non-relevant documents are left unchanged, even in the case that they contain query terms.",
                "Although, on the surface, we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents, doing so does not change the conceptual relevancy of these documents.",
                "Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.",
                "Removing a query term from relevant documents simply masks the presence of that query term in those documents.",
                "It does not in any way change the conceptual relevancy of the documents.",
                "The <br>evaluation</br> framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.",
                "The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This <br>evaluation</br> framework is not metric-specific; any metric (MAP, P@10, recall, etc.) can be used to measure IR system performance.",
                "Although test collections are difficult to come by, it should be noted that this <br>evaluation</br> framework can be used on any available test collection.",
                "In fact, using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of information that can be gained from evaluating on a particular collection.",
                "In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been expanded or not, compared in terms of some metric.",
                "In contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.",
                "It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.",
                "The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.",
                "Two decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.",
                "Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.",
                "Based on the purpose of the <br>evaluation</br> and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.",
                "Once an order for removal has been decided, a manner for term removal/masking must be decided.",
                "It must be determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).",
                "The incremental additive removal of query terms from relevant documents allows the <br>evaluation</br> to show the degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.",
                "Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 4.",
                "EXPERIMENTAL SET-UP 4.1 IR Systems We used the proposed <br>evaluation</br> framework to evaluate four IR systems on two test collections.",
                "Of the four systems used in the <br>evaluation</br>, two implement query expansion techniques: Okapi (with pseudo-feedback for QE), and a proprietary concept search engine (well call it TCS, for Thomson Concept Search).",
                "TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus (i.e., legal or news) as a knowledge source.",
                "This external knowledge source is a corpus separate from, but thematically related to, the document collection to be searched.",
                "Translation probabilities for QE [2] are calculated from these large external corpora.",
                "Okapi (without feedback) and a language model query likelihood (QL) model (implemented using Indri) are included as keyword-only baselines.",
                "Okapi without feedback is intended as an analogous baseline for Okapi with feedback, and the QL model is intended as an appropriate baseline for TCS, as they both implement language-modeling based retrieval algorithms.",
                "We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities.",
                "As a result, we expect that when query terms are removed from relevant documents, the performance of these systems should degrade more dramatically than their counterparts that implement QE.",
                "The Okapi and QL model results were obtained using the Lemur Toolkit.2 Okapi was run with the parameters k1=1.2, b=0.75, and k3=7.",
                "When run with feedback, the feedback parameters used in Okapi were set at 10 documents and 25 terms.",
                "The QL model used Jelinek-Mercer smoothing, with λ = 0.6. 4.2 Test Collections We evaluated the performance of the four IR systems outlined above on two different test collections.",
                "The two test collections used were the TREC AP89 collection (TIPSTER disk 1) and the FSupp Collection.",
                "The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries (ranging from four to twenty-two words after stop word removal) with full relevance judgments.3 The average length of documents in the FSupp Collection is 3444 words. 2 www.lemurproject.org 3 Each of the 11,953 documents was evaluated by domain experts with respect to each of the 44 queries.",
                "The TREC AP89 test collection contains 84,678 documents, averaging 252 words in length.",
                "In our <br>evaluation</br>, we used both the title and the description fields of topics 151200 as queries, so we have two sets of results for the AP89 Collection.",
                "After stop word removal, the title queries range from two to eleven words and the description queries range from four to twenty-six terms. 4.3 Query Term Removal Strategy In our experiments, we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency (IDF) with respect to the entire document collection.",
                "Terms with high IDF values tend to influence document ranking more than those with lower IDF values.",
                "Additionally, high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user, hence we start by removing these first.",
                "For the FSupp Collection, queries were evaluated incrementally with one, two, three, five, and seven terms removed from their corresponding relevant documents.",
                "The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one, two, three, five, and seven query terms removed from their relevant documents.",
                "For the shorter TREC title queries, we removed one, two, three, and five terms from the relevant documents. 4.4 Metrics In this implementation of the <br>evaluation</br> framework, we chose three metrics by which to compare IR system performance: mean average precision (MAP), precision at 10 documents (P10), and recall at 1000 documents.",
                "Although these are the metrics we chose to demonstrate this framework, any appropriate IR metrics could be used within the framework. 5.",
                "RESULTS 5.1 FSupp Collection Figures 1, 2, and 3 show the performance (in terms of MAP, P10 and Recall, respectively) for the four search engines on the FSupp Collection.",
                "As expected, the performance of the keyword-only IR systems, QL and Okapi, drops quickly as query terms are removed from the relevant documents in the collection.",
                "The performance of Okapi with feedback (Okapi FB) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its performance is worse than that of Okapi without feedback on all three measures.",
                "TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection (i.e., prior to removing any query terms).",
                "Because TCS employs implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the Okapi FB, which relies on terms from the top-ranked documents retrieved by an initial keywordonly search.",
                "Because overall search engine performance is frequently measured in terms of MAP, and because other evaluations of QE often only consider performance on the entire collection (i.e., they do not consider term mismatch), the QE implemented in TCS would be considered (in an0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL FSupp: Mean Average Precision with Query Terms Removed Figure 1: The performance of the four retrieval systems on the FSupp collection in terms of Mean Average Precision (MAP) and as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precisionat10Documents(P10) Okapi FB Okapi TCS QL FSupp: P10 with Query Terms Removed Figure 2: The performance of the four retrieval systems on the FSupp collection in terms of Precision at 10 and as a function of the number of query terms removed (the horizontal axis). other <br>evaluation</br>) to hurt performance on the FSupp Collection.",
                "However, when we look at the comparison of TCS to QL when query terms are removed from the relevant documents, we can see that the QE in TCS is indeed contributing positively to the search. 5.2 The AP89 Collection: using the description queries Figures 4, 5, and 6 show the performance of the four IR systems on the AP89 Collection, using the TREC topic descriptions as queries.",
                "The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS.",
                "On FSupp, TCS outperformed the other engines consistently (see Figures 1, 2, and 3); on the AP89 Collection, Okapi FB is clearly the best performer (see Figures 4, 5, and 6).",
                "This is all the more interesting, based on the fact that QE in Okapi FB takes place after the first search iteration, which 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall Okapi FB Okapi TCS Indri FSupp: Recall at 1000 documents with Query Terms Removed Figure 3: The Recall (at 1000) of the four retrieval systems on the FSupp collection as a function of the number of query terms removed (the horizontal axis). 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (description queries) Figure 4: MAP of the four IR systems on the AP89 Collection, using TREC description queries.",
                "MAP is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (description queries) Figure 5: Precision at 10 of the four IR systems on the AP89 Collection, using TREC description queries.",
                "P at 10 is measured as a function of the number of query terms removed. 0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC description queries, and as a function of the number of query terms removed. we would expect to be handicapped when query terms are removed.",
                "Looking at P10 in Figure 5, we can see that TCS and Okapi FB score similarly on P10, starting at the point where one query term is removed from relevant documents.",
                "At two query terms removed, TCS starts outperforming Okapi FB.",
                "If modeling this in terms of expert versus non-expert users, we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection, while Okapi FB would be best for an expert searcher.",
                "It is interesting to note that on each metric for the AP89 description queries, TCS performs more poorly than all the other systems on the original collection, but quickly surpasses the baseline systems and approaches Okapi FBs performance as terms are removed.",
                "This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch. 5.3 The AP89 Collection: using the title queries Figures 7, 8, and 9 show the performance of the four IR systems on the AP89 Collection, using the TREC topic titles as queries.",
                "As with the AP89 description queries, Okapi FB is again the best performer of the four systems in the <br>evaluation</br>.",
                "As before, the performance of the Okapi and QL systems, the non-QE baseline systems, sharply degrades as query terms are removed.",
                "On the shorter queries, TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed.",
                "Perhaps the most interesting result from our <br>evaluation</br> is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents, the performances of the engines implementing QE techniques differed dramatically between collections. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MeanAveragePrecision(MAP) Okapi FB Okapi TCS QL AP89: Mean Average Precision with Query Terms Removed (title queries) Figure 7: MAP of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Precisionat10Documents(P10) Okapi FB Okapi TCS QL AP89: P10 with Query Terms Removed (title queries) Figure 8: Precision at 10 of the four IR systems on the AP89 Collection, using TREC title queries, and as a function of the number of query terms removed. 0 1 2 3 4 5 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (title queries) Figure 9: Recall (at 1000) of the four IR systems on the AP89 Collection, using TREC title queries and as a function of the number of query terms removed. 6.",
                "DISCUSSION The intuition behind this <br>evaluation</br> framework is to measure the degree to which various QE techniques overcome term mismatch between queries and relevant documents.",
                "In general, it is easy to evaluate the overall performance of different techniques for QE in comparison to each other or against a non-QE variant on any complete test collection.",
                "Such an approach does tell us which systems perform better on a complete test collection, but it does not measure the ability of a particular QE technique to retrieve relevant documents despite partial or complete term mismatch between queries and relevant documents.",
                "A systematic <br>evaluation</br> of IR systems as outlined in this paper is useful not only with respect to measuring the general success or failure of particular QE techniques in the presence of query-document term mismatch, but it also provides insight into how a particular IR system will perform when used by expert versus non-expert users on a particular collection.",
                "The less a user knows about the domain of the document collection on which they are searching, the more prevalent query-document term mismatch is likely to be.",
                "This distinction is especially relevant in the case that the test collection is domain-specific (i.e., medical or legal, as opposed to a more general domain, such as news), where the distinction between experts and non-experts may be more marked.",
                "For example, a non-expert in the medical domain might search for whooping cough, but relevant documents might instead contain the medical term pertussis.",
                "Since query terms are masked only the in relevant documents, this <br>evaluation</br> framework is actually biased against retrieving relevant documents.",
                "This is because non-relevant documents may also contain query terms, which can cause a retrieval system to rank such documents higher than it would have before terms were masked in relevant documents.",
                "Still, we think this is a more realistic scenario than removing terms from all documents regardless of relevance.",
                "The degree to which a QE technique is well-suited to a particular collection can be evaluated in terms of its ability to still find the relevant documents, even when they are missing query terms, despite the bias of this approach against relevant documents.",
                "However, given that Okapi FB and TCS outperformed each other on two different collection sets, further investigation into the degree of compatibility between QE expansion approach and target collection is probably warranted.",
                "Furthermore, the investigation of other term removal strategies could provide insight into the behavior of different QE techniques and their overall impact on the user experience.",
                "As mentioned earlier, our choice of the term removal strategy was motivated by (1) our desire to see the highest impact on system performance as terms are removed and (2) because high IDF terms, in our domain context, are more likely to be domain specific, which allows us to better understand the performance of an IR system as experienced by expert and non-expert users.",
                "Although not attempted in our experiments, another application of this <br>evaluation</br> framework would be to remove query terms individually, rather than incrementally, to analyze which terms (or possibly which types of terms) are being helped most by a QE technique on a particular test collection.",
                "This could lead to insight as to when QE should and should not be applied.",
                "This <br>evaluation</br> framework allows us to see how IR systems perform in the presence of query-document term mismatch.",
                "In other evaluations, the performance of a system is measured only on the entire collection, in which the degree of query-term document mismatch is not known.",
                "By systematically introducing this mismatch, we can see that even if an IR system is not the best performer on the entire collection, its performance may nonetheless be more robust to query-document term mismatch than other systems.",
                "Such robustness makes a system more user-friendly, especially to non-expert users.",
                "This paper presents a novel framework for IR system <br>evaluation</br>, the applications of which are numerous.",
                "The results presented in this paper are not by any means meant to be exhaustive or entirely representative of the ways in which this <br>evaluation</br> could be applied.",
                "To be sure, there is much future work that could be done using this framework.",
                "In addition to looking at average performance of IR systems, the results of individual queries could be examined and compared more closely, perhaps giving more insight into the classification and prediction of difficult queries, or perhaps showing which QE techniques improve (or degrade) individual query performance under differing degrees of querydocument term mismatch.",
                "Indeed, this framework would also benefit from further testing on a larger collection. 7.",
                "CONCLUSION The proposed <br>evaluation</br> framework allows us to measure the degree to which different IR systems overcome (or dont overcome) term mismatch between queries and relevant documents.",
                "Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval.",
                "By systematically removing query terms from relevant documents, we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents.",
                "Further, we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.",
                "The <br>evaluation</br> framework proposed in this paper is attractive for several reasons.",
                "Most importantly, it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch.",
                "In addition, this framework takes advantage and stretches the amount of information we can get from existing test collections.",
                "Further, this <br>evaluation</br> framework is not metricspecific: information in terms of any metric (MAP, P@10, etc.) can be gained from evaluating an IR system this way.",
                "It should also be noted that this framework is generalizable to any IR system, in that it evaluates how well IR systems evaluate users information needs as represented by their queries.",
                "An IR system that is easy to use should be good at retrieving documents that are relevant to users information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 8.",
                "REFERENCES [1] Amati, G., C. Carpineto, and G. Romano.",
                "Query difficulty, robustness and selective application of query expansion.",
                "In Proceedings of the 25th European Conference on Information Retrieval (ECIR 2004), pp. 127-137. [2] Berger, A. and J.D.",
                "Lafferty. 1999.",
                "Information retrieval as statistical translation.",
                "In Research and Development in Information Retrieval, pages 222-229. [3] Billerbeck, B., F. Scholer, H. E. Williams, and J. Zobel. 2003.",
                "Query expansion using associated queries.",
                "In Proceedings of CIKM 2003, pp. 2-9. [4] Billerbeck, B., and J. Zobel. 2003.",
                "When Query Expansion Fails.",
                "In Proceedings of SIGIR 2003, pp. 387-388. [5] Billerbeck, B. and J. Zobel. 2004.",
                "Questioning Query Expansion: An Examination of Behaviour and Parameters.",
                "In Proceedings of the 15th Australasian Database Conference (ADC2004), pp. 69-76. [6] Billerbeck, B. and J. Zobel. 2005.",
                "Document Expansion versus Query Expansion for ad-hoc Retrieval.",
                "In Proceedings of the 10th Australasian Document Computing Symposium. [7] Buckley, C. and E.M. Voorhees. 2000.",
                "Evaluating <br>evaluation</br> Measure Stability.",
                "In Proceedings of SIGIR 2000, pp. 33-40. [8] Buckley, C. and E.M. Voorhees. 2004.",
                "Retrieval <br>evaluation</br> with incomplete information.",
                "In Proceedings of SIGIR 2004, pp. 25-32. [9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.",
                "What Makes A Query Difficult?",
                "In Proceedings of SIGIR 2006, pp. 390-397. [10] Carpineto, C., R. Mori and G. Romano. 1998.",
                "Informative Term Selection for Automatic Query Expansion.",
                "In The 7th Text REtrieval Conference, pp.363:369. [11] Carterette, B. and J. Allan. 2005.",
                "Incremental Test Collections.",
                "In Proceedings of CIKM 2005, pp. 680-687. [12] Carterette, B., J. Allan, and R. Sitaraman. 2006.",
                "Minimal Test Collections for Retrieval <br>evaluation</br>.",
                "In Proceedings of SIGIR 2006, pp. 268-275. [13] Cormack, G.V., C. R. Palmer, and C.L.",
                "Clarke. 1998.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR 1998, pp. 282-289. [14] Cormack, G. and T.R.",
                "Lynam. 2006.",
                "Statistical Precision of Information Retrieval <br>evaluation</br>.",
                "In Proceedings of SIGIR 2006, pp. 533-540. [15] Cronen-Townsend, S., Y. Zhou, and W.B.",
                "Croft. 2004.",
                "A Language Modeling Framework for Selective Query Expansion, CIIR Technical Report. [16] Efthimiadis, E.N.",
                "Query Expansion. 1996.",
                "In Martha E. Williams (ed. ), Annual Review of Information Systems and Technology (ARIST), v31, pp 121- 187. [17] Evans, D.A. and Lefferts, R.G. 1995.",
                "CLARIT-TREC Experiments.",
                "Information Processing & Management. 31(3): 385-295. [18] Fang, H. and C.X.",
                "Zhai. 2006.",
                "Semantic Term Matching in Axiomatic Approaches to Information Retrieval.",
                "In Proceedings of SIGIR 2006, pp. 115-122. [19] Gao, J., J. Nie, G. Wu and G. Cao. 2004.",
                "Dependence language model for information retrieval.",
                "In Proceedings of SIGIR 2004, pp. 170-177. [20] Harman, D.K. 1992.",
                "Relevance feedback revisited.",
                "In Proceedings of ACM SIGIR 1992, pp. 1-10. [21] Harman, D.K., ed. 1993.",
                "The First Text REtrieval Conference (TREC-1): 1992. [22] Harman, D.K., ed. 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993. [23] Harman, D.K., ed. 1995.",
                "The Third Text REtrieval Conference (TREC-3): 1994. [24] Harman, D.K., 1998.",
                "Towards Interactive Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 321-331. [25] Hofmann, T. 1999.",
                "Probabilistic latent semantic indexing.",
                "In Proceedings of SIGIR 1999, pp 50-57. [26] Jing, Y. and W.B.",
                "Croft. 1994.",
                "The Association Thesaurus for Information Retrieval.",
                "In Proceedings of RIAO 1994, pp. 146-160 [27] Lu, X.A. and R.B.",
                "Keefer.",
                "Query expansion/reduction and its impact on retrieval effectiveness.",
                "In: D.K.",
                "Harman, ed.",
                "The Third Text REtrieval Conference (TREC-3).",
                "Gaithersburg, MD: National Institute of Standards and Technology, 1995,231-239. [28] McNamee, P. and J. Mayfield. 2002.",
                "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources.",
                "In Proceedings of SIGIR 2002, pp. 159-166. [29] Mitra, M., A. Singhal, and C. Buckley. 1998.",
                "Improving Automatic Query Expansion.",
                "In Proceedings of SIGIR 1998, pp. 206-214. [30] Peat, H. J. and P. Willett. 1991.",
                "The limitations of term co-occurrence data for query expansion in document retrieval systems.",
                "Journal of the American Society for Information Science, 42(5): 378-383. [31] Ponte, J.M. and W.B.",
                "Croft. 1998.",
                "A language modeling approach to information retrieval.",
                "In Proceedings of SIGIR 1998, pp.275-281. [32] Qiu Y., and Frei H. 1993.",
                "Concept based query expansion.",
                "In Proceedings of SIGIR 1993, pp. 160-169. [33] Robertson, S. 2006.",
                "On GMAP - and other transformations.",
                "In Proceedings of CIKM 2006, pp. 78-83. [34] Robertson, S.E. and K. Sparck Jones. 1976.",
                "Relevance Weighting of Search Terms.",
                "Journal of the American Society for Information Science, 27(3): 129-146. [35] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1994.",
                "Okapi at TREC-2.",
                "In D.K.",
                "Harman (ed). 1994.",
                "The Second Text REtrieval Conference (TREC-2): 1993, pp. 21-34. [36] Robertson, S.E., S. Walker, S. Jones, M.M.",
                "Hancock-Beaulieu, and M. Gatford. 1995.",
                "Okapi at TREC-3.",
                "In D.K.",
                "Harman (ed). 1995.",
                "The Third Text REtrieval Conference (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J. 1971.",
                "Relevance feedback in information retrieval.",
                "In G. Salton (Ed. ), The SMART Retrieval System.",
                "Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323. [38] Salton, G. 1968.",
                "Automatic Information Organization and Retrieval.",
                "McGraw-Hill. [39] Salton, G. 1971.",
                "The SMART Retrieval System: Experiments in Automatic Document Processing.",
                "Englewood Cliffs NJ; Prentice-Hall. [40] Salton,G. 1980.",
                "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification.",
                "Information Processing & Management. 16(1): 1-15. [41] Salton, G., and C. Buckley. 1988.",
                "On the Use of Spreading Activation Methods in Automatic Information Retrieval.",
                "In Proceedings of SIGIR 1998, pp. 147-160. [42] Sanderson, M. 1994.",
                "Word sense disambiguation and information retrieval.",
                "In Proceedings of SIGIR 1994, pp. 161-175. [43] Sanderson, M. and H. Joho. 2004.",
                "Forming test collections with no system pooling.",
                "In Proceedings of SIGIR 2004, pp. 186-193. [44] Sanderson, M. and Zobel, J. 2005.",
                "Information Retrieval System <br>evaluation</br>: Effort, Sensitivity, and Reliability.",
                "In Proceedings of SIGIR 2005, pp. 162-169. [45] Smeaton, A.F. and C.J.",
                "Van Rijsbergen. 1983.",
                "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System.",
                "Computer Journal. 26(3):239-246. [46] Song, F. and W.B.",
                "Croft. 1999.",
                "A general language model for information retrieval.",
                "In Proceedings of the Eighth International Conference on Information and Knowledge Management, pages 316-321. [47] Sparck Jones, K. 1971.",
                "Automatic Keyword Classification for Information Retrieval.",
                "London: Butterworths. [48] Terra, E. and C. L. Clarke. 2004.",
                "Scoring missing terms in information retrieval tasks.",
                "In Proceedings of CIKM 2004, pp. 50-58. [49] Turtle, Howard. 1994.",
                "Natural Language vs. Boolean Query <br>evaluation</br>: A Comparison of Retrieval Performance.",
                "In Proceedings of SIGIR 1994, pp. 212-220. [50] Voorhees, E.M. 1994a.",
                "On Expanding Query Vectors with Lexically Related Words.",
                "In Harman, D. K., ed.",
                "Text REtrieval Conference (TREC-1): 1992. [51] Voorhees, E.M. 1994b.",
                "Query Expansion Using Lexical-Semantic Relations.",
                "In Proceedings of SIGIR 1994, pp. 61-69."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Durante nuestro trabajo sobre el desarrollo de dichos sistemas, concluimos que los métodos actuales de \"evaluación\" no son suficientes para este propósito.{Tos ferina, tos ferina}, {ataque cardíaco, infarto de miocardio}, {lavado de autos, limpieza de automóviles}, {abogado, asesor legal, abogado} son ejemplos de cosas que comparten el mismo significado.",
                "Si el objetivo de QE es aumentar el rendimiento de la búsqueda mediante la mitigación de los efectos del desajuste del término del documento de consulta, entonces el grado en que un sistema lo hace debe ser medible en la \"evaluación\".",
                "Un método de \"evaluación\" efectivo debe medir el rendimiento de los sistemas IR bajo diversos grados de desajuste de término de documento de consulta, no solo en términos de rendimiento general en una colección en relación con otro sistema.1 Thomson Corporation desarrolla soluciones basadas en información a los mercados profesionales, incluidos los legales, financieros, de atención médica, científica e fiscal y contabilidad.",
                "Debido a que estamos induciendo el propósito deliberadamente el malhechor entre las consultas y los documentos relevantes conocidos en nuestras colecciones de pruebas, el marco de \"evaluación\" propuesto es capaz de medir la efectividad de QE de una manera que las pruebas en toda la colección no lo son.",
                "La \"evaluación\" de los sistemas IR ha recibido mucha atención en la comunidad de investigación, tanto en términos de desarrollo de colecciones de pruebas para la \"evaluación\" de diferentes sistemas [11, 12, 13, 43] como en términos de la utilidad de las métricas de evaluación, comoComo retiró, precisión, precisión promedio media, precisión en rango, bpref, etc. [7, 8, 44, 14].",
                "Existe un interés general en la comunidad de investigación para mejorar la robustez de los sistemas IR al mejorar el rendimiento de la recuperación en consultas difíciles, como lo demuestra la pista robusta en las competiciones TREC y las nuevas medidas de \"evaluación\" como GMAP.",
                "Al inducir deliberadamente la falta de coincidencia entre los términos en consultas y documentos relevantes, nuestro marco de \"evaluación\" nos permite una manera controlada para degradar la calidad de las consultas con respecto a sus documentos relevantes, y luego medir tanto el grado (inducido (inducido (inducido) Dificultad de la consulta y el grado en que QE mejora el rendimiento de recuperación de la consulta degradada.",
                "El marco de \"evaluación\" presentado en este documento consta de tres elementos: una colección de pruebas, c;una estrategia para seleccionar qué términos de consulta eliminar de los documentos relevantes en esa colección, s;y una métrica para comparar el rendimiento de los sistemas IR, m.La recopilación de pruebas, C, consiste en una recopilación de documentos, consultas y juicios de relevancia.",
                "La estrategia, s, determina el orden y la forma en que los términos de la consulta se eliminan de los documentos relevantes en C. Este marco de \"evaluación\" no es específico de la métrica;Cualquier métrica (mapa, p@10, retirada, etc.) se puede utilizar para medir el rendimiento del sistema IR.",
                "Aunque las colecciones de pruebas son difíciles de encontrar, debe tenerse en cuenta que este marco de \"evaluación\" se puede usar en cualquier colección de pruebas disponible.",
                "Según el propósito de la \"evaluación\" y el algoritmo de recuperación que se está utilizando, podría tener más sentido elegir una orden de eliminación para S basada en las IDF del término de consulta o tal vez basado en una medida de la probabilidad de término de consulta en la recopilación de documentos.",
                "La eliminación aditiva incremental de los términos de consulta de documentos relevantes permite que la \"evaluación\" muestre el grado en que el rendimiento del sistema IR se degrada a medida que faltan más y más términos de consulta, aumentando así el grado de desajuste de término de documento de consultorio.",
                "Configuración experimental 4.1 IR Sistemas Utilizamos el marco de \"evaluación\" propuesto para evaluar cuatro sistemas IR en dos colecciones de pruebas.",
                "De los cuatro sistemas utilizados en la \"Evaluación\", dos técnicas de expansión de consultas implementadas: OKAPI (con pseudo-retroalimentación para QE) y un motor de búsqueda conceptual patentado (bien, llame TCS, para Thomson Concept Search).",
                "En nuestra \"evaluación\", utilizamos tanto el título como los campos de descripción de los temas 151200 como consultas, por lo que tenemos dos conjuntos de resultados para la colección AP89.",
                "Para las consultas de título de TREC más cortas, eliminamos uno, dos, tres y cinco términos de los documentos relevantes.4.4 Métricas En esta implementación del marco de \"evaluación\", elegimos tres métricas para comparar el rendimiento del sistema IR: precisión promedio media (MAP), precisión en 10 documentos (P10) y recordar en 1000 documentos.",
                "Debido a que el rendimiento general del motor de búsqueda se mide con frecuencia en términos de MAP, y debido a que otras evaluaciones de QE a menudo solo consideran el rendimiento en toda la colección (es decir, no consideran la falta de coincidencia de término), el QE implementado en TCS se consideraría (en AN0 12 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MediaPrecision (MAP) OKAPI FB OKAPI TCS QL FSUPP: Precisión promedio media con términos de consulta Figura 1: el rendimiento de los cuatroSistemas de recuperación en la colección FSupp en términos de precisión promedio media (MAP) y en función del número de términos de consulta eliminados (el eje horizontal). 0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de documentos relevantes 0 0.050.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 PrecisionAt10Documentos (P10) OKAPI FB OKAPI TCS QL FSUPP: P10 Con términos de consulta eliminado Figura 2: El rendimiento de los cuatro sistemas de recuperación en la colección FSUPP en términos de precisión en 10 y como unfunción del número de términos de consulta eliminados (el eje horizontal).Otra \"evaluación\") para dañar el rendimiento en la colección FSupp.",
                "Al igual que con las consultas de descripción AP89, Okapi FB es nuevamente el mejor desempeño de los cuatro sistemas en la \"evaluación\".",
                "Quizás el resultado más interesante de nuestra \"evaluación\" es que, aunque las líneas de base de solo palabras clave funcionaban de manera consistente y como se esperaba en ambas colecciones con respecto a la eliminación de términos de consulta de documentos relevantes, el rendimiento de los motores que implementan técnicas QE diferían dramáticamente entre las colecciones.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MediaSaverageRecision (MAP) OKAPI FB OKAPI TCS QL AP89: Precisión promedio media con términos de consulta (Título Queries) Figura 7: Mapa deLos cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.4 0.45 0.5 Precisión10Documentos (P10) OKAPI FB OKAPI TCS QL AP89: P10 Con Términos de consulta eliminados (Consultas de título) Figura 8: Precisión AT10 de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recuerde Okapi FB Okapi TCS QL AP89: Recuerde a 1000 documentos con términos de consulta eliminados (consultas de título) Figura 9: Recuerdos (a 1000 a 1000) de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.6.",
                "Discusión La intuición detrás de este marco de \"evaluación\" es medir el grado en que varias técnicas de QE superan el desajuste del término entre consultas y documentos relevantes.",
                "Una \"evaluación\" sistemática de los sistemas IR como se describe en este documento es útil no solo con respecto a la medición del éxito general o el fracaso de técnicas de QE particulares en presencia de la falta de coincidencia del término del documento de consulta, sino que también proporciona información sobre cómo un IR particular IR IREl sistema funcionará cuando use usuarios expertos versus no expertos en una colección en particular.",
                "Dado que los términos de consulta se enmascaran solo en los documentos relevantes, este marco de \"evaluación\" está en realidad sesgado contra la recuperación de documentos relevantes.",
                "Aunque no se intenta en nuestros experimentos, otra aplicación de este marco de \"evaluación\" sería eliminar los términos de consulta individualmente, en lugar de de forma incremental, analizar qué términos (o posiblemente qué tipos de términos) están siendo ayudados más por una técnica QE en una particularColección de pruebas.",
                "Este marco de \"evaluación\" nos permite ver cómo funcionan los sistemas IR en presencia de desajuste del término del documento de consulta.",
                "Este documento presenta un marco novedoso para la \"evaluación\" del sistema IR, cuyas aplicaciones son numerosas.",
                "Los resultados presentados en este documento no están destinados a ser exhaustivos o completamente representativos de las formas en que se podría aplicar esta \"evaluación\".",
                "Conclusión El marco de \"evaluación\" propuesto nos permite medir el grado en que los diferentes sistemas IR superan (o no superan) la no coincidencia entre consultas y documentos relevantes.",
                "El marco de \"evaluación\" propuesto en este documento es atractivo por varias razones.",
                "Además, este marco de \"evaluación\" no es específico de la métrica: la información en términos de cualquier métrica (mapa, p@10, etc.) puede obtenerse evaluar un sistema IR de esta manera.",
                "Evaluación de la \"evaluación\" medir la estabilidad.",
                "\"Evaluación\" de recuperación con información incompleta.",
                "Colecciones de pruebas mínimas para la \"evaluación\" de recuperación.",
                "Precisión estadística de la recuperación de información \"Evaluación\".",
                "Sistema de recuperación de información \"Evaluación\": esfuerzo, sensibilidad y confiabilidad.",
                "Lenguaje natural versus consulta booleana \"Evaluación\": una comparación del rendimiento de la recuperación."
            ],
            "translated_text": "",
            "candidates": [
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "Evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "Evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "Evaluación",
                "evaluación",
                "evaluación",
                "evaluación",
                "Evaluación",
                "evaluación",
                "Evaluación",
                "evaluación",
                "Evaluación"
            ],
            "error": []
        }
    }
}