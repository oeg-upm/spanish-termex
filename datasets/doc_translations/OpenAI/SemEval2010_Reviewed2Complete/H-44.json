{
    "id": "H-44",
    "original_text": "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem. As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results. In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance. These techniques can be formulated as optimization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets. Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections. Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1. INTRODUCTION In this work we address time-travel text search over temporally versioned document collections. Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds. Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents. Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing. Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates. For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians. Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives. If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need. Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered. Looking at their evolutionary history, we are faced with even larger data volumes. As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes. This paper presents an efficient solution to time-travel text search by making the following key contributions: 1. The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2. Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3. We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4. In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents. The remainder of this paper is organized as follows. The presented work is put in context with related work in Section 2. We delineate our model of a temporally versioned document collection in Section 3. We present our time-travel inverted index in Section 4. Building on it, temporal coalescing is described in Section 5. In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2. RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index. We briefly review work under these categories here. To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents. Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries. Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past. Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents. Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results. Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives. This adaptation, however, does not provide the intended time-travel text search functionality. In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28]. Unlike the inverted file index, their applicability to text search is not well understood. Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size. Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context. More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size. None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality. Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result. They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction. It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3. MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following. Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . . Each version dti has an associated timestamp ti reflecting when the version was created. Each version is a vector of searchable terms or features. Any modification to a document version results in the insertion of a new version with corresponding timestamp. We employ a discrete definition of time, so that timestamps are non-negative integers. The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥. The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now). Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity. As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well. For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined. We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered. The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) . It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti. The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively. The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4. TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems. In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list. The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload. The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document. The sort-order of index lists depends on which queries are to be supported efficiently. For Boolean queries it is favorable to sort index lists in document-order. Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31]. A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists. For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information. The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid. The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval. As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) . Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree. Unlike the tf-score, the idf-score of every term could vary with every change in the corpus. Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary. Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings. We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing. To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)). Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost. As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly. We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists. As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5. TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version. For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance. The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size. It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched. As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all. Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded. This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document. Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example. The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered. We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions. Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv. As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately. We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm. Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold . In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| . Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee. Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20]. Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence. In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings. Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time. Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5]. The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work. As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21]. This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution. Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . . O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I. While doing so, it coalesces sequences of postings having maximal length. The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details). When reading the next posting, the algorithm tries to add it to the current sequence of postings. It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee. If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized. The time complexity of the algorithm is in O(n). Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6. SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings. Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains. In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index. Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist. Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists. Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2. The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3. For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10. Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list. Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case. Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively. Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section. However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone. The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries. Further, the two techniques, can be applied separately and are independent. If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller. We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} . To aid the presentation in the rest of the paper, we first provide some definitions. Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv. Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals. We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed. We also assume that intervals in M are disjoint. We can make this assumption without ruling out any optimal solution with regard to space or performance defined below. The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1). The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ). Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved. Therefore, we will refer to this approach as Popt in the remainder. The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder. This approach requires minimal space, since it keeps each posting exactly once. Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance. The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists. In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting. If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3). The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained. In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1. Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee. Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5]. The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed. The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space. In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit. The technique presented next, which is named SB, tackles this very problem. The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt. The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance). In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1). Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t. X m∈M |Lv : m| ≤ κ |Lv| . The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization. A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5]. Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets. We obtain an approximate solution to the problem using simulated annealing [22, 23]. Simulated annealing takes a fixed number R of rounds to explore the solution space. In each round a random successor of the current solution is looked at. If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept). A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution. If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds. In addition, throughout all rounds, the method keeps track of the best solution seen so far. The solution space for the problem at hand can be efficiently explored. As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals. We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set. Note that b1 and bn are always set to true. Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }. A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables. The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round. Its space complexity is in O(n) - for keeping the n boolean variables. As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7. EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5. All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003. All data and indexes are kept in an Oracle 10g database that runs on the same machine. For our experiments we used two different datasets. The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file. This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download). We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.). This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18. We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.). The thus extracted queries contained a total of 422 distinct terms. For each extracted query, we randomly picked a time point for each month covered by the dataset. This resulted in a total of 18, 000 (= 300 × 60) time-travel queries. The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data. We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper. This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79). We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc.), and randomly sampling a time point for every month within the two year period spanned by the dataset. Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset. In total 522 terms appear in the extracted queries. The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality. For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline. WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings. As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size. Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude. Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size. Index size continues to reduce on both datasets, as we increase the value of . How does the reduction in index size affect the query results? In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively. We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement). Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01. Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph. It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits. For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index. Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95. For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively. On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6. For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10. In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations. However, note that the postings in the materialized sublists still retain their original timestamps. For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows. The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists. To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points. We report the mean EPC, as well as the 5%- and 95%-percentile. In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload. The Sopt and Popt approaches are, by their definition, parameter-free. For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0. Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0. Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds. Table 2 lists the obtained space and performance figures. Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus. Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption. Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost. The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude. We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8. CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections. Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results. The present work opens up many interesting questions for future research, e.g.: How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?. How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point? How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9. ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10. REFERENCES [1] V. N. Anh and A. Moffat. Pruned Query Evaluation Using Pre-Computed Impacts. In SIGIR, 2006. [2] V. N. Anh and A. Moffat. Pruning Strategies for Mixed-Mode Querying. In CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn. Versioning a Full-Text Information Retrieval System. In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum. A Time Machine for Text search. Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo. Coalescing in Temporal Databases. In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna. Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations. In WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita. Indexing Shared Content in Information Retrieval Systems. In EDBT, 2006. [9] C. Buckley and A. F. Lewit. Optimization of Inverted Vector Searches. In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen. Method and Apparatus for Generating and Searching Range-Based Index of Word Locations. U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke. A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems. In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer. Static Index Pruning for Information Retrieval Systems. In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing Top k Lists. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor. Optimal Aggregation Algorithms for Middleware. J. Comput. Syst. Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J. Woo. REHIST: Relative Error Histogram Construction Algorithms. In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev. Efficient Indexing of Versioned Document Sequences. In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala. Balancing Histogram Optimality and Practicality for Query Result Size Estimation. In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel. Optimal Histograms with Quality Guarantees. In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An Online Algorithm for Segmenting Time Series. In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi. Optimization by Simulated Annealing. Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos. Algorithm Design. Addison-Wesley, 2005. [24] U. Manber. Introduction to Algorithms: A Creative Approach. Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø. DyST: Dynamic and Scalable Temporal Text Indexing. In TIME, 2006. [26] J. M. Ponte and W. B. Croft. A Language Modeling Approach to Information Retrieval. In SIGIR, 1998. [27] S. E. Robertson and S. Walker. Okapi/Keenbow at TREC-8. In TREC, 1999. [28] B. Salzberg and V. J. Tsotras. Comparison of Access Methods for Time-Evolving Data. ACM Comput. Surv., 31(2):158-221, 1999. [29] M. Stack. Full Text Search of Web Archive Collections. In IWAW, 2006. [30] E. Terzi and P. Tsaparas. Efficient Algorithms for Sequence Segmentation. In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel. Top-k Query Evaluation with Probabilistic Guarantees. In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel. Efficient Search in Large Textual Collections with Redundancy. In WWW, 2007. [35] J. Zobel and A. Moffat. Inverted Files for Text Search Engines. ACM Comput. Surv., 38(2):6, 2006.",
    "original_translation": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006.",
    "original_sentences": [
        "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
        "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
        "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
        "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
        "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
        "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
        "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
        "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
        "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
        "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
        "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
        "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
        "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
        "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
        "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
        "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
        "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
        "Looking at their evolutionary history, we are faced with even larger data volumes.",
        "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
        "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
        "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
        "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
        "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
        "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
        "The remainder of this paper is organized as follows.",
        "The presented work is put in context with related work in Section 2.",
        "We delineate our model of a temporally versioned document collection in Section 3.",
        "We present our time-travel inverted index in Section 4.",
        "Building on it, temporal coalescing is described in Section 5.",
        "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
        "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
        "We briefly review work under these categories here.",
        "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
        "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
        "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
        "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
        "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
        "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
        "This adaptation, however, does not provide the intended time-travel text search functionality.",
        "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
        "Unlike the inverted file index, their applicability to text search is not well understood.",
        "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
        "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
        "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
        "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
        "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
        "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
        "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
        "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
        "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
        "Each version dti has an associated timestamp ti reflecting when the version was created.",
        "Each version is a vector of searchable terms or features.",
        "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
        "We employ a discrete definition of time, so that timestamps are non-negative integers.",
        "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
        "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
        "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
        "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
        "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
        "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
        "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
        "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
        "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
        "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
        "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
        "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
        "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
        "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
        "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
        "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
        "The sort-order of index lists depends on which queries are to be supported efficiently.",
        "For Boolean queries it is favorable to sort index lists in document-order.",
        "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
        "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
        "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
        "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
        "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
        "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
        "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
        "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
        "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
        "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
        "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
        "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
        "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
        "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
        "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
        "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
        "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
        "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
        "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
        "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
        "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
        "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
        "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
        "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
        "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
        "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
        "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
        "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
        "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
        "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
        "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
        "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
        "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
        "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
        "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
        "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
        "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
        "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
        "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
        "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
        "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
        "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
        "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
        "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
        "While doing so, it coalesces sequences of postings having maximal length.",
        "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
        "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
        "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
        "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
        "The time complexity of the algorithm is in O(n).",
        "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
        "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
        "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
        "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
        "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
        "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
        "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
        "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
        "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
        "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
        "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
        "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
        "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
        "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
        "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
        "Further, the two techniques, can be applied separately and are independent.",
        "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
        "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
        "To aid the presentation in the rest of the paper, we first provide some definitions.",
        "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
        "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
        "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
        "We also assume that intervals in M are disjoint.",
        "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
        "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
        "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
        "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
        "Therefore, we will refer to this approach as Popt in the remainder.",
        "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
        "This approach requires minimal space, since it keeps each posting exactly once.",
        "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
        "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
        "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
        "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
        "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
        "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
        "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
        "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
        "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
        "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
        "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
        "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
        "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
        "The technique presented next, which is named SB, tackles this very problem.",
        "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
        "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
        "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
        "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
        "X m∈M |Lv : m| ≤ κ |Lv| .",
        "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
        "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
        "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
        "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
        "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
        "In each round a random successor of the current solution is looked at.",
        "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
        "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
        "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
        "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
        "The solution space for the problem at hand can be efficiently explored.",
        "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
        "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
        "Note that b1 and bn are always set to true.",
        "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
        "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
        "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
        "Its space complexity is in O(n) - for keeping the n boolean variables.",
        "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
        "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
        "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
        "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
        "For our experiments we used two different datasets.",
        "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
        "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
        "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
        "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
        "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
        "The thus extracted queries contained a total of 422 distinct terms.",
        "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
        "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
        "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
        "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
        "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
        "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
        "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
        "In total 522 terms appear in the extracted queries.",
        "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
        "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
        "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
        "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
        "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
        "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
        "Index size continues to reduce on both datasets, as we increase the value of .",
        "How does the reduction in index size affect the query results?",
        "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
        "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
        "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
        "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
        "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
        "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
        "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
        "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
        "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
        "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
        "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
        "However, note that the postings in the materialized sublists still retain their original timestamps.",
        "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
        "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
        "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
        "We report the mean EPC, as well as the 5%- and 95%-percentile.",
        "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
        "The Sopt and Popt approaches are, by their definition, parameter-free.",
        "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
        "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
        "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
        "Table 2 lists the obtained space and performance figures.",
        "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
        "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
        "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
        "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
        "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
        "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
        "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
        "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
        "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
        "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
        "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
        "REFERENCES [1] V. N. Anh and A. Moffat.",
        "Pruned Query Evaluation Using Pre-Computed Impacts.",
        "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
        "Pruning Strategies for Mixed-Mode Querying.",
        "In CIKM, 2006.",
        "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
        "Versioning a Full-Text Information Retrieval System.",
        "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
        "Modern Information Retrieval.",
        "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
        "A Time Machine for Text search.",
        "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
        "Coalescing in Temporal Databases.",
        "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
        "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
        "In WAW, 2004. [8] A.",
        "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
        "Indexing Shared Content in Information Retrieval Systems.",
        "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
        "Optimization of Inverted Vector Searches.",
        "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
        "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
        "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
        "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
        "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
        "Static Index Pruning for Information Retrieval Systems.",
        "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
        "Comparing Top k Lists.",
        "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
        "Optimal Aggregation Algorithms for Middleware.",
        "J. Comput.",
        "Syst.",
        "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
        "Woo.",
        "REHIST: Relative Error Histogram Construction Algorithms.",
        "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
        "Efficient Indexing of Versioned Document Sequences.",
        "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
        "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
        "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
        "Optimal Histograms with Quality Guarantees.",
        "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
        "An Online Algorithm for Segmenting Time Series.",
        "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
        "Optimization by Simulated Annealing.",
        "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
        "Algorithm Design.",
        "Addison-Wesley, 2005. [24] U. Manber.",
        "Introduction to Algorithms: A Creative Approach.",
        "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
        "DyST: Dynamic and Scalable Temporal Text Indexing.",
        "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
        "A Language Modeling Approach to Information Retrieval.",
        "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
        "Okapi/Keenbow at TREC-8.",
        "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
        "Comparison of Access Methods for Time-Evolving Data.",
        "ACM Comput.",
        "Surv., 31(2):158-221, 1999. [29] M. Stack.",
        "Full Text Search of Web Archive Collections.",
        "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
        "Efficient Algorithms for Sequence Segmentation.",
        "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
        "Top-k Query Evaluation with Probabilistic Guarantees.",
        "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
        "Managing Gigabytes: Compressing and Indexing Documents and Images.",
        "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
        "Efficient Search in Large Textual Collections with Redundancy.",
        "In WWW, 2007. [35] J. Zobel and A. Moffat.",
        "Inverted Files for Text Search Engines.",
        "ACM Comput.",
        "Surv., 38(2):6, 2006."
    ],
    "translated_text_sentences": [
        "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación.",
        "Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal.",
        "Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados.",
        "Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento.",
        "Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente.",
        "Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala.",
        "Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados.",
        "Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1.",
        "En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente.",
        "Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo.",
        "La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados.",
        "Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral.",
        "La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo.",
        "Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados.",
        "Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web.",
        "Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas.",
        "Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea.",
        "Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes.",
        "Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes.",
        "Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1.",
        "El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2.",
        "La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos.",
        "Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4.",
        "En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados.",
        "El resto de este documento está organizado de la siguiente manera.",
        "El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2.",
        "Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3.",
        "Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4.",
        "Basándose en ello, la coalescencia temporal se describe en la Sección 5.",
        "En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7.",
        "TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice.",
        "Breve revisamos el trabajo bajo estas categorías aquí.",
        "Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente.",
        "Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas.",
        "Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo.",
        "Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos.",
        "El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados.",
        "Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web.",
        "Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista.",
        "Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28].",
        "A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida.",
        "Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice.",
        "Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto.",
        "Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice.",
        "Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo.",
        "Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta.",
        "Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice.",
        "Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3.",
        "En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación.",
        "Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . .",
        "Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión.",
        "Cada versión es un vector de términos o características buscables.",
        "Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente.",
        "Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos.",
        "La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥.",
        "El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora).",
        "Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
        "Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad.",
        "Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también.",
        "Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
        "En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t.",
        "Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t.",
        "El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti).",
        "Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti.",
        "El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente.",
        "El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión.",
        "El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas.",
        "En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida.",
        "La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así.",
        "La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento.",
        "El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente.",
        "Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos.",
        "Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31].",
        "Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices.",
        "Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal.",
        "La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida.",
        "Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez.",
        "Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)).",
        "De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+.",
        "A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus.",
        "Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido.",
        "Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas.",
        "Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo.",
        "Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)).",
        "Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S.",
        "Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S.",
        "Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices.",
        "Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5.",
        "Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento.",
        "Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre.",
        "La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices.",
        "Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas.",
        "Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto.",
        "La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado.",
        "Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento.",
        "La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo.",
        "La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual.",
        "A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas.",
        "Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv.",
        "Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
        "Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado.",
        "Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm.",
        "Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral.",
        "En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
        "En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|.",
        "Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente.",
        "Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20].",
        "Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima.",
        "En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente.",
        "Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2).",
        "Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5].",
        "La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo.",
        "Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21].",
        "Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima.",
        "Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . .",
        "El algoritmo 1 realiza un pase sobre la secuencia de entrada I.",
        "Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima.",
        "El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles).",
        "Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones.",
        "Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación.",
        "Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad.",
        "La complejidad temporal del algoritmo es de O(n).",
        "Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6.",
        "La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas.",
        "La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo.",
        "En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo.",
        "Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista.",
        "Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas.",
        "Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2.",
        "La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3.",
        "Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10.",
        "Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida.",
        "Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos.",
        "Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente.",
        "Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección.",
        "Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice.",
        "El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa.",
        "Además, las dos técnicas pueden aplicarse por separado y son independientes.",
        "Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas.",
        "Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}.",
        "Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones.",
        "Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv.",
        "Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales.",
        "Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas.",
        "También asumimos que los intervalos en M son disjuntos.",
        "Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación.",
        "El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1).",
        "El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ).",
        "Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible.",
        "Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo.",
        "El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto.",
        "Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez.",
        "Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento.",
        "Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas.",
        "En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación.",
        "Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3).",
        "La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo.",
        "En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo.",
        "Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
        "Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
        "De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento.",
        "El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5].",
        "La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados.",
        "La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo.",
        "En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado.",
        "La técnica presentada a continuación, llamada SB, aborda este mismo problema.",
        "La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt.",
        "La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado).",
        "En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1).",
        "Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a.",
        "X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .",
        "El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización.",
        "Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5].",
        "Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes.",
        "Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23].",
        "El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones.",
        "En cada ronda se examina un sucesor aleatorio de la solución actual.",
        "Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual).",
        "Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual.",
        "Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes.",
        "Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento.",
        "El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente.",
        "Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos.",
        "Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto.",
        "Ten en cuenta que b1 y bn siempre se establecen como verdaderos.",
        "Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }.",
        "Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias.",
        "La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda.",
        "Su complejidad espacial es de O(n) - para mantener las n variables booleanas.",
        "Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7.",
        "EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5.",
        "Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003.",
        "Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina.",
        "Para nuestros experimentos utilizamos dos conjuntos de datos diferentes.",
        "El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML.",
        "Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga).",
        "Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.).",
        "Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18.",
        "Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.).",
        "Las consultas extraídas contenían un total de 422 términos distintos.",
        "Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos.",
        "Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo.",
        "El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto.",
        "Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento.",
        "Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79).",
        "Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos.",
        "Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV.",
        "En total aparecen 522 términos en las consultas extraídas.",
        "Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados.",
        "Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia.",
        "WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones.",
        "Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice.",
        "Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud.",
        "Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original.",
        "El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de .",
        "¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta?",
        "Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente.",
        "Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia).",
        "La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01.",
        "Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico.",
        "Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables.",
        "Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original.",
        "Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95.",
        "Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente.",
        "En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6.",
        "Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10.",
        "Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista.",
        "Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales.",
        "Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera.",
        "El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas.",
        "Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta.",
        "Informamos la media de EPC, así como el percentil 5% y 95%.",
        "En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta.",
        "Los enfoques Sopt y Popt son, por definición, libres de parámetros.",
        "Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0.",
        "Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0.",
        "Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas.",
        "La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas.",
        "Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus.",
        "Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme.",
        "Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado.",
        "Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud.",
        "También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8.",
        "CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente.",
        "Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos.",
        "El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]?",
        "¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo?",
        "¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9.",
        "AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10.",
        "REFERENCIAS [1] V. N. Anh y A. Moffat.",
        "Evaluación de Consultas Podadas Utilizando Impactos Precomputados.",
        "En SIGIR, 2006. [2] V. N. Anh y A. Moffat.",
        "Estrategias de poda para consultas de modo mixto.",
        "En CIKM, 2006.",
        "WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn.",
        "Creación de versiones de un sistema de recuperación de información de texto completo.",
        "En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto.",
        "Recuperación de información moderna.",
        "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum.",
        "Una máquina del tiempo para búsqueda de texto.",
        "Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo.",
        "Fusionando en bases de datos temporales.",
        "En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna.",
        "Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank.",
        "En WAW, 2004. [8] A.",
        "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita.",
        "Indexación de contenido compartido en sistemas de recuperación de información.",
        "En EDBT, 2006. [9] C. Buckley y A. F. Lewit.",
        "Optimización de Búsquedas de Vectores Invertidos.",
        "En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen.",
        "Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras.",
        "Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke.",
        "Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto.",
        "En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer.",
        "Poda de Índice Estático para Sistemas de Recuperación de Información.",
        "En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar.",
        "Comparando listas Top k.",
        "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor.",
        "Algoritmos de Agregación Óptimos para Middleware.",
        "J. Comput.",
        "This is not a complete sentence. Please provide more context or a complete sentence to be translated.",
        "Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J.",
        "¡Guau!",
        "REHIST: Algoritmos de Construcción de Histogramas de Error Relativo.",
        "En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev.",
        "Indexación eficiente de secuencias de documentos versionados.",
        "En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala.",
        "Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta.",
        "En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel.",
        "Histogramas óptimos con garantías de calidad.",
        "En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani.",
        "Un algoritmo en línea para segmentar series temporales.",
        "En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi.",
        "Optimización por Recocido Simulado.",
        "Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos.",
        "Diseño de algoritmos.",
        "Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber.",
        "Introducción a los Algoritmos: Un Enfoque Creativo.",
        "Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø.",
        "DyST: Indexación de texto temporal dinámica y escalable.",
        "En TIME, 2006. [26] J. M. Ponte y W. B. Croft.",
        "Un enfoque de modelado del lenguaje para la recuperación de información.",
        "En SIGIR, 1998. [27] S. E. Robertson y S. Walker.",
        "Okapi/Keenbow en TREC-8.",
        "En TREC, 1999. [28] B. Salzberg y V. J. Tsotras.",
        "Comparación de métodos de acceso para datos en evolución temporal.",
        "ACM Comput.",
        "Rev., 31(2):158-221, 1999. [29] M. Stack.",
        "Búsqueda de texto completo en colecciones de archivos web.",
        "En IWAW, 2006. [30] E. Terzi y P. Tsaparas.",
        "Algoritmos eficientes para la segmentación de secuencias.",
        "En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel.",
        "Evaluación de consultas Top-k con garantías probabilísticas.",
        "En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell.",
        "Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes.",
        "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel.",
        "Búsqueda eficiente en colecciones textuales grandes con redundancia.",
        "En WWW, 2007. [35] J. Zobel y A. Moffat.",
        "Archivos invertidos para motores de búsqueda de texto.",
        "ACM Comput.",
        "Rev., 38(2):6, 2006."
    ],
    "error_count": 1,
    "keys": {
        "time machine": {
            "translated_key": "máquina del tiempo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A <br>time machine</br> for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient <br>time machine</br> scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A <br>time machine</br> for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "A <br>time machine</br> for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "Results unequivocally show that our methods make it possible to build an efficient <br>time machine</br> scalable to large versioned text collections.",
                "A <br>time machine</br> for Text search."
            ],
            "translated_annotated_samples": [
                "Una <br>máquina del tiempo</br> para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación.",
                "Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una <br>máquina del tiempo</br> eficiente escalable a grandes colecciones de textos versionados.",
                "Una <br>máquina del tiempo</br> para búsqueda de texto."
            ],
            "translated_text": "Una <br>máquina del tiempo</br> para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una <br>máquina del tiempo</br> eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una <br>máquina del tiempo</br> para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "text search": {
            "translated_key": "búsqueda de texto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for <br>text search</br> Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT <br>text search</br> over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel <br>text search</br> by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel <br>text search</br> over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "<br>text search</br> on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel <br>text search</br>, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel <br>text search</br> fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel <br>text search</br> by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel <br>text search</br>. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel <br>text search</br> functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to <br>text search</br> is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel <br>text search</br> functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel <br>text search</br>. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel <br>text search</br> over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel <br>text search</br> functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for <br>text search</br>.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full <br>text search</br> of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for <br>text search</br> Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "A Time Machine for <br>text search</br> Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT <br>text search</br> over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel <br>text search</br> by extending the inverted file index to make it ready for temporal search.",
                "INTRODUCTION In this work we address time-travel <br>text search</br> over temporally versioned document collections.",
                "<br>text search</br> on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Time-travel <br>text search</br>, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates."
            ],
            "translated_annotated_samples": [
                "Una máquina del tiempo para la <br>búsqueda de texto</br> Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La <br>búsqueda de texto</br> en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación.",
                "Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la <br>búsqueda de texto</br> de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal.",
                "En este trabajo abordamos la <br>búsqueda de texto</br> de viaje en el tiempo en colecciones de documentos versionados temporalmente.",
                "La <br>búsqueda de texto</br> en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados.",
                "La <br>búsqueda de texto</br> de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo."
            ],
            "translated_text": "Una máquina del tiempo para la <br>búsqueda de texto</br> Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La <br>búsqueda de texto</br> en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la <br>búsqueda de texto</br> de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la <br>búsqueda de texto</br> de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La <br>búsqueda de texto</br> en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La <br>búsqueda de texto</br> de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "inverted file index": {
            "translated_key": "índice de archivo invertido",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the <br>inverted file index</br> to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied <br>inverted file index</br> [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the <br>inverted file index</br>, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The <br>inverted file index</br> is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the <br>inverted file index</br> that make it ready for time-travel text search. 4.1 <br>inverted file index</br> An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel <br>inverted file index</br> In order to prepare an <br>inverted file index</br> for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel <br>inverted file index</br> are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard <br>inverted file index</br> above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the <br>inverted file index</br> makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the <br>inverted file index</br> to make it ready for temporal search.",
                "The popular well-studied <br>inverted file index</br> [35] is transparently extended to enable time-travel text search. 2.",
                "Unlike the <br>inverted file index</br>, their applicability to text search is not well understood.",
                "TIME-TRAVELINVERTEDFILEINDEX The <br>inverted file index</br> is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the <br>inverted file index</br> that make it ready for time-travel text search. 4.1 <br>inverted file index</br> An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list."
            ],
            "translated_annotated_samples": [
                "Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del <br>índice de archivo invertido</br> para que esté listo para la búsqueda temporal.",
                "El popular y bien estudiado <br>índice de archivo invertido</br> [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2.",
                "A diferencia del <br>índice de archivo invertido</br>, su aplicabilidad a la búsqueda de texto no está bien comprendida.",
                "El índice de <br>archivo invertido</br> es una técnica estándar para la indexación de texto, utilizada en muchos sistemas.",
                "En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al <br>índice de archivo invertido</br> que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un <br>índice de archivo invertido</br> consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del <br>índice de archivo invertido</br> para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado <br>índice de archivo invertido</br> [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del <br>índice de archivo invertido</br>, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de <br>archivo invertido</br> es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al <br>índice de archivo invertido</br> que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un <br>índice de archivo invertido</br> consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. ",
            "candidates": [],
            "error": [
                [
                    "índice de archivo invertido",
                    "índice de archivo invertido",
                    "índice de archivo invertido",
                    "archivo invertido",
                    "índice de archivo invertido",
                    "índice de archivo invertido"
                ]
            ]
        },
        "temporal search": {
            "translated_key": "búsqueda temporal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for <br>temporal search</br>.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for <br>temporal search</br>."
            ],
            "translated_annotated_samples": [
                "Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la <br>búsqueda temporal</br>."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la <br>búsqueda temporal</br>. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "approximate temporal coalescing": {
            "translated_key": "coalescencia temporal aproximada",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce <br>approximate temporal coalescing</br> as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: <br>approximate temporal coalescing</br> leads to extremely long index lists with very poor queryprocessing performance.",
                "The <br>approximate temporal coalescing</br> technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "<br>approximate temporal coalescing</br> reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "<br>approximate temporal coalescing</br> is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in <br>approximate temporal coalescing</br>, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the <br>approximate temporal coalescing</br> technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, <br>approximate temporal coalescing</br> is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that <br>approximate temporal coalescing</br> induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "We introduce <br>approximate temporal coalescing</br> as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: <br>approximate temporal coalescing</br> leads to extremely long index lists with very poor queryprocessing performance.",
                "The <br>approximate temporal coalescing</br> technique that we propose in this section counters this blowup in index-list size.",
                "<br>approximate temporal coalescing</br> reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "<br>approximate temporal coalescing</br> is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example."
            ],
            "translated_annotated_samples": [
                "Introducimos la <br>coalescencia temporal aproximada</br> como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados.",
                "Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La <br>coalescencia temporal aproximada</br> conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre.",
                "La técnica de <br>coalescencia temporal aproximada</br> que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices.",
                "La <br>coalescencia temporal aproximada</br> reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado.",
                "La <br>coalescencia temporal aproximada</br> es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la <br>coalescencia temporal aproximada</br> como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La <br>coalescencia temporal aproximada</br> conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de <br>coalescencia temporal aproximada</br> que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La <br>coalescencia temporal aproximada</br> reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La <br>coalescencia temporal aproximada</br> es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "web archive": {
            "translated_key": "archivo web",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a <br>web archive</br>, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of <br>web archive</br> Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a <br>web archive</br>, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Full Text Search of <br>web archive</br> Collections."
            ],
            "translated_annotated_samples": [
                "Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un <br>archivo web</br>, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas.",
                "Búsqueda de texto completo en colecciones de <br>archivos web</br>."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un <br>archivo web</br>, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de <br>archivos web</br>. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    "archivo web",
                    "archivos web"
                ]
            ]
        },
        "versioned document collection": {
            "translated_key": "colección de documentos versionados",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally <br>versioned document collection</br> in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally <br>versioned document collection</br> D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a <br>versioned document collection</br>, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a <br>versioned document collection</br> are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "We delineate our model of a temporally <br>versioned document collection</br> in Section 3.",
                "MODEL In the present work, we deal with a temporally <br>versioned document collection</br> D that is modeled as described in the following.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a <br>versioned document collection</br>, we obtain one posting per term per document version.",
                "It builds on the observation that most changes in a <br>versioned document collection</br> are minor, leaving large parts of the document untouched."
            ],
            "translated_annotated_samples": [
                "Delimitamos nuestro modelo de una <br>colección de documentos versionados temporalmente</br> en la Sección 3.",
                "En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación.",
                "Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una <br>colección de documentos versionados</br>, obtenemos una entrada por término por versión del documento.",
                "Se basa en la observación de que la mayoría de los cambios en una <br>colección de documentos versionados</br> son menores, dejando grandes partes del documento intactas."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una <br>colección de documentos versionados temporalmente</br> en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una <br>colección de documentos versionados</br>, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una <br>colección de documentos versionados</br> son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    "colección de documentos versionados temporalmente",
                    "colección de documentos versionados",
                    "colección de documentos versionados"
                ]
            ]
        },
        "collaborative authoring environment": {
            "translated_key": "entornos de autoría colaborativa",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, <br>collaborative authoring environment</br>s like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, <br>collaborative authoring environment</br>s like Wikis, or timestamped information feeds."
            ],
            "translated_annotated_samples": [
                "Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, <br>entornos de autoría colaborativa</br> como Wikis o flujos de información con marcas de tiempo."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, <br>entornos de autoría colaborativa</br> como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "timestamped information feed": {
            "translated_key": "flujos de información con marcas de tiempo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or <br>timestamped information feed</br>s.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or <br>timestamped information feed</br>s."
            ],
            "translated_annotated_samples": [
                "Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o <br>flujos de información con marcas de tiempo</br>."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o <br>flujos de información con marcas de tiempo</br>. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "document-content overlap": {
            "translated_key": "superposición de contenido de documentos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the <br>document-content overlap</br> or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the <br>document-content overlap</br> or by pruning portions of the index."
            ],
            "translated_annotated_samples": [
                "TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la <br>superposición de contenido de documentos</br> o podando porciones del índice."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la <br>superposición de contenido de documentos</br> o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "indexing range-based value": {
            "translated_key": "indexar valores basados en rangos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for <br>indexing range-based value</br>s and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "Burrows and Hisgen [10], in a patent description, delineate a method for <br>indexing range-based value</br>s and mention its potential use for searching based on dates associated with documents."
            ],
            "translated_annotated_samples": [
                "Burrows y Hisgen [10], en una descripción de patente, delinean un método para <br>indexar valores basados en rangos</br> y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para <br>indexar valores basados en rangos</br> y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "open source search-engine nutch": {
            "translated_key": "motor de búsqueda de código abierto Nutch",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the <br>open source search-engine nutch</br> to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "Stack [29] reports practical experiences made when adapting the <br>open source search-engine nutch</br> to search web archives."
            ],
            "translated_annotated_samples": [
                "Stack [29] informa sobre experiencias prácticas realizadas al adaptar el <br>motor de búsqueda de código abierto Nutch</br> para buscar en archivos web."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el <br>motor de búsqueda de código abierto Nutch</br> para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "static indexpruning technique": {
            "translated_key": "técnica de poda de índice estática",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": [
                []
            ]
        },
        "validity time-interval": {
            "translated_key": "intervalo de tiempo de validez",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The <br>validity time-interval</br> val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the <br>validity time-interval</br>.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose <br>validity time-interval</br> does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose <br>validity time-interval</br> spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "The <br>validity time-interval</br> val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the <br>validity time-interval</br>.",
                "To this end, sequential reading is extended by skipping all postings whose <br>validity time-interval</br> does not contain t (i.e., t ∈ [tb, te)).",
                "Note that all those postings whose <br>validity time-interval</br> spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists."
            ],
            "translated_annotated_samples": [
                "El <br>intervalo de tiempo de validez</br> val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora).",
                "Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el <br>intervalo de tiempo de validez</br>.",
                "Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo <br>intervalo de tiempo de validez</br> no contiene t (es decir, t ∈ [tb, te)).",
                "Ten en cuenta que todas aquellas publicaciones cuyo <br>intervalo de tiempo de validez</br> abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El <br>intervalo de tiempo de validez</br> val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el <br>intervalo de tiempo de validez</br>. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo <br>intervalo de tiempo de validez</br> no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo <br>intervalo de tiempo de validez</br> abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "sublist materialization": {
            "translated_key": "materialización de sublistas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two <br>sublist materialization</br> techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "<br>sublist materialization</br> Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: <br>sublist materialization</br> it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of <br>sublist materialization</br> using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the <br>sublist materialization</br> techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the <br>sublist materialization</br> improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 <br>sublist materialization</br> We now turn our attention towards evaluating the <br>sublist materialization</br> techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "We develop two <br>sublist materialization</br> techniques to improve index performance that allow trading off space vs. performance. 4.",
                "<br>sublist materialization</br> Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: <br>sublist materialization</br> it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of <br>sublist materialization</br> using an example shown in Figure 2.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the <br>sublist materialization</br> techniques presented in this section.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the <br>sublist materialization</br> improves performance by judiciously replicating entries."
            ],
            "translated_annotated_samples": [
                "Desarrollamos dos técnicas de <br>materialización de sublistas</br> para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4.",
                "La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas.",
                "Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la <br>materialización de sublistas</br> utilizando un ejemplo mostrado en la Figura 2.",
                "Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de <br>materialización de sublistas</br> presentadas en esta sección.",
                "El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la <br>materialización de sublistas</br> mejora el rendimiento al replicar entradas de manera juiciosa."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de <br>materialización de sublistas</br> para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la <br>materialización de sublistas</br> utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de <br>materialización de sublistas</br> presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la <br>materialización de sublistas</br> mejora el rendimiento al replicar entradas de manera juiciosa. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "temporal text index": {
            "translated_key": "índice de texto temporal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address time-travel text search over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to time-travel text search by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable time-travel text search. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended time-travel text search functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired time-travel text search functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the <br>temporal text index</br> we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "It should be noted that index-pruning techniques can be adapted to work along with the <br>temporal text index</br> we propose here. 3."
            ],
            "translated_annotated_samples": [
                "Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el <br>índice de texto temporal</br> que proponemos aquí. 3."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el <br>índice de texto temporal</br> que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| .\n\nX m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. \n\nAddison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "time-travel text search": {
            "translated_key": "búsqueda de texto de viaje en el tiempo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Time Machine for Text Search Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Max-Planck Institute for Informatics Saarbr¨ucken, Germany {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de ABSTRACT Text search over temporally versioned document collections such as web archives has received little attention as a research problem.",
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for <br>time-travel text search</br> by extending the inverted file index to make it ready for temporal search.",
                "We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results.",
                "In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance.",
                "These techniques can be formulated as optimization problems that can be solved to near-optimality.",
                "Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets.",
                "Results unequivocally show that our methods make it possible to build an efficient time machine scalable to large versioned text collections.",
                "Categories and Subject Descriptors H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.3 [Information Search and Retrieval]: Retrieval models, Search process General Terms Algorithms, Experimentation, Performance 1.",
                "INTRODUCTION In this work we address <br>time-travel text search</br> over temporally versioned document collections.",
                "Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t. An increasing number of such versioned document collections is available today including web archives, collaborative authoring environments like Wikis, or timestamped information feeds.",
                "Text search on these collections, however, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed independently and treated as separate documents.",
                "Even worse, for some collections, in particular web archives like the Internet Archive [18], a comprehensive text-search functionality is often completely missing.",
                "<br>time-travel text search</br>, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "For a documentary about a past political scandal, a journalist needs to research early opinions and statements made by the involved politicians.",
                "Sending an appropriate query to a major web search-engine, the majority of returned results contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives.",
                "If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got revealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalists information need.",
                "Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snapshot is considered.",
                "Looking at their evolutionary history, we are faced with even larger data volumes.",
                "As a consequence, na¨ıve approaches to <br>time-travel text search</br> fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to <br>time-travel text search</br> by making the following key contributions: 1.",
                "The popular well-studied inverted file index [35] is transparently extended to enable <br>time-travel text search</br>. 2.",
                "Temporal coalescing is introduced to avoid an indexsize explosion while keeping results highly accurate. 3.",
                "We develop two sublist materialization techniques to improve index performance that allow trading off space vs. performance. 4.",
                "In a comprehensive experimental evaluation our approach is evaluated on the English Wikipedia and parts of the Internet Archive as two large-scale real-world datasets with versioned documents.",
                "The remainder of this paper is organized as follows.",
                "The presented work is put in context with related work in Section 2.",
                "We delineate our model of a temporally versioned document collection in Section 3.",
                "We present our time-travel inverted index in Section 4.",
                "Building on it, temporal coalescing is described in Section 5.",
                "In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Section 7. 2.",
                "RELATED WORK We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with collections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index.",
                "We briefly review work under these categories here.",
                "To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned documents.",
                "Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries.",
                "Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past.",
                "Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents.",
                "Recent work by Nørv˚ag and Nybø [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results.",
                "Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives.",
                "This adaptation, however, does not provide the intended <br>time-travel text search</br> functionality.",
                "In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28].",
                "Unlike the inverted file index, their applicability to text search is not well understood.",
                "Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size.",
                "Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context.",
                "More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size.",
                "None of the approaches, however, considers time explicitly or provides the desired <br>time-travel text search</br> functionality.",
                "Static indexpruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result.",
                "They also do not consider temporal aspects of documents, and thus are technically quite different from our proposal despite having a shared goal of index-size reduction.",
                "It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here. 3.",
                "MODEL In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following.",
                "Each document d ∈ D is a sequence of its versions d = dt1 , dt2 , . . . .",
                "Each version dti has an associated timestamp ti reflecting when the version was created.",
                "Each version is a vector of searchable terms or features.",
                "Any modification to a document version results in the insertion of a new version with corresponding timestamp.",
                "We employ a discrete definition of time, so that timestamps are non-negative integers.",
                "The deletion of a document at time ti, i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special tombstone version ⊥.",
                "The validity time-interval val(dti ) of a version dti is [ti, ti+1), if a newer version with associated timestamp ti+1 exists, and [ti, now) otherwise where now points to the greatest possible value of a timestamp (i.e., ∀t : t < now).",
                "Putting all this together, we define the state Dt of the collection at time t (i.e., the set of versions valid at t that are not deletions) as Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} .",
                "As mentioned earlier, we want to enrich a keyword query q with a timestamp t, so that q be evaluated over Dt , i.e., the state of the collection at time t. The enriched time-travel query is written as q t for brevity.",
                "As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well.",
                "For our considered setting, we slightly adapt Okapi BM25 as w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) .",
                "In the above formula, the relevance w(q t , dti ) of a document version dti to the time-travel query q t is defined.",
                "We reiterate that q t is evaluated over Dt so that only the version dti valid at time t is considered.",
                "The first factor wtf (v, dti ) in the summation, further referred to as the tfscore is defined as wtf (v, dti ) = (k1 + 1) · tf(v, dti ) k1 · ((1 − b) + b · dl(d ti ) avdl(ti) ) + tf(v, dti ) .",
                "It considers the plain term frequency tf(v, dti ) of term v in version dti normalizing it, taking into account both the length dl(dti ) of the version and the average document length avdl(ti) in the collection at time ti.",
                "The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively.",
                "The second factor widf (v, t), which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 where N(t) = |Dt | is the collection size at time t and df(v, t) gives the number of documents in the collection that contain the term v at time t. While the idf-score depends on the whole corpus as of the query time t, the tf-score is specific to each version. 4.",
                "TIME-TRAVELINVERTEDFILEINDEX The inverted file index is a standard technique for text indexing, deployed in many systems.",
                "In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for <br>time-travel text search</br>. 4.1 Inverted File Index An inverted file index consists of a vocabulary, commonly organized as a B+-Tree, that maps each term to its idfscore and inverted list.",
                "The index list Lv belonging to term v contains postings of the form ( d, p ) where d is a document-identifier and p is the so-called payload.",
                "The payload p contains information about the term frequency of v in d, but may also include positional information about where the term appears in the document.",
                "The sort-order of index lists depends on which queries are to be supported efficiently.",
                "For Boolean queries it is favorable to sort index lists in document-order.",
                "Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents [1, 2, 9, 15, 31].",
                "A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists.",
                "For an excellent recent survey about inverted file indexes we refer to [35]. 4.2 Time-Travel Inverted File Index In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information.",
                "The main idea for inverted lists is that we include a validity timeinterval [tb, te) in postings to denote when the payload information was valid.",
                "The postings in our time-travel inverted file index are thus of the form ( d, p, [tb, te) ) where d and p are defined as in the standard inverted file index above and [tb, te) is the validity time-interval.",
                "As a concrete example, in our implementation, for a version dti having the Okapi BM25 tf-score wtf (v, dti ) for term v, the index list Lv contains the posting ( d, wtf (v, dti ), [ti, ti+1) ) .",
                "Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+Tree.",
                "Unlike the tf-score, the idf-score of every term could vary with every change in the corpus.",
                "Therefore, we take a simplified approach to idf-score maintenance, by computing idf-scores for all terms in the corpus at specific (possibly periodic) times. 4.3 Query Processing During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary.",
                "Then, index lists are sequentially read from disk, thereby accumulating the information contained in the postings.",
                "We transparently extend the sequential reading, which is - to the best of our knowledgecommon to all query processing techniques on inverted file indexes, thus making them suitable for time-travel queryprocessing.",
                "To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t ∈ [tb, te)).",
                "Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost.",
                "As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.",
                "We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists.",
                "As a consequence, existing query-processing techniques and most optimizations (e.g., compression techniques) remain equally applicable. 5.",
                "TEMPORAL COALESCING If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version.",
                "For frequent terms and large highly-dynamic collections, this time score non-coalesced coalesced Figure 1: Approximate Temporal Coalescing leads to extremely long index lists with very poor queryprocessing performance.",
                "The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size.",
                "It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched.",
                "As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all.",
                "Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded.",
                "This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document.",
                "Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example.",
                "The notion of temporal coalescing was originally introduced in temporal database research by B¨ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.",
                "We next formally state the problem dealt with in approximate temporal coalescing, and discuss the computation of optimal and approximate solutions.",
                "Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list Lv.",
                "As an input we are given a sequence of temporally adjacent postings I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .",
                "Each sequence represents a contiguous time period during which the term was present in a single document d. If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately.",
                "We seek to generate the minimal length output sequence of postings O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) , that adheres to the following constraints: First, O and I must cover the same time-range, i.e., ti = tj and tn = tm.",
                "Second, when coalescing a subsequence of postings of the input into a single posting of the output, we want the approximation error to be below a threshold .",
                "In other words, if (d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and O respectively, then the following must hold for a chosen error function and a threshold : tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .",
                "In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O, defined as: errrel(pi, pj) = |pi − pj| / |pi| .",
                "Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points (ti, pi) that uses a minimal number of segments while retaining the above approximation guarantee.",
                "Similar problems occur in time-series segmentation [21, 30] and histogram construction [19, 20].",
                "Typically dynamic programming is applied to obtain an optimal solution in O(n2 m∗ ) [20, 30] time with m∗ being the number of segments in an optimal sequence.",
                "In our setting, as a key difference, only a guarantee on the local error is retained - in contrast to a guarantee on the global error in the aforementioned settings.",
                "Exploiting this fact, an optimal solution is computable by means of induction [24] in O(n2 ) time.",
                "Details of the optimal algorithm are omitted here but can be found in the accompanying technical report [5].",
                "The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work.",
                "As an alternative, we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in [21].",
                "This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.",
                "Algorithm 1 Temporal Coalescing (Approximate) 1: I = ( d, pi, [ti, ti+1) ), . . .",
                "O = 2: pmin = pi pmax = pi p = pi tb = ti te = ti+1 3: for ( d, pj, [tj, tj+1) ) ∈ I do 4: pmin = min( pmin, pj ) pmax = max( pmax, pj ) 5: p = optrep(pmin, pmax) 6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then 7: pmin = pmin pmax = pmax p = p te = tj+1 8: else 9: O = O ∪ ( d, p, [tb, te) ) 10: pmin = pj pmax = pj p = pj tb = tj te = tj+1 11: end if 12: end for 13: O = O ∪ ( d, p, [tb, te) ) Algorithm 1 makes one pass over the input sequence I.",
                "While doing so, it coalesces sequences of postings having maximal length.",
                "The optimal representative for a sequence of postings depends only on their minimal and maximal payload (pmin and pmax) and can be looked up using optrep in O(1) (see [16] for details).",
                "When reading the next posting, the algorithm tries to add it to the current sequence of postings.",
                "It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee.",
                "If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized.",
                "The time complexity of the algorithm is in O(n).",
                "Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O(|Lv| log |Lv|) for sorting the index list and chopping it up into subsequences for each document. 6.",
                "SUBLIST MATERIALIZATION Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings.",
                "Temporal coalescing implicitly addresses this problem by reducing the overall index list size, but still a significant overhead remains.",
                "In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index.",
                "Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist.",
                "Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists.",
                "Thus, in order to process the query q t time t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 document 1 2 3 4 5 6 7 8 9 10 Figure 2: Sublist Materialization it is sufficient to scan any materialized sublist whose timeinterval contains t. We illustrate the idea of sublist materialization using an example shown in Figure 2.",
                "The index list Lv visualized in the figure contains a total of 10 postings from three documents d1, d2, and d3.",
                "For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t1, . . . , t10 and numbered the postings themselves as 1, . . . , 10.",
                "Now, consider the processing of a query q t with t ∈ [t1, t2) using this inverted list.",
                "Although only three postings (postings 1, 5 and 8) are valid at time t, the whole inverted list has to be read in the worst case.",
                "Suppose that we split the time axis of the list at time t2, forming two sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9, 10} respectively.",
                "Then, we can process the above query with optimal cost by reading only those postings that existed at this t. At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section.",
                "However, we reiterate that our main objective is to improve the efficiency of processing queries, not to reduce the index size alone.",
                "The use of temporal coalescing improves the performance by reducing the index size, while the sublist materialization improves performance by judiciously replicating entries.",
                "Further, the two techniques, can be applied separately and are independent.",
                "If applied in conjunction, though, there is a synergetic effect - sublists that are materialized from a temporally coalesced index are generally smaller.",
                "We employ the notation Lv : [ti, tj) to refer to the materialized sublist for the time interval [ti, tj), that is formally defined as, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .",
                "To aid the presentation in the rest of the paper, we first provide some definitions.",
                "Let T = t1 . . . tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv.",
                "Then we define E = { [ti, ti+1) | 1 ≤ i < n} to be the set of elementary time intervals.",
                "We refer to the set of time intervals for which sublists are materialized as M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } , and demand ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m , i.e., the time intervals in M must completely cover the time interval [t1, tn), so that time-travel queries q t for all t ∈ [t1, tn) can be processed.",
                "We also assume that intervals in M are disjoint.",
                "We can make this assumption without ruling out any optimal solution with regard to space or performance defined below.",
                "The space required for the materialization of sublists in a set M is defined as S( M ) = X m∈M |Lv : m| , i.e., the total length of all lists in M. Given a set M, we let π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote the time interval that is used to process queries q t with t ∈ [ti, ti+1).",
                "The performance of processing queries q t for t ∈ [ti, ti+1) inversely depends on its processing cost PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , which is assumed to be proportional to the length of the list Lv : π( [ti, ti+1) ).",
                "Thus, in order to optimize the performance of processing queries we minimize their processing costs. 6.1 Performance/Space-Optimal Approaches One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E. In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved.",
                "Therefore, we will refer to this approach as Popt in the remainder.",
                "The initial approach described above that keeps only the full list Lv and thus picks M = { [t1, tn) } is referred to as Sopt in the remainder.",
                "This approach requires minimal space, since it keeps each posting exactly once.",
                "Popt and Sopt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good performance.",
                "The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach. 6.2 Performance-Guarantee Approach The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists.",
                "In the example illustrated in Figure 2 materialized sublists for [t1, t2) and [t2, t3) differ only by one posting.",
                "If the sublist for [t1, t3) was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t ∈ [t1, t3).",
                "The technique presented next is driven by the idea that significant space savings over Popt are achievable, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guarantee relative to the optimum is to be retained.",
                "In detail, the technique, which we refer to as PG (Performance Guarantee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time interval [ti, ti+1) (and thus for any query q t with t ∈ [ti, ti+1)) that performance is worse than optimal by at most a factor of γ ≥ 1.",
                "Formally, this problem can be stated as argmin M S( M ) s.t. ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .",
                "An optimal solution to the problem can be computed by means of induction using the recurrence C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} , where C( [t1, tj) ) is the optimal cost (i.e., the space required) for the prefix subproblem { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } and condition stands for ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| .",
                "Intuitively, the recurrence states that an optimal solution for [t1, tk+1) be combined from an optimal solution to a prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1) that can be materialized without violating the performance guarantee.",
                "Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5].",
                "The time complexity of the algorithm is in O(n2 ) - for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes |L : [ti, tj)| are precomputed.",
                "The space complexity is in O(n2 ) - the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems. 6.3 Space-Bound Approach So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space.",
                "In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit.",
                "The technique presented next, which is named SB, tackles this very problem.",
                "The space restriction is modeled by means of a user-specified parameter κ ≥ 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt.",
                "The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance).",
                "In the definition of the expected processing cost, P( [ti, ti+1) ) denotes the probability of a query time-point being in [ti, ti+1).",
                "Formally, this space-bound sublist-materialization problem can be stated as argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.",
                "X m∈M |Lv : m| ≤ κ |Lv| .",
                "The problem can be solved by using dynamic programming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best materialization decision from the previous time intervals, and keeping track of the required space consumption for materialization.",
                "A detailed description of the algorithm is omitted here, but can be found in the accompanying technical report [5].",
                "Unfortunately, the algorithm has time complexity in O(n3 |Lv|) and its space complexity is in O(n2 |Lv|), which is not practical for large data sets.",
                "We obtain an approximate solution to the problem using simulated annealing [22, 23].",
                "Simulated annealing takes a fixed number R of rounds to explore the solution space.",
                "In each round a random successor of the current solution is looked at.",
                "If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept).",
                "A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution.",
                "If it achieves higher expected processing cost, it is randomly accepted with probability e−∆/r where ∆ is the increase in expected processing cost and R ≥ r ≥ 1 denotes the number of remaining rounds.",
                "In addition, throughout all rounds, the method keeps track of the best solution seen so far.",
                "The solution space for the problem at hand can be efficiently explored.",
                "As we argued above, we solely have to look at sets M that completely cover the time interval [t1, tn) and do not contain overlapping time intervals.",
                "We represent such a set M as an array of n boolean variables b1 . . . bn that convey the boundaries of time intervals in the set.",
                "Note that b1 and bn are always set to true.",
                "Initially, all n − 2 intermediate variables assume false, which corresponds to the set M = { [t1, tn) }.",
                "A random successor can now be easily generated by switching the value of one of the n − 2 intermediate variables.",
                "The time complexity of the method is in O(n2 ) - the expected processing cost must be computed in each round.",
                "Its space complexity is in O(n) - for keeping the n boolean variables.",
                "As a side remark note that for κ = 1.0 the SB method does not necessarily produce the solution that is obtained from Sopt, but may produce a solution that requires the same amount of space while achieving better expected performance. 7.",
                "EXPERIMENTAL EVALUATION We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper. 7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.",
                "All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.",
                "All data and indexes are kept in an Oracle 10g database that runs on the same machine.",
                "For our experiments we used two different datasets.",
                "The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.",
                "This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our download).",
                "We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).",
                "This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.",
                "We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).",
                "The thus extracted queries contained a total of 422 distinct terms.",
                "For each extracted query, we randomly picked a time point for each month covered by the dataset.",
                "This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.",
                "The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.",
                "We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.",
                "This included a total of 502,617 documents with 8,687,108 versions (µ = 17.28 and σ = 13.79).",
                "We built a corresponding query workload as mentioned before, this time choosing keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc. ), and randomly sampling a time point for every month within the two year period spanned by the dataset.",
                "Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.",
                "In total 522 terms appear in the extracted queries.",
                "The collection statistics (i.e., N and avdl) and term statistics (i.e., DF) were computed at monthly granularity for both datasets. 7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique, described in Section 5, in terms of index-size reduction and its effect on the result quality.",
                "For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.",
                "WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.",
                "As these results demonstrate, approximate temporal coalescing is highly effective in reducing index size.",
                "Even a small threshold value, e.g. = 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.",
                "Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.",
                "Index size continues to reduce on both datasets, as we increase the value of .",
                "How does the reduction in index size affect the query results?",
                "In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.",
                "We used the following two measures for comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k . (ii) Kendalls τ (see [7, 14] for a detailed definition) at cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).",
                "Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.",
                "Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.",
                "It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.",
                "For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendalls τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendalls τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendalls τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendalls τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendalls τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.",
                "Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.",
                "For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.",
                "On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values. 7.3 Sublist Materialization We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6.",
                "For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.",
                "In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations.",
                "However, note that the postings in the materialized sublists still retain their original timestamps.",
                "For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.",
                "The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.",
                "To assess performance we compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform probability distribution among query time-points.",
                "We report the mean EPC, as well as the 5%- and 95%-percentile.",
                "In other words, the mean EPC reflects the expected length of the index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.",
                "The Sopt and Popt approaches are, by their definition, parameter-free.",
                "For the PG approach, we varied its parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.",
                "Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.",
                "Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.",
                "Table 2 lists the obtained space and performance figures.",
                "Note that EPC values are smaller on WIKI than on UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.",
                "Based on the depicted results, we make the following key observations. i) As expected, Popt achieves optimal performance at the cost of an enormous space consumption.",
                "Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.",
                "The PG and SB methods, for different values of their respective parameter, produce solutions whose space and performance lie in between the extremes that Popt and Sopt represent. ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.",
                "We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 8.",
                "CONCLUSIONS In this work we have developed an efficient solution for <br>time-travel text search</br> over temporally versioned document collections.",
                "Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.",
                "The present work opens up many interesting questions for future research, e.g. : How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?.",
                "How can we extend the approach for queries q [tb, te] specifying a time interval instead of a time point?",
                "How can the described <br>time-travel text search</br> functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)? 9.",
                "ACKNOWLEDGMENTS We are grateful to the anonymous reviewers for their valuable comments - in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Section 5 and Section 6.2. 10.",
                "REFERENCES [1] V. N. Anh and A. Moffat.",
                "Pruned Query Evaluation Using Pre-Computed Impacts.",
                "In SIGIR, 2006. [2] V. N. Anh and A. Moffat.",
                "Pruning Strategies for Mixed-Mode Querying.",
                "In CIKM, 2006.",
                "WIKI UKGOV S(M) EPC S(M) EPC 5% Mean 95% 5% Mean 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43 Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10) [3] P. G. Anick and R. A. Flynn.",
                "Versioning a Full-Text Information Retrieval System.",
                "In SIGIR, 1992. [4] R. A. Baeza-Yates and B. Ribeiro-Neto.",
                "Modern Information Retrieval.",
                "Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann, and G. Weikum.",
                "A Time Machine for Text search.",
                "Technical Report MPI-I-2007-5-002, Max-Planck Institute for Informatics, 2007. [6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.",
                "Coalescing in Temporal Databases.",
                "In VLDB, 1996. [7] P. Boldi, M. Santini, and S. Vigna.",
                "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations.",
                "In WAW, 2004. [8] A.",
                "Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.",
                "Indexing Shared Content in Information Retrieval Systems.",
                "In EDBT, 2006. [9] C. Buckley and A. F. Lewit.",
                "Optimization of Inverted Vector Searches.",
                "In SIGIR, 1985. [10] M. Burrows and A. L. Hisgen.",
                "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations.",
                "U.S. Patent 5,915,251, 1999. [11] S. B¨uttcher and C. L. A. Clarke.",
                "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems.",
                "In CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer.",
                "Static Index Pruning for Information Retrieval Systems.",
                "In SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar.",
                "Comparing Top k Lists.",
                "SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, and M. Naor.",
                "Optimal Aggregation Algorithms for Middleware.",
                "J. Comput.",
                "Syst.",
                "Sci., 66(4):614-656, 2003. [16] S. Guha, K. Shim, and J.",
                "Woo.",
                "REHIST: Relative Error Histogram Construction Algorithms.",
                "In VLDB, 2004. [17] M. Hersovici, R. Lempel, and S. Yogev.",
                "Efficient Indexing of Versioned Document Sequences.",
                "In ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala.",
                "Balancing Histogram Optimality and Practicality for Query Result Size Estimation.",
                "In SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik, and T. Suel.",
                "Optimal Histograms with Quality Guarantees.",
                "In VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani.",
                "An Online Algorithm for Segmenting Time Series.",
                "In ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.",
                "Optimization by Simulated Annealing.",
                "Science, 220(4598):671-680, 1983. [23] J. Kleinberg and E. Tardos.",
                "Algorithm Design.",
                "Addison-Wesley, 2005. [24] U. Manber.",
                "Introduction to Algorithms: A Creative Approach.",
                "Addison-Wesley, 1989. [25] K. Nørv˚ag and A. O. N. Nybø.",
                "DyST: Dynamic and Scalable Temporal Text Indexing.",
                "In TIME, 2006. [26] J. M. Ponte and W. B. Croft.",
                "A Language Modeling Approach to Information Retrieval.",
                "In SIGIR, 1998. [27] S. E. Robertson and S. Walker.",
                "Okapi/Keenbow at TREC-8.",
                "In TREC, 1999. [28] B. Salzberg and V. J. Tsotras.",
                "Comparison of Access Methods for Time-Evolving Data.",
                "ACM Comput.",
                "Surv., 31(2):158-221, 1999. [29] M. Stack.",
                "Full Text Search of Web Archive Collections.",
                "In IWAW, 2006. [30] E. Terzi and P. Tsaparas.",
                "Efficient Algorithms for Sequence Segmentation.",
                "In SIAM-DM, 2006. [31] M. Theobald, G. Weikum, and R. Schenkel.",
                "Top-k Query Evaluation with Probabilistic Guarantees.",
                "In VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang and T. Suel.",
                "Efficient Search in Large Textual Collections with Redundancy.",
                "In WWW, 2007. [35] J. Zobel and A. Moffat.",
                "Inverted Files for Text Search Engines.",
                "ACM Comput.",
                "Surv., 38(2):6, 2006."
            ],
            "original_annotated_samples": [
                "As a consequence, there is no scalable and principled solution to search such a collection as of a specified time t. In this work, we address this shortcoming and propose an efficient solution for <br>time-travel text search</br> by extending the inverted file index to make it ready for temporal search.",
                "INTRODUCTION In this work we address <br>time-travel text search</br> over temporally versioned document collections.",
                "<br>time-travel text search</br>, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates.",
                "As a consequence, na¨ıve approaches to <br>time-travel text search</br> fail, and viable approaches must scale-up well to such large data volumes.",
                "This paper presents an efficient solution to <br>time-travel text search</br> by making the following key contributions: 1."
            ],
            "translated_annotated_samples": [
                "Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la <br>búsqueda de texto de viaje en el tiempo</br> mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal.",
                "En este trabajo abordamos la <br>búsqueda de texto de viaje en el tiempo</br> en colecciones de documentos versionados temporalmente.",
                "La <br>búsqueda de texto de viaje en el tiempo</br>, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo.",
                "Como consecuencia, los enfoques ingenuos para la <br>búsqueda de texto en viajes en el tiempo</br> fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes.",
                "Este documento presenta una solución eficiente para la <br>búsqueda de texto de viaje en el tiempo</br> al hacer las siguientes contribuciones clave: 1."
            ],
            "translated_text": "Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la <br>búsqueda de texto de viaje en el tiempo</br> mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la <br>búsqueda de texto de viaje en el tiempo</br> en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La <br>búsqueda de texto de viaje en el tiempo</br>, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la <br>búsqueda de texto en viajes en el tiempo</br> fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la <br>búsqueda de texto de viaje en el tiempo</br> al hacer las siguientes contribuciones clave: 1. ",
            "candidates": [],
            "error": [
                [
                    "búsqueda de texto de viaje en el tiempo",
                    "búsqueda de texto de viaje en el tiempo",
                    "búsqueda de texto de viaje en el tiempo",
                    "búsqueda de texto en viajes en el tiempo",
                    "búsqueda de texto de viaje en el tiempo"
                ]
            ]
        }
    }
}