{
    "id": "S0885230816300043",
    "original_text": "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.",
    "original_translation": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante.",
    "original_sentences": [
        "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
        "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
        "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
        "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
        "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
        "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
    ],
    "error_count": 1,
    "keys": {
        "aCMLLR transforms": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the <br>aCMLLR transforms</br> on the baseline GMM–HMM model.",
                "However, training show-based <br>aCMLLR transforms</br> on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based <br>aCMLLR transforms</br> was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "This new model only provided an improvement of 0.3%, similar to using the <br>aCMLLR transforms</br> on the baseline GMM–HMM model.",
                "However, training show-based <br>aCMLLR transforms</br> on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based <br>aCMLLR transforms</br> was tested."
            ],
            "translated_annotated_samples": [
                "Output: Este nuevo modelo solo proporcionó una mejora del 0.3%, similar a usar las transformaciones <br>aCMLLR</br> en el modelo GMM-HMM base.",
                "Output: Sin embargo, el entrenamiento de <br>transformaciones aCMLLR basadas en espectáculos</br> sobre el modelo entrenado de forma adaptativa aumentó la mejora a 0.8% absoluto.",
                "Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR sobre el modelo aNAT y transformaciones <br>aCMLLR basadas en el espectáculo</br>."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Output: Este nuevo modelo solo proporcionó una mejora del 0.3%, similar a usar las transformaciones <br>aCMLLR</br> en el modelo GMM-HMM base. Output: Sin embargo, el entrenamiento de <br>transformaciones aCMLLR basadas en espectáculos</br> sobre el modelo entrenado de forma adaptativa aumentó la mejora a 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR sobre el modelo aNAT y transformaciones <br>aCMLLR basadas en el espectáculo</br>. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    "aCMLLR",
                    "transformaciones aCMLLR basadas en espectáculos",
                    "aCMLLR basadas en el espectáculo"
                ],
                [
                    "aCMLLR",
                    "transformaciones aCMLLR basadas en espectáculos",
                    "aCMLLR basadas en el espectáculo"
                ]
            ]
        },
        "adaptive retraining of the GMM–HMM parameters": {
            "translated_key": "reentrenamiento adaptativo de los parámetros GMM-HMM",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an <br>adaptive retraining of the GMM–HMM parameters</br> following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "The final set of experiments involved an <br>adaptive retraining of the GMM–HMM parameters</br> following the aNAT procedure."
            ],
            "translated_annotated_samples": [
                "Output: El conjunto final de experimentos involucró un <br>reentrenamiento adaptativo de los parámetros GMM-HMM</br> siguiendo el procedimiento aNAT."
            ],
            "translated_text": "Output: El conjunto final de experimentos involucró un <br>reentrenamiento adaptativo de los parámetros GMM-HMM</br> siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "adaptive training": {
            "translated_key": "entrenamiento adaptativo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how <br>adaptive training</br> provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "This showed how <br>adaptive training</br> provided a better flexibility of the model to adapt to specific background conditions existing in each show."
            ],
            "translated_annotated_samples": [
                "Output: Esto mostró cómo el <br>entrenamiento adaptativo</br> proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones específicas del fondo existentes en cada espectáculo."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Output: Esto mostró cómo el <br>entrenamiento adaptativo</br> proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones específicas del fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "aNAT model": {
            "translated_key": "aNAT",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the <br>aNAT model</br> and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "Finally, the factorisation approach using MLLR speaker transforms on top of the <br>aNAT model</br> and show-based aCMLLR transforms was tested."
            ],
            "translated_annotated_samples": [
                "Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR encima del modelo <br>aNAT</br> y transformaciones aCMLLR basadas en el espectáculo."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de altavoz MLLR encima del modelo <br>aNAT</br> y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "aNAT procedure": {
            "translated_key": "aNAT",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the <br>aNAT procedure</br>.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the <br>aNAT procedure</br>."
            ],
            "translated_annotated_samples": [
                "Output: El conjunto final de experimentos involucró un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento <br>aNAT</br>."
            ],
            "translated_text": "Output: El conjunto final de experimentos involucró un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento <br>aNAT</br>. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "factorisation approach using MLLR speaker transforms": {
            "translated_key": "factorización utilizando transformaciones de altavoz MLLR",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the <br>factorisation approach using MLLR speaker transforms</br> on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "Finally, the <br>factorisation approach using MLLR speaker transforms</br> on top of the aNAT model and show-based aCMLLR transforms was tested."
            ],
            "translated_annotated_samples": [
                "Finalmente, se probó el enfoque de <br>factorización utilizando transformaciones de altavoz MLLR</br> sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de <br>factorización utilizando transformaciones de altavoz MLLR</br> sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "GMM–HMM model": {
            "translated_key": "GMM-HMM",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline <br>GMM–HMM model</br>.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline <br>GMM–HMM model</br>."
            ],
            "translated_annotated_samples": [
                "Output: Este nuevo modelo solo proporcionó una mejora del 0.3%, similar a usar las transformadas aCMLLR en el modelo base <br>GMM-HMM</br>."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Output: Este nuevo modelo solo proporcionó una mejora del 0.3%, similar a usar las transformadas aCMLLR en el modelo base <br>GMM-HMM</br>. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "speaker adaptation": {
            "translated_key": "adaptación del orador",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers <br>speaker adaptation</br>."
            ],
            "original_annotated_samples": [
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers <br>speaker adaptation</br>."
            ],
            "translated_annotated_samples": [
                "Output: Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de oradores en esta tarea y cómo esto realmente obstaculiza la <br>adaptación del orador</br>."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Output: Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de oradores en esta tarea y cómo esto realmente obstaculiza la <br>adaptación del orador</br>. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "speaker clustering": {
            "translated_key": "altavoces",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate <br>speaker clustering</br> in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate <br>speaker clustering</br> in this task and how this actually hampers speaker adaptation."
            ],
            "translated_annotated_samples": [
                "Output: Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de <br>altavoces</br> en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Sin embargo, el entrenamiento de transformaciones basadas en espectáculos aCMLLR sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Output: Esto solo aumentó la mejora al 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de <br>altavoces</br> en esta tarea y cómo esto realmente obstaculiza la adaptación del altavoz. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "training show-based aCMLLR transforms": {
            "translated_key": "transformaciones aCMLLR basadas en el espectáculo de entrenamiento",
            "is_in_text": true,
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, <br>training show-based aCMLLR transforms</br> on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [
                "However, <br>training show-based aCMLLR transforms</br> on top of the adaptively trained model boosted the improvement to 0.8% absolute."
            ],
            "translated_annotated_samples": [
                "Output: Sin embargo, la aplicación de <br>transformaciones aCMLLR basadas en el espectáculo de entrenamiento</br> sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto."
            ],
            "translated_text": "El conjunto final de experimentos implicó un reentrenamiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo solo proporcionó una mejora del 0.3%, similar al uso de las transformaciones aCMLLR en el modelo GMM-HMM base. Output: Sin embargo, la aplicación de <br>transformaciones aCMLLR basadas en el espectáculo de entrenamiento</br> sobre el modelo entrenado de forma adaptativa aumentó la mejora a un 0.8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad al modelo para adaptarse a las condiciones específicas de fondo existentes en cada espectáculo. Finalmente, se probó el enfoque de factorización utilizando transformaciones de hablante MLLR sobre el modelo aNAT y transformaciones aCMLLR basadas en el espectáculo. Esto solo aumentó la mejora a un 0.9% absoluto (2.9% relativo), lo cual refleja la dificultad de realizar un agrupamiento preciso de los hablantes en esta tarea y cómo esto realmente obstaculiza la adaptación del hablante. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}