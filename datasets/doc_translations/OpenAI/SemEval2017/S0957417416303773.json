{
    "id": "S0957417416303773",
    "original_text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.",
    "original_translation": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos.",
    "original_sentences": [
        "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
        "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
        "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
        "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
        "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
        "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
        "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
        "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
    ],
    "error_count": 0,
    "keys": {
        "a set of orthogonal vectors": {
            "translated_key": "un conjunto de vectores ortogonales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is <br>a set of orthogonal vectors</br> sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "The result is <br>a set of orthogonal vectors</br> sorted in descending order of achieved variance."
            ],
            "translated_annotated_samples": [
                "Output: El resultado es <br>un conjunto de vectores ortogonales</br> ordenados en orden descendente de varianza lograda."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. Output: El resultado es <br>un conjunto de vectores ortogonales</br> ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "data mining": {
            "translated_key": "minería de datos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of <br>data mining</br> many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In the recent years and mainly motivated by the impulse of <br>data mining</br> many methods for dimensionality reduction have arisen."
            ],
            "translated_annotated_samples": [
                "Output: En los últimos años y principalmente motivado por el impulso de la <br>minería de datos</br>, han surgido muchos métodos para la reducción de dimensionalidad."
            ],
            "translated_text": "Output: En los últimos años y principalmente motivado por el impulso de la <br>minería de datos</br>, han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "dimensionality reduction": {
            "translated_key": "reducción de dimensionalidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for <br>dimensionality reduction</br> have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for <br>dimensionality reduction</br> have arisen."
            ],
            "translated_annotated_samples": [
                "Output: En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la <br>reducción de dimensionalidad</br>."
            ],
            "translated_text": "Output: En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la <br>reducción de dimensionalidad</br>. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "finds the mutually-uncorrelated vectors": {
            "translated_key": "encuentra los vectores mutuamente no correlacionados",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that <br>finds the mutually-uncorrelated vectors</br> onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that <br>finds the mutually-uncorrelated vectors</br> onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "Output: En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que <br>encuentra los vectores mutuamente no correlacionados</br> sobre los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). Output: En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que <br>encuentra los vectores mutuamente no correlacionados</br> sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "linear PCA": {
            "translated_key": "PCA lineal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (<br>linear PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, the simplest version of PCA (<br>linear PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "En un espacio vectorial de N dimensiones, la versión más simple de PCA (<br>PCA lineal</br>) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (<br>PCA lineal</br>) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "N-dimensional vector space basis": {
            "translated_key": "base del espacio vectorial de N dimensiones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the <br>N-dimensional vector space basis</br>, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In this sense, the original KPIs constitute the <br>N-dimensional vector space basis</br>, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance."
            ],
            "translated_annotated_samples": [
                "En este sentido, los KPI originales constituyen la <br>base del espacio vectorial de N dimensiones</br>, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la <br>base del espacio vectorial de N dimensiones</br>, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "N^ synthetic KPIs": {
            "translated_key": "KPIs sintéticos N^",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the <br>N^ synthetic KPIs</br> represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the <br>N^ synthetic KPIs</br> represent the orthogonal vectors with the highest variance."
            ],
            "translated_annotated_samples": [
                "Output: En este sentido, los KPI originales constituyen la base del espacio vectorial N-dimensional, mientras que los <br>KPIs sintéticos N^</br> representan los vectores ortogonales con la mayor varianza."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. Output: En este sentido, los KPI originales constituyen la base del espacio vectorial N-dimensional, mientras que los <br>KPIs sintéticos N^</br> representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "PCA": {
            "translated_key": "PCA",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (<br>PCA</br>) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of <br>PCA</br> (linear <br>PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "Within these, it is worth highlighting the Principal Component Analysis method (<br>PCA</br>) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of <br>PCA</br> (linear <br>PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "Output: Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (<br>PCA</br>) (Jolliffe, 2002).",
                "Output: En un espacio vectorial de N dimensiones, la versión más simple de <br>PCA</br> (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Output: Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (<br>PCA</br>) (Jolliffe, 2002). Output: En un espacio vectorial de N dimensiones, la versión más simple de <br>PCA</br> (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "Principal Component Analysis method": {
            "translated_key": "Análisis de Componentes Principales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the <br>Principal Component Analysis method</br> (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "Within these, it is worth highlighting the <br>Principal Component Analysis method</br> (PCA) (Jolliffe, 2002)."
            ],
            "translated_annotated_samples": [
                "Output: Dentro de estos, vale la pena destacar el método de <br>Análisis de Componentes Principales</br> (PCA) (Jolliffe, 2002)."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Output: Dentro de estos, vale la pena destacar el método de <br>Análisis de Componentes Principales</br> (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the first N^": {
            "translated_key": "los primeros N^",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, <br>the first N^</br>, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "However, only a small set of them, <br>the first N^</br>, is enough to account for most of the variance of the data."
            ],
            "translated_annotated_samples": [
                "Output: Sin embargo, solo un pequeño conjunto de ellos, <br>los primeros N^</br>, es suficiente para explicar la mayor parte de la varianza de los datos."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Output: Sin embargo, solo un pequeño conjunto de ellos, <br>los primeros N^</br>, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the projection of the samples generates the highest variances": {
            "translated_key": "la proyección de las muestras genera las varianzas más altas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which <br>the projection of the samples generates the highest variances</br>.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which <br>the projection of the samples generates the highest variances</br>."
            ],
            "translated_annotated_samples": [
                "Output: En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales <br>la proyección de las muestras genera las varianzas más altas</br>."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). Output: En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales <br>la proyección de las muestras genera las varianzas más altas</br>. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the simplest version of PCA": {
            "translated_key": "la versión más simple de PCA",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, <br>the simplest version of PCA</br> (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, <br>the simplest version of PCA</br> (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "Output: En un espacio vectorial de N dimensiones, <br>la versión más simple de PCA</br> (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). Output: En un espacio vectorial de N dimensiones, <br>la versión más simple de PCA</br> (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the variance of the projection of the samples is maximum": {
            "translated_key": "la varianza de la proyección de las muestras es máxima",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which <br>the variance of the projection of the samples is maximum</br>.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "The first of these vectors is that onto which <br>the variance of the projection of the samples is maximum</br>."
            ],
            "translated_annotated_samples": [
                "El primero de estos vectores es aquel en el que <br>la varianza de la proyección de las muestras es máxima</br>."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que <br>la varianza de la proyección de las muestras es máxima</br>. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs ortogonales sintéticos. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "up to N synthetic orthogonal KPIs": {
            "translated_key": "hasta N KPIs ortogonales sintéticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, <br>up to N synthetic orthogonal KPIs</br> may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "To be rigorous, <br>up to N synthetic orthogonal KPIs</br> may be computed."
            ],
            "translated_annotated_samples": [
                "Output: Para ser rigurosos, <br>hasta N KPIs ortogonales sintéticos</br> pueden ser calculados."
            ],
            "translated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de la dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple del ACP (ACP lineal) es una técnica que encuentra los vectores mutuamente no correlacionados sobre los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el que la varianza de la proyección de las muestras es máxima. En este sentido, los KPI originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza. Output: Para ser rigurosos, <br>hasta N KPIs ortogonales sintéticos</br> pueden ser calculados. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para explicar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}