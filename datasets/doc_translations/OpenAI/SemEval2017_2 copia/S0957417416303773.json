{
    "id": "S0957417416303773",
    "original_text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.",
    "original_translation": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos.",
    "original_sentences": [
        "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
        "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
        "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
        "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
        "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
        "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
        "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
        "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
    ],
    "translated_text_sentences": [
        "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad.",
        "Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002).",
        "En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas.",
        "El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda.",
        "El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima.",
        "En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza.",
        "Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales.",
        "Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos."
    ],
    "error_count": 0,
    "keys": {
        "a set of orthogonal vectors": {
            "translated_key": "un conjunto de vectores ortogonales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is <br>a set of orthogonal vectors</br> sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "The result is <br>a set of orthogonal vectors</br> sorted in descending order of achieved variance."
            ],
            "translated_annotated_samples": [
                "El resultado es <br>un conjunto de vectores ortogonales</br> ordenados en orden descendente de varianza lograda."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es <br>un conjunto de vectores ortogonales</br> ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "data mining": {
            "translated_key": "minería de datos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of <br>data mining</br> many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In the recent years and mainly motivated by the impulse of <br>data mining</br> many methods for dimensionality reduction have arisen."
            ],
            "translated_annotated_samples": [
                "En los últimos años y principalmente motivado por el impulso de la <br>minería de datos</br> han surgido muchos métodos para la reducción de dimensionalidad."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la <br>minería de datos</br> han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "dimensionality reduction": {
            "translated_key": "reducción de dimensionalidad",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for <br>dimensionality reduction</br> have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for <br>dimensionality reduction</br> have arisen."
            ],
            "translated_annotated_samples": [
                "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la <br>reducción de dimensionalidad</br>."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la <br>reducción de dimensionalidad</br>. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "finds the mutually-uncorrelated vectors": {
            "translated_key": "encuentra los vectores mutuamente no correlacionados",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that <br>finds the mutually-uncorrelated vectors</br> onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that <br>finds the mutually-uncorrelated vectors</br> onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que <br>encuentra los vectores mutuamente no correlacionados</br> en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que <br>encuentra los vectores mutuamente no correlacionados</br> en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "linear PCA": {
            "translated_key": "PCA lineal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (<br>linear PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, the simplest version of PCA (<br>linear PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "En un espacio vectorial de N dimensiones, la versión más simple de PCA (<br>PCA lineal</br>) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (<br>PCA lineal</br>) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "N-dimensional vector space basis": {
            "translated_key": "base del espacio vectorial de N dimensiones",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the <br>N-dimensional vector space basis</br>, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In this sense, the original KPIs constitute the <br>N-dimensional vector space basis</br>, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance."
            ],
            "translated_annotated_samples": [
                "En este sentido, los KPIs originales constituyen la <br>base del espacio vectorial de N dimensiones</br>, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la <br>base del espacio vectorial de N dimensiones</br>, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "N^ synthetic KPIs": {
            "translated_key": "KPIs sintéticos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the <br>N^ synthetic KPIs</br> represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the <br>N^ synthetic KPIs</br> represent the orthogonal vectors with the highest variance."
            ],
            "translated_annotated_samples": [
                "En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los <br>KPIs sintéticos</br> de N^ representan los vectores ortogonales con la mayor varianza."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los <br>KPIs sintéticos</br> de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "PCA": {
            "translated_key": "método de Análisis de Componentes Principales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (<br>PCA</br>) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of <br>PCA</br> (linear <br>PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "Within these, it is worth highlighting the Principal Component Analysis method (<br>PCA</br>) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of <br>PCA</br> (linear <br>PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "Dentro de estos, vale la pena destacar el <br>método de Análisis de Componentes Principales</br> (PCA) (Jolliffe, 2002).",
                "En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el <br>método de Análisis de Componentes Principales</br> (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "Principal Component Analysis method": {
            "translated_key": "Análisis de Componentes Principales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the <br>Principal Component Analysis method</br> (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "Within these, it is worth highlighting the <br>Principal Component Analysis method</br> (PCA) (Jolliffe, 2002)."
            ],
            "translated_annotated_samples": [
                "Dentro de estos, vale la pena destacar el método de <br>Análisis de Componentes Principales</br> (PCA) (Jolliffe, 2002)."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de <br>Análisis de Componentes Principales</br> (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the first N^": {
            "translated_key": "los primeros N",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, <br>the first N^</br>, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "However, only a small set of them, <br>the first N^</br>, is enough to account for most of the variance of the data."
            ],
            "translated_annotated_samples": [
                "Sin embargo, solo un pequeño conjunto de ellos, <br>los primeros N</br>^, es suficiente para representar la mayor parte de la varianza de los datos."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, <br>los primeros N</br>^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the projection of the samples generates the highest variances": {
            "translated_key": "las varianzas más altas",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which <br>the projection of the samples generates the highest variances</br>.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which <br>the projection of the samples generates the highest variances</br>."
            ],
            "translated_annotated_samples": [
                "En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera <br>las varianzas más altas</br>."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera <br>las varianzas más altas</br>. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the simplest version of PCA": {
            "translated_key": "PCA lineal",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, <br>the simplest version of PCA</br> (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "In an N-dimensional vector space, <br>the simplest version of PCA</br> (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances."
            ],
            "translated_annotated_samples": [
                "En un espacio vectorial de N dimensiones, la versión más simple de PCA (<br>PCA lineal</br>) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (<br>PCA lineal</br>) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "the variance of the projection of the samples is maximum": {
            "translated_key": "la varianza de la proyección de las muestras",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which <br>the variance of the projection of the samples is maximum</br>.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "The first of these vectors is that onto which <br>the variance of the projection of the samples is maximum</br>."
            ],
            "translated_annotated_samples": [
                "El primero de estos vectores es aquel en el cual <br>la varianza de la proyección de las muestras</br> es máxima."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual <br>la varianza de la proyección de las muestras</br> es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N KPIs sintéticos ortogonales. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        },
        "up to N synthetic orthogonal KPIs": {
            "translated_key": "KPIs sintéticos ortogonales",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, <br>up to N synthetic orthogonal KPIs</br> may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [
                "To be rigorous, <br>up to N synthetic orthogonal KPIs</br> may be computed."
            ],
            "translated_annotated_samples": [
                "Para ser rigurosos, se pueden calcular hasta N <br>KPIs sintéticos ortogonales</br>."
            ],
            "translated_text": "En los últimos años y principalmente motivado por el impulso de la minería de datos han surgido muchos métodos para la reducción de dimensionalidad. Dentro de estos, vale la pena destacar el método de Análisis de Componentes Principales (PCA) (Jolliffe, 2002). En un espacio vectorial de N dimensiones, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente no correlacionados en los cuales la proyección de las muestras genera las varianzas más altas. El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza lograda. El primero de estos vectores es aquel en el cual la varianza de la proyección de las muestras es máxima. En este sentido, los KPIs originales constituyen la base del espacio vectorial de N dimensiones, mientras que los KPIs sintéticos de N^ representan los vectores ortogonales con la mayor varianza. Para ser rigurosos, se pueden calcular hasta N <br>KPIs sintéticos ortogonales</br>. Sin embargo, solo un pequeño conjunto de ellos, los primeros N^, es suficiente para representar la mayor parte de la varianza de los datos. ",
            "candidates": [],
            "error": [
                [
                    ""
                ]
            ]
        }
    }
}