{
    "original_text": "Electromagnetics computations using the MPI parallel implementation of the steepest descent fast multipole method (SDFMM) The computational solution of large-scale linear systems of equations necessitates the use of fast algorithms but is also greatly enhanced by employing parallelization techniques. The objective of this work is to demonstrate the speedup achieved by the MPI (message passing interface) parallel implementation of the steepest descent fast multipole method (SDFMM). Although this algorithm has already been optimized to take advantage of the structure of the physics of scattering problems, there is still the opportunity to speed up the calculation by dividing tasks into components using multiple processors and solve them in parallel. The SDFMM has three bottlenecks ordered as (1) filling the sparse impedance matrix associated with the near-field method of moments interactions (MoM), (2) the matrix vector multiplications associated with this sparse matrix and (3) the far field interactions associated with the fast multipole method. The parallel implementation task is accomplished using a thirty-one node Intel Pentium Beowulf cluster and is also validated on a 4-processor Alpha workstation. The Beowulf cluster consists of thirty-one nodes of 350 MHz Intel Pentium IIs with 256 MB of RAM and one node of a 4*450 MHz Intel Pentium II Xeon shared memory processor with 2 GB of RAM with all nodes connected to a 100 BaseTX Ethernet network. The Alpha workstation has a maximum of four 667 MHz processors. Our numerical results show significant linear speedup in filling the sparse impedance matrix. Using the 32-processors on the Beowulf cluster lead to a 7.2 overall speedup while a 2.5 overall speedup is gained using the 4-processors on the Alpha workstation",
    "original_translation": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha",
    "error_count": 22,
    "keys": {
        "electromagnetics computations": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "MPI parallel implementation": {
            "translated_key": "MPI",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela <br>MPI</br> del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "steepest descent fast multipole method": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más profundo <br> (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más profundo <br> (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "large-scale linear systems": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales a gran escala <br> de ecuaciones requiere el uso de algoritmos rápidos, pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "fast algorithms": {
            "translated_key": "fast",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos <br>fast</br>, pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "message passing interface": {
            "translated_key": "interfaz de paso de mensajes",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad lograda por el MPI (<br>interfaz de paso de mensajes</br>) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM).  Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "physics": {
            "translated_key": "physics",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la <br>physics</br> de los problemas de dispersión, todavía existe la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolviéndolos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "multiple processors": {
            "translated_key": "múltiples procesadores",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando <br>múltiples procesadores</br> y resolverlos en paralelo.  El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "sparse impedance matrix": {
            "translated_key": "sparse",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia <br>sparse</br> asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de impedancia <br>sparse</br>. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "near-field MoM": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "method of moments": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido.  La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "scattering problems": {
            "translated_key": "scattering problems",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de <br>scattering problems</br>, todavía existe la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolviéndolos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "matrix vector multiplications": {
            "translated_key": "matrix",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de escasa impedancia asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales <br>matrix</br> asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "Intel Pentium Beowulf cluster": {
            "translated_key": "Intel Pentium Beowulf cluster",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se realiza utilizando un nodo 31 <br>Intel Pentium Beowulf cluster</br> y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "4-processor Alpha workstation": {
            "translated_key": "4-procesador",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha <br>4-procesador</br>. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "Intel Pentium II": {
            "translated_key": "Intel Pentium II",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz <br>Intel Pentium II</br> Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "RAM": {
            "translated_key": "RAM",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de <br>RAM</br> y un nodo de un procesador de memoria compartida Intel Pentium II Xeon de 4*450 MHz con 2 GB de <br>RAM</br> con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "Xeon shared memory processor": {
            "translated_key": "Xeon procesador de memoria compartida",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II <br>Xeon procesador de memoria compartida</br> con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "100 BaseTX Ethernet network": {
            "translated_key": "100 BaseTX",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet <br>100 BaseTX</br>. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "scattered electric field": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "scattered magnetic field": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "350 MHz": {
            "translated_key": "350 MHz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de <br>350 MHz</br> Intel Pentium IIs con 256 MB de RAM y un nodo de un procesador de memoria compartida Intel Pentium II Xeon de 4*450 MHz con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "256 MByte": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "450 MHz": {
            "translated_key": "450 MHz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*<br>450 MHz</br> Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "2 GByte": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "667 MHz": {
            "translated_key": "667 MHz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores <br>667 MHz</br>. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "application program interfaces": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "electric fields": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "electromagnetic wave scattering": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "electromagnetism": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "impedance matrix": {
            "translated_key": [
                "matriz de impedancia",
                " "
            ],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la <br>matriz de impedancia</br> escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia <br> </br>. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                "matriz de impedancia",
                " "
            ]
        },
        "magnetic fields": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "matrix multiplication": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "message passing": {
            "translated_key": "message passing",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por la interfaz MPI (<br>message passing</br>) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "parallel algorithms": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "parallel architectures": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "physics computing": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "sparse matrices": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "workstations": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        }
    }
}