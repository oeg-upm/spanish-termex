{
    "original_text": "Automatic Extraction of Titles from General Documents using Machine Learning Yunhua Hu1 Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucao}@microsoft.com Qinghua Zheng Computer Science Department Xian Jiaotong University No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond, WA, USA, 98052 dmitriym@microsoft.com ABSTRACT In this paper, we propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in document retrieval by using the extracted titles. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Search Process; H.4.1 [Information Systems Applications]: Office Automation - Word processing; D.2.8 [Software Engineering]: Metrics - complexity measures, performance measures General Terms Algorithms, Experimentation, Performance. 1. INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Ideally, metadata is defined by the authors of documents and is then used by various systems. However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools [26]. Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue. Methods for performing the task have been proposed. However, the focus was mainly on extraction from research papers. For instance, Han et al. [10] proposed a machine learning based method to conduct extraction from research papers. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents. By general documents, we mean documents that may belong to any one of a number of specific genres. General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed. Research papers usually have well-formed styles and noticeable characteristics. In contrast, the styles of general documents can vary greatly. It has not been clarified whether a machine learning based approach can work well for this task. There are many types of metadata: title, author, date of creation, etc. As a case study, we consider title extraction in this paper. General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. As a case study, we consider extraction from Office including Word and PowerPoint. We take a machine learning approach. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models. In the models, we mainly utilize formatting information such as font size as features. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. Experimental results indicate that our approach works well for title extraction from general documents. Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles. Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively. It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%). We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications. The rest of the paper is organized as follows. In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work. In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles. Section 6 gives our experimental results. We make concluding remarks in section 7. 2. RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers. The proposed methods fall into two categories: the rule based approach and the machine learning based approach. Giuffrida et al. [9], for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript. They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes. Liddy et al. [14] and Yilmazel el al. [23] performed metadata extraction from educational materials using rule-based natural language processing technologies. Mao et al. [16] also conducted automatic metadata extraction from research papers using rules on formatting information. The rule-based approach can achieve high performance. However, it also has disadvantages. It is less adaptive and robust when compared with the machine learning approach. Han et al. [10], for instance, conducted metadata extraction with the machine learning approach. They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier. They mainly used linguistic information as features. They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested. Hidden Markov Model [6], Maximum Entropy Model [1, 4], Maximum Entropy Markov Model [17], Support Vector Machines [3], Conditional Random Field [12], and Voted Perceptron [2] are widely used information extraction models. Information extraction has been applied, for instance, to part-ofspeech tagging [20], named entity recognition [25] and table extraction [19]. 2.3 Search Using Title Information Title information is useful for document retrieval. In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers [8]. In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as titles of the pages [5]. Many search engines seem to utilize them for web page retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata [24]. To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3. MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents. By general documents, we mean documents that belong to one of any number of specific genres. The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes. General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed. Figure 1 shows an estimate on distributions of file formats on intranet and internet [15]. Office and PDF are the main file formats on the intranet. Even on the internet, the documents in the formats are still not negligible, given its extremely large size. In this paper, without loss of generality, we take Office documents as an example. Figure 1. Distributions of file formats in internet and intranet. For Office documents, users can define titles as file properties using a feature provided by Office. We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate. That is to say, titles in file properties are usually inconsistent with the true titles in the file bodies that are created by the authors and are visible to readers. We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct. We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details). A number of reasons can be considered. For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file. In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate. We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google. We found that nearly all the titles presented in the search results were from the file properties of the documents. However, only 0.272 of them were correct. Actually, true titles usually exist at the beginnings of the bodies of documents. If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing. This is exactly the problem we address in this paper. More specifically, given a Word document, we are to extract the title from the top region of the first page. Given a PowerPoint document, we are to extract the title from the first slide. A title sometimes consists of a main title and one or two subtitles. We only consider extraction of the main title. As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles. Figure 2. Title extraction from Word document. Figure 3. Title extraction from PowerPoint document. Next, we define a specification for human judgments in title data annotation. The annotated data will be used in training and testing of the title extraction methods. Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification. However, there are many cases in which the identification is not easy. There are some rules defined in the specification that guide identification for such cases. The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like draft, 147 whitepaper, etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.) Figures 2 and 3 show examples of Office documents from which we conduct title extraction. In Figure 2, Differences in Win32 API Implementations among Windows Operating Systems is the title of the Word document. Microsoft Windows on the top of this page is a picture and thus is ignored. In Figure 3, Building Competitive Advantages through an Agile Infrastructure is the title of the PowerPoint document. We have developed a tool for annotation of titles by human annotators. Figure 4 shows a snapshot of the tool. Figure 4. Title annotation tool. 4. TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction. The same pre-processing step occurs before training and extraction. During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted. If a line (lines are separated by return symbols) only has a single format, then the line will become a unit. If a line has several parts and each of them has its own format, then each part will become a unit. Each unit will be treated as an instance in learning. A unit contains not only content information (linguistic information) but also formatting information. The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances). Figure 5 shows the units obtained from the document in Figure 2. Figure 5. Example of units. In learning, the input is sequences of units where each sequence corresponds to a document. We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other. We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM). In extraction, the input is a sequence of units from one document. We employ one type of model to identify whether a unit is title_begin, title_end, or other. We then extract units from the unit labeled with title_begin to the unit labeled with title_end. The result is the extracted title of the document. The unique characteristic of our approach is that we mainly utilize formatting information for title extraction. Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction. This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework. That is why we apply them together to our current problem. Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ). Recall that an instance here represents a unit. A label represents title_begin, title_end, or other. Here, k is the number of units in a document. In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ). Learning Tool Extraction Tool 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6. Metadata extraction model. We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L . Thus, we have )|()|( )|( 11 11 kk kk XYPXYP XXYYP L LL = In this way, we decompose the model into a number of classifiers. We train the classifiers locally using the labeled data. As the classifier, we employ the Perceptron or Maximum Entropy model. We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L . Thus, we have )|()|( )|( 111 11 kkk kk XYYPXYP XXYYP −= L LL Again, we obtain a number of classifiers. However, the classifiers are conditioned on the previous label. When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively. That is to say, the two models are more precise. In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction. For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics. Specifically, we first identify the most likely title_begin. Then we find the most likely title_end within three units after the title_begin. Finally, we extract as a title the units between the title_begin and the title_end. For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence. In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin [13]. This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem. We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron [2]. In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features. We mainly use the former. The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font). If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0. If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0. If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0. If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0. It is necessary to conduct normalization on font sizes. For example, in one document the largest font size might be 12pt, while in another the smallest one might be 18pt. Boldface: This binary feature represents whether or not the current unit is in boldface. Alignment: There are four binary features that respectively represent the location of the current unit: left, center, right, and unknown alignment. The following format features with respect to context play an important role in title extraction. Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines. Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit. Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one. Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words. Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words. The positive words include title:, subject:, subject line: For example, in some documents the lines of titles and authors have the same formats. However, if lines begin with one of the positive words, then it is likely that they are title lines. Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words. The negative words include To, By, created by, updated by, etc. There are more negative words than positive words. The above linguistic features are language dependent. Word Count: A title should not be too long. We heuristically create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one feature for each interval. If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0. Ending Character: This feature represents whether the unit ends with :, -, or other special characters. A title usually does not end with such a character. 5. DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles. Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text. A ranking function in search can use different weights for different fields of 149 the document. Also, titles are typically assigned high weights, indicating that they are important for document retrieval. As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document. By doing this, we attempt to improve the overall precision. In this paper, we employ a modification of BM25 that allows field weighting [21]. As fields, we make use of body, title, extracted title and anchor. First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: ∑= f tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field. Average document length in the corpus becomes the average of all weighted document lengths. ∑= f ff dlwwdl In our experiments we used 75.0,8.11 == bk . Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6. EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments. First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft. We call it MS hereafter. Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively. Figure 7 shows the distributions of the genres of the documents. We see that the documents are indeed general documents as we define them. Figure 7. Distributions of document genres. Third, a data set in Chinese was also downloaded from the internet. It includes 500 Word documents and 500 PowerPoint documents in Chinese. We manually labeled the titles of all the documents, on the basis of our specification. Not all the documents in the two data sets have titles. Table 1 shows the percentages of the documents having titles. We see that DotCom and DotGov have more PowerPoint documents with titles than MS. This might be because PowerPoint documents published on the internet are more formal than those on the intranet. Table 1. The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure. The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2. Table 2. Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2. They are denoted as largest font size and first line respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable. We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles. We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation). This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces). Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Table 3. Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS). As the model, we used Perceptron. We conduct 4-fold cross validation. Thus, all the results reported here are those averaged over 4 trials. Tables 4 and 5 show the results. We see that Perceptron significantly outperforms the baselines. In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles. Table 4. Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5. Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction. For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines. For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines. We conduct significance tests. The results are shown in Table 6. Here, Largest denotes the baseline of using the largest font size, First denotes the baseline of using the first line. The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6. Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction. However, it is also obvious that using only these two features is not enough. There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like Confidential, White paper, etc. For those cases, the largest font size method cannot work well. For similar reasons, the first line method alone cannot work well, either. With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First. We investigate the performance of solely using linguistic features. We found that it does not work well. It seems that the format features play important roles and the linguistic features are supplements.. Figure 8. An example Word document. Figure 9. An example PowerPoint document. We conducted an error analysis on the results of Perceptron. We found that the errors fell into three categories. (1) About one third of the errors were related to hard cases. In these documents, the layouts of the first pages were difficult to understand, even for humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets. Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3). Confusions between main titles and subtitles were another type of error. Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect. This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment. Again, we perform 4-fold cross 151 validation on the first data set (MS). Table 7, 8 shows the results of all the four models. It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst. In general, the Markovian models perform better than or as well as their classifier counterparts. This seems to be because the Markovian models are trained globally, while the classifiers are trained locally. The Perceptron based models perform better than the ME based counterparts. This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction. Table 7. Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8. Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov). Tables 9-12 show the results. Table 9. Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10. Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11. Accuracies of title extraction with Word in DotCom Precisio n Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12. Performance of PowerPoint document title extraction in DotCom Precisio n Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well. There is almost no drop in accuracy. The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese. Tables 13-14 show the results. Table 13. Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14. Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language. There are only small drops in accuracy. Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible. The results indicate that the patterns of title formats exist across different languages. From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval. As a baseline, we employed BM25 without using extracted titles. The ranking mechanism was as described in Section 5. The weights were heuristically set. We did not conduct optimization on the weights. The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranets search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly. Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results. In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10. Search ranking results. Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: • Blue bar - BM25 including the fields body, title (file property), and anchor text. • Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title. With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7. CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents. We have tried using a machine learning approach to address the problem. Previous work showed that the machine learning approach can work well for metadata extraction from research papers. In this paper, we showed that the approach can work for extraction from general documents as well. Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents. Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information. It appeared that using formatting information is a key for successfully conducting title extraction from general documents. We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron. We found that the performance of the Perceptorn models was the best. We applied models constructed in one domain to another domain and applied models trained in one language to another language. We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic. We also attempted to use the extracted titles in document retrieval. We observed a significant improvement in document ranking performance for search when using extracted title information. All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach. 8. ACKNOWLEDGEMENTS We thank Chunyu Wei and Bojuan Zhao for their work on data annotation. We acknowledge Jinzhu Li for his assistance in conducting the experiments. We thank Ming Zhou, John Chen, Jun Xu, and the anonymous reviewers of JCDL05 for their valuable comments on this paper. 9. REFERENCES [1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39-71, 1996. [2] Collins, M. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, 1-8, 2002. [3] Cortes, C. and Vapnik, V. Support-vector networks. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to information extraction from semi-structured and free text. In Proceedings of the Eighteenth National Conference on Artificial Intelligence, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia newsblaster: multilingual news summarization on the Web. In Proceedings of Human Language Technology conference / North American chapter of the Association for Computational Linguistics annual meeting, 1-4, 2004. [6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov models. Machine Learning, 29:245-273, 1997. [7] Gheel, J. and Anderson, T. Data and metadata for finding and reminding, In Proceedings of the 1999 International Conference on Information Visualization, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a niche search engine for e-Business. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 413414, 2003. [9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based metadata extraction from PostScript files. In Proceedings of the Fifth ACM Conference on Digital Libraries, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and Fox, E. A. Automatic document metadata extraction using support vector machines. In Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, 2003. [11] Kobayashi, M., and Takeda, K. Information retrieval on the Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., and Pereira, F. Conditional random fields: probabilistic models for segmenting and 153 labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and Kandola, J. S. The perceptron algorithm with uneven margins. In Proceedings of the Nineteenth International Conference on Machine Learning, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., and Silverstein, J. Automatic Metadata generation & evaluation. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 401-402, 2002. [15] Littlefield, A. Effective enterprise information retrieval across new content formats. In Proceedings of the Seventh Search Engine Conference, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature generation system for automated metadata extraction in preservation of digital materials. In Proceedings of the First International Workshop on Document Image Analysis for Libraries, 225-232, 2004. [17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy markov models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, 591-598, 2000. [18] Murphy, L. D. Digital document metadata in organizations: roles, analytical approaches, and future research directions. In Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B. Table extraction using conditional random fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 235242, 2003. [20] Ratnaparkhi, A. Unsupervised statistical models for prepositional phrase attachment. In Proceedings of the Seventeenth International Conference on Computational Linguistics. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 extension to multiple weighted fields, In Proceedings of ACM Thirteenth Conference on Information and Knowledge Management, 42-49, 2004. [22] Yi, J. and Sundaresan, N. Metadata based Web mining for relevance, In Proceedings of the 2000 International Symposium on Database Engineering & Applications, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: An NLP system to automatically assign metadata. In Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries, 241-242, 2004. [24] Zhang, J. and Dimitroff, A. Internet search engines response to metadata Dublin Core implementation. Journal of Information Science, 30:310-320, 2004. [25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using named entities: focused named entity recognition using machine learning. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154",
    "original_translation": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154",
    "error_count": 15,
    "keys": {
        "title extraction": {
            "translated_key": [
                "extracción del título",
                "extracción automática del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de títulos",
                "extracción de títulos",
                "extracción del título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de títulos",
                "extracción del título",
                "extracción de título"
            ],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la \"extracción del título\" de trabajos de investigación. No ha quedado claro si sería posible realizar automáticamente la \"extracción automática del título\" de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos \"extracción de título\" usando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y memoria para \"extracción de título\" de Word es 0.810 y 0.837 respectivamente, y precisión y memoria para \"extracción de título\" de PowerPoint es 0.875 y 0.895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la \"extracción del título\" en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar \"extracción de título\" usando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la \"extracción de título\"; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la \"extracción del título\" de los documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. Precisión y memoria para \"extracción de título\" de Word son 0.810 y 0.837 respectivamente, y precisión y memoria para \"extracción de título\" de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de las características de formato es la clave para una \"extracción de título\" exitosa. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que de hecho podemos llevar a cabo una \"extracción de título\" fiable de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de \"extracción de título\", y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesaria la investigación sobre la \"extracción de títulos\" de ellos. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la \"extracción de títulos\", utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la formación y la prueba de los métodos de \"extracción del título\". Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Las figuras 2 y 3 muestran ejemplos de documentos de la Oficina desde los cuales realizamos la \"extracción del título\". En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente la información de formateo para \"extracción de título\". Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la \"extracción de título\". Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la \"extracción del título\". Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre \"extracción de título\" en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la \"extracción del título\" Is title Is not title Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de \"extracción de título\" con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases de referencia Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de \"extracción de título\" con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la \"extracción del título\". Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la \"extracción de título\", sugiriendo que el tamaño de fuente Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la \"extracción de título\" con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para \"extracción de título\" con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de \"extracción de título\" con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de \"extracción de título\" con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de \"extracción de título\" con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento del documento de PowerPoint \"extracción de título\" en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de \"extracción de título\" con Word en China Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de \"extracción de título\" con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos en el uso de \"extracción de título\" para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la \"extracción de títulos\" de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formateo es una clave para llevar a cabo con éxito la \"extracción del título\" de los documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y el significado del enfoque de \"extracción de título\". 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                "extracción del título",
                "extracción automática del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de títulos",
                "extracción de títulos",
                "extracción del título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción del título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de título",
                "extracción de títulos",
                "extracción del título",
                "extracción de título"
            ]
        },
        "formatting information": {
            "translated_key": [
                "información de formateo",
                "información de formateo",
                "información de formateo",
                "formatear información",
                "formatear la información",
                "información de formateo",
                "formatear información",
                "formatear información",
                "información de formateo",
                "información de formato"
            ],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente \"información de formateo\" como tamaño de fuente como características en los modelos. Resulta que el uso de \"información de formateo\" puede conducir a una extracción bastante precisa de documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente \"información de formateo\" como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también llevó a cabo la extracción automática de metadatos de documentos de investigación utilizando reglas sobre \"formatear información\". El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también \"formatear la información\". La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente \"información de formateo\" para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente \"formatear información\". 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de \"formatear información\" es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente \"información de formateo\". Al parecer, el uso de \"información de formato\" es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                "información de formateo",
                "información de formateo",
                "información de formateo",
                "formatear información",
                "formatear la información",
                "información de formateo",
                "formatear información",
                "formatear información",
                "información de formateo",
                "información de formato"
            ]
        },
        "language independence": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "metada of document": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "linguistic feature": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "comparison between model": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "generality of model": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "model generality": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "usefulness of extracted title": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "extracted title usefulness": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "genre": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "classifier": {
            "translated_key": [
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificadoras"
            ],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el \"clasificador\". Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como el \"clasificador\". Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como \"clasificador\", empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Entropía Máxima como \"clasificador\", los modelos se convierten en un Modelo Percepton Markov o Modelo Máximo de Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes \"clasificadoras\". Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificador",
                "clasificadoras"
            ]
        },
        "document retrieval": {
            "translated_key": "recuperación de documentos",
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente el ranking de resultados de búsqueda en \"recuperación de documentos\" utilizando los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la \"recuperación de documentos\" (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de \"recuperación de documentos\" usando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la \"recuperación de documentos\". En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de \"recuperación de documentos\" utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la \"recuperación de documentos\". Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos en el uso de extracción de título para \"recuperación de documentos\". Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de \"recuperación de documentos\" con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la \"recuperación de documentos\". 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos utilizar los títulos extraídos en \"recuperación de documentos\". Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                ""
            ]
        },
        "automatic title extraction": {
            "translated_key": "extracción automática de títulos",
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible llevar a cabo la \"extracción automática de títulos\" de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                ""
            ]
        },
        "information extraction": {
            "translated_key": "extracción de información",
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de \"extracción de información\", en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12], y Perceptrón Votado [2] son ampliamente utilizados modelos de \"extracción de información\". Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de máxima entropía para la \"extracción de información\" del texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para \"extracción de información\" y segmentación. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                ""
            ]
        },
        "metada extraction": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "machine learn": {
            "translated_key": [],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente la clasificación de resultados de búsqueda en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos como búsqueda, navegación y filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en la búsqueda de metadatos de documentos [8]. En la búsqueda web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la búsqueda eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en búsqueda puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como la búsqueda, sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de estos registros de consultas de motores de búsqueda intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Se observó una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de búsqueda de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda en Internet a los metadatos Aplicación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": []
        },
        "search": {
            "translated_key": [
                "buscar",
                "buscar",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "buscar",
                "búsqueda",
                "buscar",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda"
            ],
            "translated_annotated_text": "Extracción automática de títulos de documentos generales utilizando el aprendizaje automático Yunhua Hu1 Departamento de Ciencias de la Computación Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 yunhuahu@mail.xjtu.edu.cn Hang Li, Yunbo Cao Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080 {hangli,yucaomicrosoft.com Qinghua Zheng Departamento de Ciencias Informáticas Xian Jiaotong Universidad No 28, Xianning West Road Xian, China, 710049 qhzheng@mail.xjtu.edu.cn Dmitriy Meyerzon Microsoft Corporation One Microsoft Way Redmond Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos, incluyendo presentaciones, capítulos de libros, documentos técnicos, folletos, informes y cartas. Anteriormente, los métodos se habían propuesto principalmente para la extracción del título de los trabajos de investigación. No ha quedado claro si sería posible proceder automáticamente a la extracción de títulos de los documentos generales. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de formación, formamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que utilizamos principalmente información de formateo como tamaño de fuente como características en los modelos. Resulta que el uso de la información de formateo puede conducir a una extracción bastante precisa de los documentos generales. Precisión y recuperación para la extracción de título de Word es 0,810 y 0,837 respectivamente, y precisión y recuperación para la extracción de título de PowerPoint es 0,875 y 0,895 respectivamente en un experimento sobre datos intranet. Otros hallazgos nuevos importantes en este trabajo incluyen que podemos formar modelos en un dominio y aplicarlos a otro dominio, e incluso más sorprendentemente podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Por otra parte, podemos mejorar significativamente \"buscar\" resultados de clasificación en la recuperación de documentos mediante el uso de los títulos extraídos. Categorías y sujetos Descriptores H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda; H.4.1 [Aplicaciones de sistemas de información]: Automatización de oficinas - Procesamiento de textos; D.2.8 [Engineering de software]: Métricas - medidas de complejidad, medidas de rendimiento Términos generales Algoritmos, experimentación, rendimiento. 1. INTRODUCCIÓN Los metadatos de documentos son útiles para muchos tipos de procesamiento de documentos, como \"buscar\", navegar y filtrar. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen metadatos de documentos por sí mismas, incluso cuando tienen herramientas de definición de metadatos convenientes [26]. Así, cómo extraer automáticamente metadatos de los cuerpos de documentos resulta ser una cuestión de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, la atención se centró principalmente en la extracción de artículos de investigación. Por ejemplo, Han et al. [10] propuso un método basado en el aprendizaje automático para realizar la extracción de artículos de investigación. Formalizaron el problema como el de la clasificación y emplearon las máquinas vectoriales de soporte como el clasificador. Utilizaron principalmente características lingüísticas en el modelo.1 En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de los géneros específicos. Los documentos generales están más disponibles en las bibliotecas digitales, las intranets y la Internet, por lo que la investigación sobre su extracción es sumamente necesaria. Los trabajos de investigación suelen tener estilos bien formados y características notables. En cambio, los estilos de los documentos generales pueden variar mucho. No se ha aclarado si un enfoque basado en el aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como un estudio de caso, consideramos la extracción de título en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como un estudio de caso, consideramos la extracción de Office incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizar la extracción de título utilizando cualquier tipo de los modelos entrenados. En los modelos, utilizamos principalmente información de formateo como tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con márgenes desiguales, Modelo Markov Máxima Entropía y Perceptrón Votado. En este trabajo, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, qué modelo es el mejor para la extracción de títulos; (2) Generalidad del modelo: si es posible formar un modelo en un dominio y aplicarlo a otro dominio, y si es posible formar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como \"búsqueda\". Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a las líneas de base: una que siempre utiliza las primeras líneas como títulos y la otra que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la memoria para la extracción del título de Word son 0,810 y 0,837 respectivamente, y la precisión y la memoria para la extracción del título de PowerPoint son 0,875 y 0,895 respectivamente. Resulta que el uso de las características de formato es la clave para la extracción exitosa del título. (1) Hemos observado que los modelos basados en Perceptron funcionan mejor en términos de precisiónes de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados a otro, y pueden ser entrenados en un idioma y aplicados a otro. (3) Hemos encontrado que utilizando los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que, de hecho, podemos realizar una extracción fiable de títulos de documentos generales y utilizar los resultados extraídos para mejorar las aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, introducimos trabajos relacionados, y en la sección 3, explicamos la motivación y el entorno de problemas de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando títulos extraídos. La sección 6 da nuestros resultados experimentales. En la sección 7 formulamos observaciones finales. 2. 2.1 Se han propuesto métodos de extracción de metadatos de documentos para la extracción automática de metadatos de documentos; sin embargo, la atención se centró principalmente en la extracción de documentos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en normas y el enfoque basado en el aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrolló un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como los títulos se encuentran generalmente en las porciones superiores de las primeras páginas y por lo general están en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel el al. [23] realizó la extracción de metadatos a partir de materiales educativos utilizando tecnologías de procesamiento de lenguaje natural basadas en normas. Mao et al. [16] también realizó la extracción automática de metadatos de documentos de investigación utilizando normas sobre el formato de la información. El enfoque basado en normas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, llevó a cabo la extracción de metadatos con el enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar las máquinas vectoriales de soporte como clasificador. Utilizaban principalmente la información lingüística como características. Informaron de una alta precisión de extracción de artículos de investigación en términos de precisión y memoria. 2.2 Información Extracción La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la que, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Modelo de Markov oculto [6], Modelo de Entropía Máxima [1, 4], Modelo de Markov de Entropía Máxima [17], Máquinas Vectoras de Soporte [3], Campo Aleatorio Condicional [12] y Perceptrón Votado [2] son modelos de extracción de información ampliamente utilizados. Se ha aplicado la extracción de información, por ejemplo, al etiquetado de una parte de la voz [20], denominado reconocimiento de entidad [25] y la extracción de cuadros [19]. 2.3 Búsqueda Uso de la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. logró extraer títulos de trabajos de investigación y hacer uso de los títulos extraídos en metadatos \"búsqueda\" de documentos [8]. En la \"búsqueda\" web, los campos de título (es decir, propiedades del archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de \"búsqueda\" parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al., encontraron que las páginas web con metadatos bien definidos se recuperan más fácilmente que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de la Oficina) para \"buscar\" los documentos. 146 3. MOTIVACIÓN Y ESTABLECIMIENTO DE PROBLEMA Consideramos la cuestión de la extracción automática de títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de cualquier número de géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, documentos técnicos, folletos, informes, notas, especificaciones, cartas, anuncios o curriculum vitae. Los documentos generales están más disponibles en bibliotecas digitales, intranets e Internet, por lo que es muy necesario investigar sobre la extracción de títulos de ellas. La figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e Internet [15]. Office y PDF son los formatos de archivo principales en la intranet. Incluso en Internet, los documentos en los formatos todavía no son insignificantes, dado su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de la Oficina. Gráfico 1 Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir los títulos como propiedades de archivo utilizando una característica proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la característica y por lo tanto los títulos en propiedades de archivo son generalmente muy inexactos. Es decir, los títulos en propiedades de archivo son generalmente inconsistentes con los títulos verdaderos en los cuerpos de archivo que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 Word y 6,000 PowerPoint documentos de una intranet e Internet y examinamos cuántos títulos en las propiedades del archivo son correctos. Sorprendentemente, la precisión fue de sólo 0,265 (véase la sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, la propiedad del archivo nuevo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en propiedades de archivos de documentos de Office en \"búsqueda\" y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para \"buscar\" documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devuelta por Google. Encontramos que casi todos los títulos presentados en los resultados de la \"búsqueda\" eran de las propiedades del archivo de los documentos. Sin embargo, sólo 0,272 de ellos eran correctos. En realidad, los títulos verdaderos generalmente existen al principio de los cuerpos de documentos. Si podemos extraer con precisión los títulos de los cuerpos de documentos, entonces podemos aprovechar la información confiable del título en el procesamiento de documentos. Este es exactamente el problema que abordamos en este documento. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, vamos a extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Sólo consideramos la extracción del título principal. Como líneas de base para la extracción de títulos, utilizamos la de usar siempre las primeras líneas como títulos y la de usar siempre las líneas con mayores tamaños de fuentes como títulos. Gráfico 2 Extraer el título del documento de Word. Gráfico 3 Extraer el título del documento de PowerPoint. A continuación, definimos una especificación para juicios humanos en la anotación de datos de título. Los datos anotados se utilizarán en la capacitación y el ensayo de los métodos de extracción del título. Resumen de la especificación: El título de un documento debe identificarse sobre la base del sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación de tales casos. Las reglas incluyen un título generalmente en líneas consecutivas en el mismo formato, un documento no puede tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como draft, 147 whitepaper, etc, si es difícil determinar cuál es el título, seleccione el en el tamaño de fuente más grande, y si todavía es difícil determinar cuál es el título, seleccione el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos.) Los gráficos 2 y 3 muestran ejemplos de documentos de la Oficina de los que se extrae el título. En la Figura 2, Las diferencias en las implementaciones de la API Win32 entre los sistemas operativos de Windows es el título del documento de Word. Microsoft Windows en la parte superior de esta página es una imagen y por lo tanto se ignora. En la Figura 3, Construyendo ventajas competitivas a través de una infraestructura ágil es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotación humana. La Figura 4 muestra una instantánea de la herramienta. Gráfico 4 Herramienta de anotación de título. 4. TÍTULO MÉTODO DE EXTRACCIÓN 4.1 Esquema La extracción de títulos basada en el aprendizaje automático consiste en entrenamiento y extracción. La misma etapa de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el pre-procesamiento, de la región superior de la primera página de un documento de Word o la primera diapositiva de un documento de PowerPoint se extraen una serie de unidades para el procesamiento. Si una línea (líneas separadas por símbolos de retorno) sólo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no sólo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Gráfico 5 Ejemplo de unidades. En el aprendizaje, la entrada es secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end, u otra) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin title_end, u otra. Empleamos cuatro tipos de modelos: Perceptron, Entropía Máxima (ME), Modelo Markov de Perceptrón (PMM) y Modelo Markov de Entropía Máxima (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es title_begin, title_end u otro. A continuación, extraemos unidades de la unidad etiquetada con title_begin a la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que utilizamos principalmente información de formateo para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar los patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el que sólo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos Los cuatro modelos en realidad se pueden considerar en el mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias xxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki,2,1 L= ). Recuerde que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end u otra. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )( 11 kk XXYP LL donde iX e iY denotan variables aleatorias tomando como ejemplo ix y etiqueta iy como valores, respectivamente ( ki,2,1 L= ). Herramienta de aprendizaje Herramienta de extracción 2112122122221 11211111211 nknnknn kk yyxxxx yyxxxx yyxxxx LL LL LL LL → → )(maxarg 11 mkmmkm xxyyP LL )( 11 kk XXYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general con el fin de hacerlo lo suficientemente simple para el entrenamiento. 148 Por ejemplo, podemos asumir que kYY,,1 L son independientes entre sí dado kXX,1 L. Por lo tanto, tenemos )()( )( 11 11 kk kk XYPXYP XXYP L L = De esta manera, descomponemos el modelo en un número de clasificadores. Entrenamos a los clasificadores localmente usando los datos etiquetados. Como clasificador, empleamos el modelo Perceptron o Entropía Máxima. También podemos asumir que el primer pedido propiedad Markov se mantiene para kYY,,1 L dado kXX,,1 L. Por lo tanto, tenemos )()( )( 111 11 kkk kk XYPXYP XXYP = L LL De nuevo, obtenemos un número de clasificadores. Sin embargo, los clasificadores están condicionados a la etiqueta anterior. Cuando empleamos el modelo Percepton o Máxima Entropía como clasificador, los modelos se convierten en un Modelo Percepton Markov o Modelo Máxima Entropía Markov, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptron y ME, asignamos etiquetas localmente y combinamos los resultados globalmente más tarde usando heurística. Específicamente, primero identificamos el título_comienza más probable. Luego encontramos el título_end más probable dentro de tres unidades después del título_begin. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo Viterbi para encontrar la secuencia de etiquetas óptima a nivel mundial. En este artículo, para Perceptron, en realidad empleamos una variante mejorada de la misma, llamada Perceptron con margen desigual [13]. Esta versión de Perceptron puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren mucho, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del modelo Perceptron Markov en el que el modelo Perceptron es el llamado Perceptron Votado [2]. Además, en materia de capacitación, los parámetros del modelo se actualizan a nivel mundial y no a nivel local. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Usamos principalmente el primero. Las características se utilizan tanto para el título-principio y el título-fin clasificadores. 4.3.1 Formato Características Tamaño de fuente: Hay cuatro características binarias que representan el tamaño normalizado de la fuente de la unidad (recuerda que una unidad tiene sólo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario 0. Si el tamaño de la fuente está por encima del tamaño medio de la fuente y no el más grande del documento, entonces la segunda característica será 1, de lo contrario 0. Si el tamaño de la fuente está por debajo del tamaño medio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario 0. Es necesario llevar a cabo la normalización en tamaños de letra. Por ejemplo, en un documento el tamaño de fuente más grande podría ser 12pt, mientras que en otro el más pequeño podría ser 18pt. Boldface: Esta característica binaria representa si la unidad actual está o no en negrita. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. Las siguientes características de formato con respecto al contexto desempeñan un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la unidad siguiente difieren de la de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la unidad siguiente difieren de la de la unidad actual. Mismo párrafo: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la siguiente unidad están o no en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Palabra Positiva: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, tema:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, entonces es probable que sean líneas de título. Palabra negativa: Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que palabras positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Cuenta de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, فارسى) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario 0. Carácter final: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título por lo general no termina con tal carácter. 5. MÉTODO DE RETRIEVACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Típicamente, en la recuperación de información un documento se divide en una serie de campos incluyendo cuerpo, título y texto ancla. Una función de clasificación en \"búsqueda\" puede utilizar diferentes pesos para diferentes campos de 149 el documento. Además, los títulos suelen tener pesos altos, lo que indica que son importantes para la recuperación de documentos. Como se ha explicado anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, y por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite ponderar el campo [21]. Como campos, utilizamos cuerpo, título, título extraído y ancla. En primer lugar, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; cada frecuencia del campo se pondera entonces de acuerdo con el parámetro de peso correspondiente:  f tfft tfwwtf Del mismo modo, calculamos la longitud del documento como una suma ponderada de longitudes de cada campo. La longitud media del documento en el corpus se convierte en la media de todas las longitudes ponderadas del documento. En nuestros experimentos usamos 75,0,8.11 == bk. El peso para el contenido fue 1.0, el título fue 10.0, el ancla fue 10.0, y el título extraído fue 5.0. 6. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos al azar 5.000 documentos Word y 5.000 documentos PowerPoint de una intranet de Microsoft. Lo llamamos MS en adelante. En segundo lugar, descargamos y seleccionamos al azar 500 documentos Word y 500 PowerPoint de los dominios DotGov y DotCom en Internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, en efecto, documentos generales tal como los definimos. Gráfico 7 Distribuciones de géneros de documentos. En tercer lugar, un conjunto de datos en chino también fue descargado de Internet. Incluye 500 documentos Word y 500 documentos PowerPoint en chino. Hemos etiquetado manualmente los títulos de todos los documentos, sobre la base de nuestra especificación. No todos los documentos de los dos conjuntos de datos tienen títulos. El cuadro 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría ser porque los documentos de PowerPoint publicados en Internet son más formales que los de la intranet. Cuadro 1 La porción de documentos con títulos Tipo de dominio MS DotCom DotGov Word 75,7% 77,8% 75,6% PowerPoint 82,1% 93,4% 96,4% En nuestros experimentos, realizamos evaluaciones sobre extracción de títulos en términos de precisión, recuerdo y medición F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / ( A + B ) Recordar: R = A / ( A + C ) F-medida: F1 = 2PR / ( P + R ) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Cuadro 2 Tabla de contingencias con respecto a la extracción del título Es título No es título Extraído A B No extraído C D 6.2 Bases de referencia Probamos las exactitudes de las dos líneas de base descritas en la sección 4.2. Se denotan como tamaño de fuente más grande y primera línea, respectivamente. 6.3 Precisión de los títulos en propiedades de archivo Investigamos cuántos títulos en las propiedades de archivo de los documentos son confiables. Vemos los títulos anotados por humanos como verdaderos títulos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos Edit Distance para llevar a cabo la coincidencia aproximada. (La coincidencia aproximada sólo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados humanos pueden ser ligeramente diferentes de los títulos en propiedades de archivo en la superficie, por ejemplo, contienen espacios adicionales). Dado la cadena A y la cadena B: si ( (D == 0) o ( D / ( La + Lb ) < Exactitudes de los títulos en propiedades de archivos Tipo de archivo Dominio Precision Recordar F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Como modelo, usamos Perceptron. Realizamos la validación cruzada de 4 veces. Por lo tanto, todos los resultados reportados aquí son los promedios de más de 4 ensayos. Los cuadros 4 y 5 muestran los resultados. Vemos que Perceptron supera significativamente los resultados basales. En la evaluación, utilizamos la correspondencia exacta entre los títulos verdaderos anotados por los seres humanos y los títulos extraídos. Cuadro 4 Exactitudes de la extracción del título con Word Precision Record F1 Modelo Perceptron 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Bases basales Primera línea 0.707 0.767 0.736 Tabla 5. Exactitudes de la extracción del título con PowerPoint Precision Record F1 Modelo Perceptron 0.875 0. 895 0,885 Tamaño de fuente más grande 0,844 0,887 0,865 Bases de referencia Primera línea 0,639 0,671 0,655 Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como el recuerdo del enfoque son un 8 por ciento más altos que los de las líneas de base. Para PowerPoint tanto la precisión y la memoria de la aproximación son 2 por ciento más altos que los de las líneas de base. Realizamos pruebas de significación. Los resultados se muestran en la Tabla 6. Aquí, Largest denota la línea de base de usar el tamaño de fuente más grande, Primero denota la línea de base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático respecto a los valores basales son estadísticamente significativas (en el sentido valor p < 0,05) Tabla 6. Signature test results Documentos Tipo Signature test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 Vemos, a partir de los resultados, que las dos líneas de base pueden funcionar bien para la extracción del título, sugiriendo que el tamaño de fuente y la Sin embargo, también es obvio que el uso de sólo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande sólo contienen descripciones generales como Confidencial, Libro Blanco, etc. Para esos casos, el método de tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio de título), Perceptron puede superar a Largest y First. Investigamos el desempeño del uso exclusivo de características lingüísticas. Encontramos que no funciona bien. Parece que las características de formato juegan papeles importantes y las características lingüísticas son suplementos. Gráfico 8 Un ejemplo de documento de Word. Gráfico 9 Un ejemplo de documento de PowerPoint. Se realizó un análisis de error sobre los resultados de Perceptron. Encontramos que los errores cayeron en tres categorías. (1) Alrededor de un tercio de los errores se relacionaron con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores fueron de los documentos que no tienen títulos verdaderos, pero sólo contienen balas. Puesto que realizamos la extracción de las regiones superiores, es difícil deshacerse de estos errores con el enfoque actual. (3). Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que sólo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error hace poco daño al procesamiento de documentos como \"búsqueda\", sin embargo. 6.5 Comparación entre modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Una vez más, realizamos 4 veces la validación transversal 151 en el primer conjunto de datos (MS). La Tabla 7, 8 muestra los resultados de los cuatro modelos. Resulta que Perceptron y PMM realizan lo mejor, seguido por MEMM, y ME realiza lo peor. En general, los modelos de Markovian funcionan mejor que o así como sus contrapartes clasificadoras. Esto parece deberse a que los modelos de Markovian son entrenados globalmente, mientras que los clasificadores son entrenados localmente. Los modelos basados en Perceptron funcionan mejor que las contrapartes basadas en ME. Esto parece ser porque los modelos basados en Perceptron se crean para hacer mejores clasificaciones, mientras que los modelos ME se construyen para una mejor predicción. Cuadro 7 Comparación entre diferentes modelos de aprendizaje para la extracción del título con Word Model Precision Record F1 Perceptron 0,810 0,837 0,823 MEMM 0,797 0,824 0,810 PMM 0,827 0,823 0,825 ME 0,801 0,621 0,699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de título con PowerPoint Modelo Precision Record F1 Perceptron 0.875 0. 895 0. 885 MEMM 0,841 0,861 0,851 PMM 0,873 0,896 0,885 ME 0,753 0,766 0,759 6.6 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). En los cuadros 9 a 12 se muestran los resultados. Cuadro 9 Exactitudes de la extracción del título con Word en DotGov Precision Record F1 Modelo Perceptron 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Exactitudes de la extracción del título con PowerPoint en DotGov Precision Record F1 Modelo Perceptron 0,900 0,906 0,903 Tamaño de fuente más grande 0,871 0,888 0,879Baselines Primera línea 0,554 0,564 0,559 Tabla 11. Exactitudes de la extracción del título con Word en DotCom Precisio n Recordar F1 Modelo Perceptron 0,832 0,880 0,855 Tamaño de fuente más grande 0,676 0,753 0,712Baselines Primera línea 0,577 0,643 0,608 Tabla 12. Rendimiento de la extracción del título del documento de PowerPoint en DotCom Precisio n Recordar F1 Modelo Perceptron 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos se pueden adaptar bien a diferentes dominios. Casi no hay caída en la precisión. Los resultados indican que los patrones de formatos de título existen en diferentes dominios, y es posible construir un modelo independiente de dominio utilizando principalmente información de formateo. 6.7 Adaptación del idioma Aplicamos el modelo formado con los datos en inglés (MS) al conjunto de datos en chino. En los cuadros 13 a 14 se muestran los resultados. Cuadro 13 Exactitudes de la extracción del título con Word in Chinese Precision Record F1 Modelo Perceptron 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Exactitudes de la extracción del título con PowerPoint en China Precision Record F1 Modelo Perceptron 0.766 0.812 0.789 Tamaño de fuente más grande 0.753 0.813 0.782Baselines Primera línea 0.627 0.676 0.650 Vemos que los modelos se pueden adaptar a un idioma diferente. Sólo hay pequeñas gotas de precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no utilizarlas es insignificante. Los resultados indican que los patrones de formatos de título existen en diferentes idiomas. A partir de los resultados de adaptación del dominio y adaptación del lenguaje, concluimos que el uso de la información formateada es la clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con títulos extraídos Realizamos experimentos sobre el uso de la extracción de título para la recuperación de documentos. Como base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación era el descrito en la sección 5. Los pesos se fijaron heurísticamente. No condujimos la optimización en los pesos. La evaluación se llevó a cabo en un corpus de 1,3 M documentos arrastrados de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de esta intranet \"búsqueda\" motor de registros de consultas. 50 consultas fueron del conjunto más popular, mientras que otras 50 fueron elegidas al azar. Se pidió a los usuarios que proporcionaran juicios del grado de relevancia de los documentos de una escala de 1 a 5 (1 significa perjudicial, 2 - malo, 3 - justo, 4 - bueno y 5 - excelente). 152 La Figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión considerando documentos buenos o excelentes como relevantes (izquierda 3 barras con umbral de relevancia 0,5), o considerando sólo documentos excelentes como relevantes (derecha 3 barras con umbral de relevancia 1,0) 0 0,05 0,10 0,15 0,20,25 0,35 0,445 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anclaje, Título, Cuerpo BM25 Resultados del ranking de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo el cuerpo de campos, título (propiedad del archivo) y texto ancla. • Barra púrpura - BM25 incluyendo el cuerpo de los campos, el título (propiedad archivo), el texto de anclaje y el título extraído. Con el campo adicional de título extraído incluido en BM25 la precisión @10 aumentó de 0,132 a 0,145, o en ~10%. Por lo tanto, es seguro decir que el uso del título extraído puede mejorar la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este artículo, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. El trabajo anterior mostró que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de documentos de investigación. En este trabajo, mostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas de base en la extracción de títulos de documentos de Office. El trabajo anterior sobre extracción de metadatos utilizó principalmente características lingüísticas en documentos, mientras que utilizamos principalmente información de formateo. Al parecer, el uso de la información de formato es una clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Hemos probado diferentes modelos de aprendizaje automático incluyendo Perceptron, Entropía Máxima, Modelo Markov Máxima Entropía, y Perceptrón Votado. Encontramos que el rendimiento de los modelos Perceptorn era el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos formados en un idioma a otro idioma. Encontramos que las accuracies no cayeron sustancialmente en diferentes dominios y idiomas, lo que indica que los modelos eran genéricos. También intentamos usar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para \"búsqueda\" cuando se utiliza la información extraída del título. Todas las investigaciones anteriores no fueron realizadas en trabajos previos, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción del título. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu, y a los revisores anónimos de JCDL05 sus valiosos comentarios sobre este documento. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de máxima entropía para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminatorio para modelos de markov ocultos: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en el Procesamiento de Lenguas Naturales, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de apoyo-vectores. Machine Learning, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información a partir de texto semiestructurado y libre. En Actas de la XVIII Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia Newsblaster: resumen multilingüe de noticias en la Web. En Actas de la Conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos de markov ocultos factoriales. Machine Learning, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de 1999 sobre Visualización de la Información, 446-451,1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: un motor de \"búsqueda\" de nicho para el negocio electrónico. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 413414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. extracción de metadatos basada en el conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de la ACM sobre Bibliotecas Digitales, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos mediante máquinas vectoriales de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y 153 datos de secuencias de etiquetado. En Actas de la 18a Conferencia Internacional sobre el Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre el Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial a través de nuevos formatos de contenido. En Actas de la Séptima Conferencia del Motor de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W. y Thoma, G. R. Un sistema dinámico de generación de características para la extracción automatizada de metadatos en la preservación de materiales digitales. En las actas del primer seminario internacional sobre análisis de imágenes de documentos para bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de entropía máxima markov para extracción y segmentación de información. En Actas de la Decimoséptima Conferencia Internacional sobre el Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésima Primera Conferencia Internacional Anual de Hawaii sobre Ciencias de los Sistemas, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas mediante campos aleatorios condicionales. En Actas de la 26a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para el apego de frase preposicional. En Actas de la Decimoséptima Conferencia Internacional sobre Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Simple extensión BM25 a múltiples campos ponderados, En Actas de la 13a Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Metadata basado en minería web para su relevancia, en Actas del Simposio Internacional de 2000 sobre Ingeniería y Aplicaciones de Bases de Datos, 113121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema NLP para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE de 2004 sobre Bibliotecas Digitales, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Motores de \"búsqueda\" de Internet responden a los metadatos Implementación del núcleo de Dublín. Revista de Ciencias de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y. y Zhang, T. Reconocimiento y uso de entidades nombradas: se centró en el reconocimiento de entidades nombradas utilizando el aprendizaje automático. En Actas de la 27a Conferencia Internacional de la ACM SIGIR sobre Investigación y Desarrollo en la Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154 ",
            "error": [
                "buscar",
                "buscar",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "buscar",
                "búsqueda",
                "buscar",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda",
                "búsqueda"
            ]
        }
    }
}