{
    "original_text": "Using k-nearest-neighbor classification in the leaves of a tree We construct a hybrid (composite) classifier by combining two classifiers in common use - classification trees and k-nearest-neighbor (k-NN). In our scheme we divide the feature space up by a classification tree, and then classify test set items using the k-NN rule just among those training items in the same leaf as the test item. This reduces somewhat the computational load associated with k-NN, and it produces a classification rule that performs better than either trees or the usual k-NN in a number of well-known data sets",
    "original_translation": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos",
    "error_count": 7,
    "keys": {
        "k-nearest-neighbor classification": {
            "translated_key": "clasificación k-nearest-neighbor",
            "translated_annotated_text": "Utilizando \"clasificación k-nearest-neighbor\" en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": [
                ""
            ]
        },
        "tree leaves": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        },
        "hybrid composite classifier": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        },
        "classification trees": {
            "translated_key": "árboles de clasificación",
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - \"árboles de clasificación\" y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": [
                ""
            ]
        },
        "feature space division": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        },
        "computational load": {
            "translated_key": "carga computacional",
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la \"carga computacional\" asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": [
                ""
            ]
        },
        "data sets": {
            "translated_key": "conjuntos de datos",
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conocidos \"conjuntos de datos\" ",
            "error": [
                ""
            ]
        },
        "k-NN rule": {
            "translated_key": "regla de k-NN",
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la \"regla de k-NN\" entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": [
                ""
            ]
        },
        "learning (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        },
        "pattern classification": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        },
        "statistical analysis": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        },
        "tree data structures": {
            "translated_key": [],
            "translated_annotated_text": "Utilizando la clasificación k-nearest-neighbor en las hojas de un árbol Construimos un clasificador híbrido (compuesto) combinando dos clasificadores en uso común - árboles de clasificación y k-nearest-neighbor (k-NN). En nuestro esquema dividimos el espacio de características por un árbol de clasificación, y luego clasificamos los ítems del conjunto de pruebas usando la regla k-NN entre aquellos ítems de entrenamiento en la misma hoja que el ítem de prueba. Esto reduce un poco la carga computacional asociada con k-NN, y produce una regla de clasificación que funciona mejor que los árboles o el k-NN habitual en una serie de conjuntos de datos bien conocidos ",
            "error": []
        }
    }
}