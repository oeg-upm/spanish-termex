{
    "original_text": "Computational complexity of probabilistic disambiguation Recent models of natural language processing employ statistical reasoning for dealing with the ambiguity of formal grammars. In this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, e.g., derivations, parse-trees and sentences. The extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (e.g., maximum probability parse-tree given some input sentence). The implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. In this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. We provide proofs that some, frequently occurring problems of ambiguity resolution are NP-complete. These problems are encountered in various applications, e.g., language understanding for textand speech-based applications. Assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems",
    "original_translation": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización.",
    "error_count": 3,
    "keys": {
        "natural language processing": {
            "translated_key": "procesamiento del lenguaje natural",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de \"procesamiento del lenguaje natural\" emplean un razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "statistical reasoning": {
            "translated_key": "razonamiento estadístico",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean \"razonamiento estadístico\" para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "formal grammars": {
            "translated_key": "gramáticas formales",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las \"gramáticas formales\". En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "statistics": {
            "translated_key": "estadísticas",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las \"estadísticas\", relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática dada, por ejemplo, derivaciones, árboles de parse y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "computational complexity": {
            "translated_key": "complejidad computacional",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la \"complejidad computacional\" de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "probabilistic disambiguation": {
            "translated_key": "desambiguación probabilística",
            "translated_annotated_text": "Complejidad computacional de la \"desambiguación probabilística\" Los modelos recientes de procesamiento del lenguaje natural emplean el razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "NP-completeness results": {
            "translated_key": [],
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": []
        },
        "parsing problems": {
            "translated_key": [],
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": []
        },
        "speech processing": {
            "translated_key": [],
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": []
        },
        "state ambiguity resolution": {
            "translated_key": "establecer la resolución de ambigüedad",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible \"establecer la resolución de ambigüedad\" como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "constrained optimization formula": {
            "translated_key": "fórmula de optimización restringida",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una \"fórmula de optimización restringida\", que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "probabilistic models": {
            "translated_key": "modelos probabilísticos",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedad bajo varios tipos de \"modelos probabilísticos\". Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos \"modelos probabilísticos\" existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "language understanding": {
            "translated_key": "comprensión lingüística",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las gramáticas formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de gramáticas con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la \"comprensión lingüística\" para aplicaciones basadas en textos y en voz. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        },
        "grammars": {
            "translated_key": "grammars",
            "translated_annotated_text": "Complejidad computacional de la desambiguación probabilística Los modelos recientes de procesamiento del lenguaje natural emplean razonamiento estadístico para lidiar con la ambigüedad de las \"grammars\" formales. En este enfoque, las estadísticas, relativas a los diversos fenómenos lingüísticos de interés, se recogen a partir de datos lingüísticos reales y se utilizan para estimar las probabilidades de las diversas entidades generadas por una gramática determinada, por ejemplo, derivaciones, árboles de análisis y oraciones. La extensión de \"grammars\" con probabilidades hace posible establecer la resolución de ambigüedad como una fórmula de optimización limitada, que tiene como objetivo maximizar la probabilidad de alguna entidad que la gramática genera dada la entrada (por ejemplo, parse-tree de probabilidad máxima dada alguna frase de entrada). Sin embargo, la implementación de estas fórmulas de optimización en algoritmos eficientes no siempre avanza sin problemas. En este trabajo, abordamos la complejidad computacional de la resolución de ambigüedades bajo varios tipos de modelos probabilísticos. Presentamos pruebas de que algunos de los problemas de resolución de ambigüedad que ocurren con frecuencia están completos en el NP. Estos problemas se encuentran en diversas aplicaciones, por ejemplo, la comprensión del lenguaje para aplicaciones basadas en textos y en el habla. Asumiendo el modelo común de computación, este resultado implica que, para muchos modelos probabilísticos existentes, no es posible idear algoritmos tratables para resolver estos problemas de optimización. ",
            "error": [
                ""
            ]
        }
    }
}