{
    "original_text": "Training multilayer perceptrons via minimization of sum of ridge functions Motivated by the problem of training multilayer perceptrons in neural networks, we consider the problem of minimizing E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), where xi /sub i/ in R/sup S/, 1<or=i<or=n, and each f/sub i/( xi /sub i/.x) is a ridge function. We show that when n is small the problem of minimizing E can be treated as one of minimizing univariate functions, and we use the gradient algorithms for minimizing E when n is moderately large. For a large n, we present the online gradient algorithms and especially show the monotonicity and weak convergence of the algorithms",
    "original_translation": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos",
    "error_count": 5,
    "keys": {
        "multilayer perceptrons": {
            "translated_key": "perceptores multicapa",
            "translated_annotated_text": "Entrenamiento <br>perceptores multicapa</br> mediante minimización de la suma de funciones de cresta Motivados por el problema del entrenamiento <br>perceptores multicapa</br> en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "online gradient algorithms": {
            "translated_key": "online",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente <br>online</br> y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "ridge functions": {
            "translated_key": "ridge",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de <br>ridge</br> Motivados por el problema de entrenar perceptrones multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "minimization": {
            "translated_key": "minimización",
            "translated_annotated_text": "Entrenando perceptrones multicapa a través de <br>minimización</br> de la suma de funciones de cresta Motivados por el problema de entrenar perceptrones multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "neural networks": {
            "translated_key": " ",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales <br> </br>, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "gradient algorithms": {
            "translated_key": [
                "gradient",
                "gradient en línea"
            ],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos <br>gradient</br> para minimizar E cuando n es moderadamente grande. Para un n grande, presentamos los algoritmos <br>gradient en línea</br> y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                "gradient",
                "gradient en línea"
            ]
        },
        "univariate functions": {
            "translated_key": "funciones univariadas",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar <br>funciones univariadas</br>, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "monotonicity": {
            "translated_key": "monotonicidad",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la <br>monotonicidad</br> y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "weak convergence": {
            "translated_key": "convergencia débil",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para un n grande, presentamos los algoritmos de gradiente en línea y especialmente mostrar la monotonicidad y la <br>convergencia débil</br> de los algoritmos.  ",
            "error": [
                ""
            ]
        },
        "convergence": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para un n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia <br> de los algoritmos ",
            "error": []
        },
        "gradient methods": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": []
        },
        "learning (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": []
        },
        "minimisation": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, y cada f/sub i/( xi /sub i/.x) es una función de cresta. Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": []
        }
    }
}