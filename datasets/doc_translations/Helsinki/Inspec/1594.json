{
    "original_text": "Training multilayer perceptrons via minimization of sum of ridge functions Motivated by the problem of training multilayer perceptrons in neural networks, we consider the problem of minimizing E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), where xi /sub i/ in R/sup S/, 1<or=i<or=n, and each f/sub i/( xi /sub i/.x) is a ridge function. We show that when n is small the problem of minimizing E can be treated as one of minimizing univariate functions, and we use the gradient algorithms for minimizing E when n is moderately large. For a large n, we present the online gradient algorithms and especially show the monotonicity and weak convergence of the algorithms",
    "original_translation": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos",
    "error_count": 3,
    "keys": {
        "multilayer perceptrons": {
            "translated_key": "perceptores multicapa",
            "translated_annotated_text": "Entrenamiento de \"perceptores multicapa\" a través de la minimización de la suma de funciones de la cresta Motivados por el problema del entrenamiento de \"perceptores multicapa\" en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1< Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "online gradient algorithms": {
            "translated_key": "algoritmos de gradiente en línea",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los \"algoritmos de gradiente en línea\" y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "ridge functions": {
            "translated_key": "funciones del ridge",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante la minimización de la suma de \"funciones del ridge\" Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "minimization": {
            "translated_key": "minimización",
            "translated_annotated_text": "Entrenando perceptrones multicapa a través de la \"minimización\" de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or= Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "neural networks": {
            "translated_key": "redes neuronales",
            "translated_annotated_text": "Entrenamiento de perceptores multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en \"redes neuronales\", consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "gradient algorithms": {
            "translated_key": "algoritmos gradientes",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los \"algoritmos gradientes\" para minimizar E cuando n es moderadamente grande. Para una gran n, presentamos los \"algoritmos gradientes\" en línea y especialmente mostrar la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "univariate functions": {
            "translated_key": "funciones univariadas",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E se puede tratar como uno de minimizar \"funciones univariadas\", y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "monotonicity": {
            "translated_key": "monotonicidad",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una gran n, presentamos los algoritmos de gradiente en línea y especialmente mostrar la \"monotonicidad\" y la débil convergencia de los algoritmos ",
            "error": [
                ""
            ]
        },
        "weak convergence": {
            "translated_key": "convergencia débil",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una gran n, presentamos los algoritmos de gradiente en línea y especialmente mostrar la monotonicidad y \"convergencia débil\" de los algoritmos ",
            "error": [
                ""
            ]
        },
        "convergence": {
            "translated_key": "convergencia",
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una gran n, presentamos los algoritmos de gradiente en línea y especialmente mostrar la monotonicidad y débil \"convergencia\" de los algoritmos ",
            "error": [
                ""
            ]
        },
        "gradient methods": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": []
        },
        "learning (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": []
        },
        "minimisation": {
            "translated_key": [],
            "translated_annotated_text": "Entrenando perceptrones multicapa mediante minimización de la suma de funciones de cresta Motivados por el problema de entrenar perceptores multicapa en redes neuronales, consideramos el problema de minimizar E(x)= Sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), donde xi /sub i/ en R/sup S/, 1<or=i<or=n, Demostramos que cuando n es pequeño el problema de minimizar E puede ser tratado como uno de minimizar funciones univariadas, y utilizamos los algoritmos de gradiente para minimizar E cuando n es moderadamente grande. Para una n grande, presentamos los algoritmos de gradiente en línea y especialmente mostramos la monotonicidad y la débil convergencia de los algoritmos ",
            "error": []
        }
    }
}