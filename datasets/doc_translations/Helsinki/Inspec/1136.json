{
    "original_text": "Q-learning for risk-sensitive control We propose for risk-sensitive control of finite Markov chains a counterpart of the popular Q-learning algorithm for classical Markov decision processes. The algorithm is shown to converge with probability one to the desired solution. The proof technique is an adaptation of the o.d.e. approach for the analysis of stochastic approximation algorithms, with most of the work involved used for the analysis of the specific o.d.e.s that arise",
    "original_translation": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen",
    "error_count": 9,
    "keys": {
        "finite Markov chains": {
            "translated_key": "cadenas de Markov infinitas",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de \"cadenas de Markov infinitas\" una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "risk-sensitive control": {
            "translated_key": "control sensible al riesgo",
            "translated_annotated_text": "Q-learning para el \"control sensible al riesgo\" Proponemos para el \"control sensible al riesgo\" de las cadenas finitas de Markov una contraparte del popular algoritmo Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "Q-learning algorithm": {
            "translated_key": "algoritmo de aprendizaje Q",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular \"algoritmo de aprendizaje Q\" para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "classical Markov decision processes": {
            "translated_key": "procesos de decisión clásicos de Markov",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para \"procesos de decisión clásicos de Markov\". El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "algorithm convergence": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "reinforcement learning algorithms": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "proof technique": {
            "translated_key": "técnica de la prueba",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La \"técnica de la prueba\" es una adaptación del o.d.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "stochastic approximation algorithms": {
            "translated_key": "algoritmos de aproximación estocástica",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de \"algoritmos de aproximación estocástica\", con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "dynamic programming": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "ordinary differential equations": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "decision theory": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "differential equations": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "learning (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "Markov processes": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        }
    }
}