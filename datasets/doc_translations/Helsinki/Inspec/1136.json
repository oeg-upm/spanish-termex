{
    "original_text": "Q-learning for risk-sensitive control We propose for risk-sensitive control of finite Markov chains a counterpart of the popular Q-learning algorithm for classical Markov decision processes. The algorithm is shown to converge with probability one to the desired solution. The proof technique is an adaptation of the o.d.e. approach for the analysis of stochastic approximation algorithms, with most of the work involved used for the analysis of the specific o.d.e.s that arise",
    "original_translation": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen",
    "error_count": 9,
    "keys": {
        "finite Markov chains": {
            "translated_key": "finite ",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas Markov <br>finite </br> una contraparte del popular algoritmo Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "risk-sensitive control": {
            "translated_key": " risk-sensitive control",
            "translated_annotated_text": "Q-learning for <br> risk-sensitive control</br> Proponemos para <br> risk-sensitive control</br> de cadenas finitas Markov una contraparte del popular algoritmo Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "Q-learning algorithm": {
            "translated_key": "Q-learning",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de cadenas finitas Markov una contraparte del algoritmo popular <br>Q-learning</br> para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "classical Markov decision processes": {
            "translated_key": "clásicos de Markov",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para procesos de decisión <br>clásicos de Markov</br>. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "algorithm convergence": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "reinforcement learning algorithms": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "proof technique": {
            "translated_key": "proof",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica <br>proof</br> es una adaptación del o.d.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": [
                ""
            ]
        },
        "stochastic approximation algorithms": {
            "translated_key": "estocásticos",
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación <br>estocásticos</br>, con la mayor parte del trabajo involucrado utilizado para el análisis de los o.d.s específicos que surgen ",
            "error": [
                ""
            ]
        },
        "dynamic programming": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "ordinary differential equations": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "decision theory": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "differential equations": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "learning (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        },
        "Markov processes": {
            "translated_key": [],
            "translated_annotated_text": "Q-learning para el control sensible al riesgo Proponemos para el control sensible al riesgo de las cadenas finitas de Markov una contraparte del popular algoritmo de Q-learning para los procesos clásicos de decisión de Markov. El algoritmo se muestra para converger con probabilidad uno a la solución deseada. La técnica de prueba es una adaptación del O.D.e. enfoque para el análisis de algoritmos de aproximación estocásticos, con la mayor parte del trabajo involucrado utilizado para el análisis de los O.D.E. específicos que surgen ",
            "error": []
        }
    }
}