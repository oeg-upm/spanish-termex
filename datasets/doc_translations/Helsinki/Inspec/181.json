{
    "original_text": "Electromagnetics computations using the MPI parallel implementation of the steepest descent fast multipole method (SDFMM) The computational solution of large-scale linear systems of equations necessitates the use of fast algorithms but is also greatly enhanced by employing parallelization techniques. The objective of this work is to demonstrate the speedup achieved by the MPI (message passing interface) parallel implementation of the steepest descent fast multipole method (SDFMM). Although this algorithm has already been optimized to take advantage of the structure of the physics of scattering problems, there is still the opportunity to speed up the calculation by dividing tasks into components using multiple processors and solve them in parallel. The SDFMM has three bottlenecks ordered as (1) filling the sparse impedance matrix associated with the near-field method of moments interactions (MoM), (2) the matrix vector multiplications associated with this sparse matrix and (3) the far field interactions associated with the fast multipole method. The parallel implementation task is accomplished using a thirty-one node Intel Pentium Beowulf cluster and is also validated on a 4-processor Alpha workstation. The Beowulf cluster consists of thirty-one nodes of 350 MHz Intel Pentium IIs with 256 MB of RAM and one node of a 4*450 MHz Intel Pentium II Xeon shared memory processor with 2 GB of RAM with all nodes connected to a 100 BaseTX Ethernet network. The Alpha workstation has a maximum of four 667 MHz processors. Our numerical results show significant linear speedup in filling the sparse impedance matrix. Using the 32-processors on the Beowulf cluster lead to a 7.2 overall speedup while a 2.5 overall speedup is gained using the 4-processors on the Alpha workstation",
    "original_translation": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha",
    "error_count": 17,
    "keys": {
        "electromagnetics computations": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "MPI parallel implementation": {
            "translated_key": "implementación paralela de MPI",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la \"implementación paralela de MPI\" del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "steepest descent fast multipole method": {
            "translated_key": "método multipolo rápido de descenso más profundo",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del \"método multipolo rápido de descenso más profundo\" (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad lograda por el MPI (interfaz de paso de mensajes) implementación paralela del \"método multipolo rápido de descenso más profundo\" (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "large-scale linear systems": {
            "translated_key": "sistemas lineales a gran escala",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de \"sistemas lineales a gran escala\" de ecuaciones requiere el uso de algoritmos rápidos, pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "fast algorithms": {
            "translated_key": "algoritmos rápidos",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de \"algoritmos rápidos\", pero también es muy mejorada mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "message passing interface": {
            "translated_key": "interfaz de paso del mensaje",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad lograda por el MPI (\"interfaz de paso del mensaje\") implementación paralela del método multipolo rápido de descenso más pronunciado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "physics": {
            "translated_key": "física",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la \"física\" de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "multiple processors": {
            "translated_key": "procesadores múltiples",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando \"procesadores múltiples\" y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "sparse impedance matrix": {
            "translated_key": "matriz de impedancia esparcida",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la \"matriz de impedancia esparcida\" asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la \"matriz de impedancia esparcida\". El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "near-field MoM": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "method of moments": {
            "translated_key": "método de momentos",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el \"método de momentos\" de campo cercano interacciones (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "scattering problems": {
            "translated_key": "problemas de dispersión",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los \"problemas de dispersión\", todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "matrix vector multiplications": {
            "translated_key": "multiplicaciones vectoriales de matriz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las \"multiplicaciones vectoriales de matriz\" asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "Intel Pentium Beowulf cluster": {
            "translated_key": "Intel Pentium Beowulf cluster",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se realiza utilizando un nodo 31 \"Intel Pentium Beowulf cluster\" y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "4-processor Alpha workstation": {
            "translated_key": "Estación de trabajo Alpha de 4 procesadores",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una \"Estación de trabajo Alpha de 4 procesadores\". El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "Intel Pentium II": {
            "translated_key": "Intel Pentium II",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz \"Intel Pentium II\" Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "RAM": {
            "translated_key": "RAM",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de \"RAM\" y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de \"RAM\" con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "Xeon shared memory processor": {
            "translated_key": "Procesador de memoria compartida Xeon",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II \"Procesador de memoria compartida Xeon\" con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "100 BaseTX Ethernet network": {
            "translated_key": "red Ethernet 100 BaseTX",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de un procesador de memoria compartida Intel Pentium II Xeon de 4*450 MHz con 2 GB de RAM con todos los nodos conectados a una \"red Ethernet 100 BaseTX\". La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "scattered electric field": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "scattered magnetic field": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "350 MHz": {
            "translated_key": "350 MHz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de Intel Pentium IIs \"350 MHz\" con 256 MB de RAM y un nodo de un procesador de memoria compartida Intel Pentium II Xeon de 4*450 MHz con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "256 MByte": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "450 MHz": {
            "translated_key": "450 MHz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*\"450 MHz\" Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "2 GByte": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "667 MHz": {
            "translated_key": "667 MHz",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores \"667 MHz\". Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "application program interfaces": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "electric fields": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "electromagnetic wave scattering": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "electromagnetism": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "impedance matrix": {
            "translated_key": "matriz de impedancia",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la \"matriz de impedancia\" escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la escasa \"matriz de impedancia\". El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "magnetic fields": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "matrix multiplication": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "message passing": {
            "translated_key": "interfaz de paso de mensajes",
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad lograda por el MPI (\"interfaz de paso de mensajes\") implementación paralela del método multipolo rápido de descenso más pronunciado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": [
                ""
            ]
        },
        "parallel algorithms": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "parallel architectures": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "physics computing": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "sparse matrices": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        },
        "workstations": {
            "translated_key": [],
            "translated_annotated_text": "Cálculos electromagnéticos utilizando la implementación paralela MPI del método multipolo rápido de descenso más pronunciado (SDFMM) La solución computacional de sistemas lineales de ecuaciones a gran escala requiere el uso de algoritmos rápidos, pero también se mejora enormemente mediante el empleo de técnicas de paralelización. El objetivo de este trabajo es demostrar la velocidad alcanzada por el MPI (interfaz de paso de mensajes) implementación paralela del método multipolo rápido de descenso más empinado (SDFMM). Aunque este algoritmo ya ha sido optimizado para aprovechar la estructura de la física de los problemas de dispersión, todavía hay la oportunidad de acelerar el cálculo dividiendo tareas en componentes utilizando múltiples procesadores y resolverlos en paralelo. El SDFMM tiene tres cuellos de botella ordenados como (1) llenando la matriz de impedancia escasa asociada con el método de campo cercano de interacciones de momentos (MoM), (2) las multiplicaciones vectoriales de matriz asociadas con esta matriz escasa y (3) las interacciones de campo lejano asociadas con el método multipolo rápido. La tarea de implementación paralela se lleva a cabo utilizando un nodo 31 Intel Pentium Beowulf cluster y también se valida en una estación de trabajo Alpha de 4 procesadores. El clúster Beowulf consta de treinta y un nodos de 350 MHz Intel Pentium IIs con 256 MB de RAM y un nodo de 4*450 MHz Intel Pentium II Xeon procesador de memoria compartida con 2 GB de RAM con todos los nodos conectados a una red Ethernet 100 BaseTX. La estación de trabajo Alpha tiene un máximo de cuatro procesadores de 667 MHz. Nuestros resultados numéricos muestran una significativa aceleración lineal en el llenado de la matriz de escasa impedancia. El uso de los 32 procesadores en el clúster Beowulf conduce a una velocidad total de 7,2 mientras que una velocidad total de 2,5 se obtiene utilizando los 4 procesadores en la estación de trabajo Alpha ",
            "error": []
        }
    }
}