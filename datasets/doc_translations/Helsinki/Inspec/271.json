{
    "original_text": "On the use of neural network ensembles in QSAR and QSPR Despite their growing popularity among neural network practitioners, ensemble methods have not been widely adopted in structure-activity and structure-property correlation. Neural networks are inherently unstable, in that small changes in the training set and/or training parameters can lead to large changes in their generalization performance. Recent research has shown that by capitalizing on the diversity of the individual models, ensemble techniques can minimize uncertainty and produce more stable and accurate predictors. In this work, we present a critical assessment of the most common ensemble technique known as bootstrap aggregation, or bagging, as applied to QSAR and QSPR. Although aggregation does offer definitive advantages, we demonstrate that bagging may not be the best possible choice and that simpler techniques such as retraining with the full sample can often produce superior results. These findings are rationalized using Krogh and Vedelsbys (1995) decomposition of the generalization error into a term that measures the average generalization performance of the individual networks and a term that measures the diversity among them. For networks that are designed to resist over-fitting, the benefits of aggregation are clear but not overwhelming",
    "original_translation": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores",
    "error_count": 9,
    "keys": {
        "neural network ensembles": {
            "translated_key": "conjuntos de red neuronal",
            "translated_annotated_text": "Sobre el uso de \"conjuntos de red neuronal\" en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de la red neuronal, los métodos de conjunto no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "QSAR": {
            "translated_key": "QSAR",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en \"QSAR\" y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a \"QSAR\" y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "QSPR": {
            "translated_key": "QSPR",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y \"QSPR\" A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y \"QSPR\". Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "training set": {
            "translated_key": "conjunto de entrenamiento",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son inherentemente inestables, ya que pequeños cambios en el \"conjunto de entrenamiento\" y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "training parameters": {
            "translated_key": "parámetros de entrenamiento",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son inherentemente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o \"parámetros de entrenamiento\" pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "generalization performance": {
            "translated_key": "rendimiento de generalización",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son inherentemente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su \"rendimiento de generalización\". Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el \"rendimiento de generalización\" promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "uncertainty": {
            "translated_key": "incertidumbre",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la \"incertidumbre\" y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "bootstrap aggregation": {
            "translated_key": "agregación bootstrap",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como \"agregación bootstrap\", o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "bagging": {
            "translated_key": [
                "bagging",
                "baging"
            ],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o \"bagging\", como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que \"baging\" puede no ser la mejor opción posible y que técnicas más simples como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                "bagging",
                "baging"
            ]
        },
        "retraining": {
            "translated_key": "reentrenamiento",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más simples como el \"reentrenamiento\" con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "generalization error decomposition": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "structure-activity correlation": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "structure-property correlation": {
            "translated_key": "correlación estructura-propiedad",
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la estructura-actividad y \"correlación estructura-propiedad\". Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": [
                ""
            ]
        },
        "chemical structure": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "chemistry computing": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "generalisation (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "learning (artificial intelligence)": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "neural nets": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        },
        "uncertainty handling": {
            "translated_key": [],
            "translated_annotated_text": "Sobre el uso de conjuntos de redes neuronales en QSAR y QSPR A pesar de su creciente popularidad entre los profesionales de redes neuronales, los métodos de conjuntos no han sido ampliamente adoptados en la correlación estructura-actividad y estructura-propiedad. Las redes neuronales son intrínsecamente inestables, ya que pequeños cambios en el conjunto de entrenamiento y/o los parámetros de entrenamiento pueden conducir a grandes cambios en su desempeño de generalización. Investigaciones recientes han demostrado que al capitalizar la diversidad de los modelos individuales, las técnicas de conjunto pueden minimizar la incertidumbre y producir predictores más estables y precisos. En este trabajo, presentamos una evaluación crítica de la técnica de conjunto más común conocida como agregación bootstrap, o embolsamiento, como se aplica a QSAR y QSPR. Aunque la agregación ofrece ventajas definitivas, demostramos que el embolsado puede no ser la mejor opción posible y que técnicas más sencillas como el reciclaje con la muestra completa a menudo pueden producir resultados superiores. Estos hallazgos se racionalizan utilizando Krogh y Vedelsbys (1995) la descomposición del error de generalización en un término que mide el desempeño de generalización promedio de las redes individuales y un término que mide la diversidad entre ellas. Para las redes que están diseñadas para resistir el exceso de ajuste, los beneficios de la agregación son claros pero no abrumadores ",
            "error": []
        }
    }
}