{"The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.": "El conjunto final de experimentos incluyó un readiestramiento adaptativo de los parámetros GMM-HMM siguiendo el procedimiento aNAT. Este nuevo modelo sólo proporcionó una mejora del 0,3%, similar al uso del aCMLLR transforma en el modelo GMM-HMM de referencia. Sin embargo, el entrenamiento basado en la demostración aCMLLR transforma en la parte superior del modelo adaptativo aumentó la mejora al 0,8% absoluto. Esto mostró cómo el entrenamiento adaptativo proporcionó una mejor flexibilidad del modelo para adaptarse a las condiciones de fondo específicas existentes en cada espectáculo. Finalmente, el enfoque de factorización usando el altavoz MLLR se transforma en la parte superior del modelo aNAT y las transformaciones aCMLLR basadas en la demostración se probaron."}