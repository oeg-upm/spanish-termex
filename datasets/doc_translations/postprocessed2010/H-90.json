{
    "id": "H-90",
    "original_text": "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1. INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents. From a single query, however, the retrieval system can only have very limited clue about the users information need. An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available. Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2]. There are many kinds of context that we can exploit. Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy. However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents. Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information. Thus the effectiveness of relevance feedback may be limited in real applications. For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12]. In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8]. For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied. In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading). We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results. A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort. For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia. As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island. However, any particular user is unlikely searching for both types of documents. Such an ambiguity can be resolved by exploiting history information. For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for. Implicit feedback was studied in several previous works. In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people. In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated. In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user. Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6]. While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval. Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy. We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information. We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model. One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation. We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models. To the best of our knowledge, this is the first test set for implicit feedback. We evaluate the proposed models using this data set. The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user. The remaining sections are organized as follows. In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later. In Section 3, we propose several implicit feedback models based on statistical language models. In Section 4, we describe how we create the data set for implicit feedback experiments. In Section 5, we evaluate different implicit feedback models on the created data set. Section 6 is our conclusions and future work. 2. PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback. One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session. A session can be considered as a period consisting of all interactions for the same information need. The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context. Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search. In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session. The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time. Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session. In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context. In a single search session, a user may interact with the search system several times. During interactions, the user would continuously modify the query. Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session. Note that we assume that the session boundaries are known in this paper. In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16]. Traditionally, the retrieval system only uses the current query Qk to do retrieval. But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section. Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy. In addition to the query history, there may be other short-term context information available. For example, a user would presumably frequently click some documents to view. We refer to data associated with these actions as clickthrough history. The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document. Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need. Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval. In general, we may have a history of clicked summaries C1, ..., Ck−1. We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk. Previous work has also shown positive results using similar clickthrough information [11, 17]. Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them. In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3. LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk. An important research question is how we can exploit such information effectively. We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method. According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document. One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model. Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk. Let HC = (C1, ..., Ck−1) be the clickthrough history. Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally. Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC . We now describe several different language models for exploiting HQ and HC to estimate p(w|θk). We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text. We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ). Then we linearly interpolate these two history models to obtain the history model p(w|H). Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk). These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information. If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries. But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history. To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator. The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ). We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length. Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α. Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries. This means that all previous queries are treated equally and so are all clicked summaries. However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better. Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable. Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries. Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way. In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents. In order to rank documents, the system must have some model for the users information need. In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query. A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general. Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary). To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior. We use Dirichlet prior because it is a conjugate prior for multinomial distributions. With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation. In general, we assume that the retrieval system maintains a current query model φi at any moment. As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model. Initially, before we see any user query, we may already have some information about the user. For example, we may have some information about what documents the user has viewed in the past. We use such information to define a prior on the query model, which is denoted by φ0. After we observe the first query Q1, we can update the query model based on the new observed data Q1. The updated query model φ1 can then be used for ranking documents in response to Q1. As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1. As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2. In general, we may repeat such an updating process to iteratively update the query model. Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci. In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data. Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary. If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci). On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model. Thus the model remains the same as if we do not observe any new text evidence. In general, the parameters µi and νi may have different values for different i. For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi. But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj. Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents. This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen. To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight. This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document. One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries. The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries. As in OnlineUp, we set all µis and νis to the same value. And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4. DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic. Since there is no such data set available to us, we have to create one. There are two choices. One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine). But the problem is that we have no relevance judgments on such data. The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file. Unfortunately, there are no query history and clickthrough history data. We decide to augment a TREC data set by collecting query history and clickthrough history data. We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments. There are altogether 242918 news articles and the average document length is 416 words. Most articles have titles. If not, we select the first sentence of the text as the title. For the preprocessing, we only do case folding and do not do stopword removal or stemming. We select 30 relatively difficult topics from TREC topics 1-150. These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20]. The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user. In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well. We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles. We use 3 subjects to do experiments to collect query history and clickthrough history data. Each subject is assigned 10 topics and given the topic descriptions provided by TREC. For each topic, the first query is the title of the topic given in the original TREC topic description. After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject. The subject will browse the results and maybe click one or more results to browse the full text of article(s). The subject may also modify the query to do another search. For each topic, the subject composes at least 4 queries. In our experiment, only the first 4 queries for each topic are used. The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study. We use a relational database to store user interactions, including the submitted queries and clicked documents. For each query, we store the query terms and the associated result pages. And for each clicked document, we store the summary as shown on the search result page. The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing). Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words. Altogether there are 91 documents clicked to view. So on average, there are around 3 clicks per topic. The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words. Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file. This data set is publicly available 1 . 5. EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy. In particular, the search context can provide extra information to help us estimate a better query model than using just the current query. So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context. Since we collected four versions of queries for each topic, we make such comparisons for each version of queries. We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents. In all cases, the reported figure is the average over all of the 30 topics. We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp). Each model has precisely two parameters (α and β for FixInt; µ and ν for others). Note that µ and ν may need to be interpreted differently for different methods. We vary these parameters and identify the optimal performance for each method. We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1. A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context. We can make several observations from this table: 1. Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best. Users generally formulate better and better queries. 2. Using search context generally has positive effect, especially when the context is rich. This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2. Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse. When the search context is rich, the performance improvement can be quite substantial. For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3. Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp. Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense. The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries. Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying. While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient. Overall, BatchUp appears to be the best method when we vary the parameter settings. We have two different kinds of search context - query history and clickthrough data. We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately. This allows us to evaluate the effect of using query history alone. We use the same parameter setting for query history as in Table 1. The results are shown in Table 2. Here we see that in general, the benefit of using query history is very limited with mixed results. This is different from what is reported in a previous study [15], where using query history is consistently helpful. Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4. This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations. Yet another observation is that when using query history only, the BayesInt model appears to be better than other models. Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm. The displayed results thus reflect the variation caused by parameter µ. A smaller setting of 2.0 is seen better than a larger value of 5.0. A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ. The value of µ can be interpreted as how many words we regard the query history is worth. A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich. Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2. As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information. This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have. The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query. The results are shown in Table 4. We see that the benefit of using clickthrough information is much more significant than that of using query history. We see an overall positive effect, often with significant improvement over the baseline. It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve. Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial. These results show that the clicked summary text is in general quite useful for inferring a users information need. Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant. Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement. However, such improvement is really not beneficial for the user as the user has already seen these relevant documents. To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file. The results are shown in Table 5. Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results. From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly. Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant. To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4. The results are shown in Table 6. We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries. These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs. In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5. FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking. Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information. In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model. In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others. BatchUp has two parameters µ and ν. We first look at µ. When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query. If we increase µ, we will gradually incorporate more information from the previous queries. In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved. We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2. The pattern is also similar when we set ν to other values. In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4. The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query. Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone. We now turn to the other parameter ν. When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query. With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9. We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15. This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable. Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6. CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance. Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp. We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models. Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort. The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information. It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history. For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query. Second, the proposed models can be implemented in any practical systems. We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms. We will also do a user study to evaluate effectiveness of these models in the real web search. Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7. ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472. We thank the anonymous reviewers for their useful comments. 8. REFERENCES [1] E. Adar and D. Karger. Haystack: Per-user information environments. In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al. Challenges in information retrieval and language modeling. Workshop at University of Amherst, 2002. [3] K. Bharat. Searchpad: Explicit capture of search context to support web search. In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko. Relevance feedback and personalization: A language modeling perspective. In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic query expansion using query logs. In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. Implicit queries (IQ) for contextualized search (demo description). In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search in context: The concept revisited. In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang. Query session based term suggestion for interactive web search. In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic web log session identification with statistical language models. Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom. Scaling personalized web search. In Proceeding of WWW 2003, 2003. [11] T. Joachims. Optimizing search engines using clickthrough data. In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin. Display time as implicit feedback: Understanding task effects. In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan. Implicit feedback for inferring user preference. SIGIR Forum, 32(2), 2003. [14] J. Rocchio. Relevance feedback information retrieval. In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen and C. Zhai. Exploiting query history for document ranking in interactive information retrieval (poster). In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai. A session-based search engine (poster). In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web search based on user profile constructed without any effort from users. In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven. A simulated study of implicit feedback models. In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty. Model-based feedback in the KL-divergence retrieval model. In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad-hoc information retrieval. In Proceedings of SIGIR 2001, 2001.",
    "original_translation": "Recuperación de información sensible al contexto Uso de retroalimentación implícita Xuehua Departamento de Informática Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Chengxiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana Champaign Resumen A MajorLa limitación de la mayoría de los modelos y sistemas de recuperación existentes es que la decisión de recuperación se toma basada únicamente en la consulta y la recopilación de documentos;La información sobre el usuario real y el contexto de búsqueda se ignora en gran medida. En este documento, estudiamos cómo explotar la información de retroalimentación implícita, incluidas consultas anteriores e información de clics, para mejorar la precisión de la recuperación en un entorno de recuperación de información interactiva. Proponemos varios algoritmos de recuperación sensibles al contexto basados en modelos de lenguaje estadístico para combinar las consultas anteriores y hacer clic en resúmenes de documentos con la consulta actual para una mejor clasificación de documentos. Utilizamos los datos de TREC AP para crear una recopilación de pruebas con información de contexto de búsqueda y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de la retroalimentación implícita, especialmente los resúmenes de documentos haciendo clic, puede mejorar sustancialmente el rendimiento de la recuperación. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Términos generales Algoritmos 1. Introducción En la mayoría de los modelos de recuperación de información existentes, el problema de recuperación se trata como que involucra una sola consulta y un conjunto de documentos. Sin embargo, de una sola consulta, el sistema de recuperación solo puede tener una pista muy limitada sobre la necesidad de la información de los usuarios. Por lo tanto, un sistema de recuperación óptimo debe tratar de explotar la mayor cantidad de información de contexto adicional posible para mejorar la precisión de la recuperación, siempre que esté disponible. De hecho, la recuperación sensible al contexto se ha identificado como un desafío importante en la investigación de recuperación de información [2]. Hay muchos tipos de contexto que podemos explotar. La retroalimentación de relevancia [14] puede considerarse como una forma para que un usuario proporcione más contexto de búsqueda y se sabe que es efectivo para mejorar la precisión de la recuperación. Sin embargo, la retroalimentación de relevancia requiere que un usuario proporcione explícitamente información de retroalimentación, como especificar la categoría de la necesidad de información o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a involucrar actividades adicionales, mientras que los beneficios no siempre son obvios para el usuario, un usuario a menudo es reacio a proporcionar dicha información de retroalimentación. Por lo tanto, la efectividad de la retroalimentación de relevancia puede ser limitada en aplicaciones reales. Por esta razón, la retroalimentación implícita ha atraído mucha atención recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperación utilizando la consulta inicial de los usuarios pueden no ser satisfactorios;A menudo, el usuario necesitaría revisar la consulta para mejorar la precisión de recuperación/clasificación [8]. Para una necesidad de información compleja o difícil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de información esté completamente satisfecha. En un escenario de recuperación tan interactivo, la información naturalmente disponible para el sistema de recuperación es más que solo la consulta de usuario actual y la recopilación de documentos; en general, todo el historial de interacción puede estar disponible para el sistema de recuperación, incluidas las consultas anteriores, información sobre la cualDocumentos El usuario ha elegido ver, e incluso cómo un usuario ha leído un documento (por ejemplo, que parte de un documento que el usuario pasa mucho tiempo en lectura). Definimos la retroalimentación implícita en términos generales como explotando todo el historial de interacción naturalmente disponible para mejorar los resultados de la recuperación. Una ventaja importante de la retroalimentación implícita es que podemos mejorar la precisión de la recuperación sin requerir ningún esfuerzo del usuario. Por ejemplo, si la consulta actual es Java, sin conocer ninguna información adicional, sería imposible saber si está destinado a significar el lenguaje de programación de Java o la isla Java en Indonesia. Como resultado, los documentos recuperados probablemente tendrán ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programación y otros pueden estar sobre la isla. Sin embargo, cualquier usuario en particular es poco probable que busque ambos tipos de documentos. Tal ambigüedad se puede resolver explotando la información del historial. Por ejemplo, si sabemos que la consulta anterior del usuario es la programación CGI, sugeriría fuertemente que es el lenguaje de programación que el usuario está buscando. La retroalimentación implícita se estudió en varios trabajos anteriores. En [11], Joachims exploró cómo capturar y explotar la información de clics y demostró que dicha información de retroalimentación implícita puede mejorar la precisión de búsqueda para un grupo de personas. En [18], se realizó un estudio de simulación de la efectividad de diferentes algoritmos de retroalimentación implícitos, y se propusieron y evaluaron varios modelos de recuperación diseñados para explotar la información de clics. En [17], algunos algoritmos de recuperación existentes se adaptan para mejorar los resultados de búsqueda en función del historial de navegación de un usuario. Otro trabajo relacionado sobre el uso del contexto incluye búsqueda personalizada [1, 3, 4, 7, 10], análisis de registro de consultas [5], factores de contexto [12] y consultas implícitas [6]. Si bien el trabajo anterior se ha centrado principalmente en usar información de clics, en este documento, utilizamos información de clics y consultas anteriores, y nos centramos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperación. Específicamente, desarrollamos modelos para usar información de retroalimentación implícita, como consulta e historial de clics de la sesión de búsqueda actual para mejorar la precisión de la recuperación. Utilizamos el modelo de recuperación de divergencia KL [19] como base y proponemos tratar la recuperación sensible al contexto como estimación de un modelo de lenguaje de consulta basado en la consulta actual y cualquier información de contexto de búsqueda. Proponemos varios modelos de lenguaje estadístico para incorporar la consulta y el historial de clics en el modelo de divergencia KL. Un desafío en el estudio de modelos de retroalimentación implícitos es que no existe una recopilación de pruebas adecuada para la evaluación. Por lo tanto, utilizamos los datos de TREC AP para crear una recopilación de pruebas con información de retroalimentación implícita, que puede usarse para evaluar cuantitativamente modelos de retroalimentación implícitos. Hasta donde sabemos, esta es la primera prueba establecida para la retroalimentación implícita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de información de retroalimentación implícita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de la recuperación sin requerir un esfuerzo adicional del usuario. Las secciones restantes se organizan de la siguiente manera. En la Sección 2, intentamos definir el problema de la retroalimentación implícita e introducir algunos términos que usaremos más adelante. En la Sección 3, proponemos varios modelos de retroalimentación implícitos basados en modelos de lenguaje estadístico. En la Sección 4, describimos cómo creamos el conjunto de datos para experimentos de retroalimentación implícitos. En la Sección 5, evaluamos diferentes modelos de retroalimentación implícitos en el conjunto de datos creado. La Sección 6 es nuestras conclusiones y trabajo futuro.2. Definición del problema Hay dos tipos de información de contexto que podemos usar para la retroalimentación implícita. Uno es el contexto a corto plazo, que es la información circundante inmediata que arroja luz sobre la necesidad actual de la información actual de los usuarios en una sola sesión. Una sesión puede considerarse como un período que consiste en todas las interacciones para la misma necesidad de información. La categoría de información de los usuarios necesita (por ejemplo, niños o deportes), consultas anteriores y documentos recientemente vistos son ejemplos de contexto a corto plazo. Dicha información se relaciona más directamente con la necesidad de información actual del usuario y, por lo tanto, se puede esperar que sea más útil para mejorar la búsqueda actual. En general, el contexto a corto plazo es más útil para mejorar la búsqueda en la sesión actual, pero puede no ser tan útil para las actividades de búsqueda en una sesión diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a información como un nivel educativo de usuarios e interés general, el historial de consultas de usuarios acumulados e información de clics de los usuarios anteriores;Dicha información es generalmente estable durante mucho tiempo y a menudo se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisión de búsqueda para una sesión en particular. En este artículo, nos centramos en el contexto a corto plazo, aunque algunos de nuestros métodos también se pueden usar para incorporar naturalmente algún contexto a largo plazo. En una sola sesión de búsqueda, un usuario puede interactuar con el sistema de búsqueda varias veces. Durante las interacciones, el usuario modificaría continuamente la consulta. Por lo tanto, para la consulta actual QK (a excepción de la primera consulta de una sesión de búsqueda), hay un historial de consulta, HQ = (Q1, ..., QK - 1) asociado con ella, que consiste en las consultas anteriores dadas por elmismo usuario en la sesión actual. Tenga en cuenta que suponemos que los límites de la sesión se conocen en este documento. En la práctica, necesitamos técnicas para descubrir automáticamente los límites de las sesiones, que se han estudiado en [9, 16]. Tradicionalmente, el sistema de recuperación solo utiliza la consulta actual QK para hacer la recuperación. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas útiles sobre la necesidad actual de la información actual de los usuarios como se ve en el ejemplo de Java dado en la sección anterior. De hecho, nuestro trabajo anterior [15] ha demostrado que el historial de consultas a corto plazo es útil para mejorar la precisión de la recuperación. Además del historial de consultas, puede haber otra información de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente frecuentemente hace clic en algunos documentos para ver. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el título, el resumen y quizás también el contenido y la ubicación (por ejemplo, la URL) del documento hecho hecho. Aunque no está claro si un documento visto es realmente relevante para la necesidad de la información del usuario, podemos asumir de manera segura que la información de resumen/título mostrada sobre el documento es atractiva para el usuario, por lo que transmite información sobre la necesidad de la información de los usuarios. Supongamos que concatenamos toda la información de texto que se muestra sobre un documento (generalmente título y resumen) juntos, también tendremos un resumen CI en cada ronda de recuperación. En general, podemos tener un historial de resúmenes c1, ..., CK - 1. También explotaremos dicho historial de clics HC = (C1, ..., CK - 1) para mejorar nuestra precisión de búsqueda para la consulta actual QK. El trabajo anterior también ha mostrado resultados positivos utilizando información de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son información de retroalimentación implícita, que naturalmente existe en la recuperación de información interactiva, por lo tanto, no se necesita ningún esfuerzo adicional para el usuario para recopilarlos. En este documento, estudiamos cómo explotar dicha información (HQ y HC), desarrollamos modelos para incorporar el historial de consultas y el historial de clics en una función de clasificación de recuperación y evaluar cuantitativamente estos modelos.3. Los modelos de lenguaje para contextenciales de información sensible a la prueba intuitiva de la consulta HQ y el historial de clics HC son útiles para mejorar la precisión de búsqueda para la consulta actual QK. Una pregunta de investigación importante es cómo podemos explotar dicha información de manera efectiva. Proponemos utilizar modelos de lenguaje estadístico para modelar una necesidad de información de los usuarios y desarrollar cuatro modelos de lenguaje sensibles al contexto específicos para incorporar información de contexto en un modelo de recuperación básica.3.1 Modelo de recuperación básica Usamos el método de divergencia Kullback-Leibbler (KL) [19] como nuestro método de recuperación básica. Según este modelo, la tarea de recuperación implica calcular un modelo de lenguaje de consulta θq para una consulta dada y un modelo de lenguaje de documento θd para un documento y luego calcular su divergencia KL d (θq || θd), que sirve como la puntuación del documento. Una ventaja de este enfoque es que podemos incorporar naturalmente el contexto de búsqueda como evidencia adicional para mejorar nuestra estimación del modelo de lenguaje de consulta. Formalmente, deje que HQ = (Q1, ..., Qk - 1) sea el historial de consulta y la consulta actual sea QK. Deje que HC = (C1, ..., CK - 1) sea el historial de clics. Tenga en cuenta que CI es la concatenación de todos los resúmenes de documentos en la ronda de recuperación, ya que podemos tratar razonablemente todos estos resúmenes por igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos mediante P (W | θk), basado en la consulta actual QK, así como en el Histor de la consulta HQ e History History HC. Ahora describimos varios modelos de idiomas diferentes para explotar HQ y HC para estimar P (W | θk). Usaremos C (w, x) para denotar el recuento de la palabra w en el texto x, que podría ser una consulta o un resumen de documentos haciendo clic o cualquier otro texto. Usaremos | x |para denotar la longitud del texto x o el número total de palabras en X. 3.2 Interpolación de coeficiente fijo (fijación) Nuestra primera idea es resumir el HQ del historial de consultas con un modelo de lenguaje unigram P (W | HQ) y la historia de clic HC conOtro modelo de lenguaje unigram P (W | HC). Luego interpolamos linealmente estos dos modelos de historia para obtener el modelo de historia P (W | H). Finalmente, interpolamos el modelo de historia P (W | H) con el modelo de consulta actual P (W | QK). Estos modelos se definen de la siguiente manera.p (w | qi) = c (w, qi) | qi |p (w | hq) = 1 k - 1 i = k - 1 i = 1 p (w | qi) p (w | ci) = c (w, ci) | ci |p (w | hc) = 1 k - 1 i = k - 1 i = 1 p (w | ci) p (w | h) = βp (w | hc) + (1 - β) p (w | hq)p (w | θk) = αp (w | qk) + (1 - α) p (w | h) donde β ∈ [0, 1] es un parámetro para controlar el peso en cada modelo de historia, y donde α ∈ [0, 1] es un parámetro para controlar el peso en la consulta actual y la información del historial. Si combinamos estas ecuaciones, vemos que p (w | θk) = αp (w | qk) + (1 - α) [βp (w | hc) + (1 - β) p (w | hq)] es decir, el modelo de consulta de contexto estimado es solo una interpolación de coeficiente fijo de tres modelos P (W | QK), P (W | HQ) y P (W | HC).3.3 Interpolación bayesiana (BayesInt) Un posible problema con el enfoque de fijación es que los coeficientes, especialmente α, se fijan en todas las consultas. Pero intuitivamente, si nuestra consulta actual QK es muy larga, deberíamos confiar más en la consulta actual, mientras que si QK solo tiene una palabra, puede ser beneficioso poner más peso en la historia. Para capturar esta intuición, tratamos P (W | HQ) y P (W | HC) como Dirichlet Priors y QK como datos observados para estimar un modelo de consulta de contexto utilizando estimador bayesiano. El modelo estimado viene dado por p (w | θk) = c (w, qk) + µp (w | hq) + νp (w | hc) | qk |+ µ + ν = | qk || QK |+ µ + ν p (w | qk) + µ + ν | qk |+ µ + ν [µ µ + ν p (w | hq) + ν µ + ν p (w | hc)] donde µ es el tamaño de muestra anterior para p (w | hq) y ν es el tamaño de muestra anterior para p(W | HC). Vemos que la única diferencia entre Bayesint y Fixint es que los coeficientes de interpolación ahora se adaptan a la longitud de la consulta. De hecho, al ver Bayesint como fijación, vemos que α = | qk || QK |+µ+ν, β = ν ν+µ, por lo tanto con µ y ν fijo, tendremos un α dependiente de la consulta. Más tarde mostraremos que un α tan adaptativo funciona empíricamente mejor que un α fijo.3.4 Actualización bayesiana en línea (en línea) Tanto FixInt como BayesInt resuman la información del historial promediando los modelos de idiomas unigram estimados en función de consultas anteriores o resúmenes. Esto significa que todas las consultas anteriores se tratan por igual y, por lo tanto, se hacen clic en los resúmenes. Sin embargo, a medida que el usuario interactúa con el sistema y adquiere más conocimiento sobre la información en la recopilación, presumiblemente, las consultas reformuladas mejorarán cada vez más. Por lo tanto, asignar pesos en descomposición a las consultas anteriores para confiar en una consulta reciente más que una consulta anterior parece ser razonable. Curiosamente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de la información de los usuarios después de ver cada consulta, naturalmente podríamos obtener pesos en descomposición en las consultas anteriores. Dado que una estrategia de actualización en línea incremental puede usarse para explotar cualquier evidencia en un sistema de recuperación interactivo, la presentamos de una manera más general. En un sistema de recuperación típico, el sistema de recuperación responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar los documentos, el sistema debe tener algún modelo para la necesidad de información de los usuarios. En el modelo de recuperación de divergencia KL, esto significa que el sistema debe calcular un modelo de consulta siempre que un usuario ingrese una consulta (nueva). Una forma principalmente de actualizar el modelo de consulta es utilizar la estimación bayesiana, que discutimos a continuación.3.4.1 Actualización bayesiana Primero discutimos cómo aplicamos la estimación bayesiana para actualizar un modelo de consulta en general. Sea P (W | φ) nuestro modelo de consulta actual y t sea una nueva evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen en el que se hace clic). Para actualizar el modelo de consulta basado en t, usamos φ para definir un Dirichlet previamente parametrizado como Dir (µt p (w1 | φ), ..., µt p (wn | φ)) donde µt es el tamaño de muestra equivalente delprevio. Usamos Dirichlet Prior porque es un conjugado anterior para distribuciones multinomiales. Con tal conjugado anterior, la distribución predictiva de φ (o de manera equivalente, la media de la distribución posterior de φ viene dada por p (w | φ) = c (w, t) + µt p (w | φ) | t |+ µt (1) donde c (w, t) es el recuento de w en t y | t | es la longitud de T. parámetro µt indica nuestra confianza en el anterior expresado en términos de una muestra de texto equivalente comparable con T. para T.Ejemplo, µt = 1 indica que la influencia del anterior es equivalente a agregar una palabra adicional a T. 3.4.2 Actualización del modelo de consulta secuencial Ahora discutimos cómo podemos actualizar nuestro modelo de consulta a lo largo del tiempo durante un proceso de recuperación interactivo utilizando la estimación bayesiana. En general, suponemos que el sistema de recuperación mantiene un modelo de consulta actual φi en cualquier momento. Tan pronto como obtengamos alguna evidencia de retroalimentación implícita en forma de una pieza de texto TI, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos información sobre el usuario. Por ejemplo, podemos tener información sobre qué documentos ha visto el usuario en el pasado. Utilizamos dicha información para definir un anterior en el modelo de consulta, que se denota por φ0. Después de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados Q1. El modelo de consulta actualizado φ1 se puede usar para clasificar documentos en respuesta a Q1. A medida que el usuario ve algunos documentos, el texto de resumen mostrado para dichos documentos C1 (es decir, resúmenes en los que se hace clic) puede servir como algunos datos nuevos para que actualicemos aún más el modelo de consulta para obtener φ1. A medida que obtenemos la segunda consulta Q2 del usuario, podemos actualizar φ1 para obtener un nuevo modelo φ2. En general, podemos repetir este proceso de actualización para actualizar iterativamente el modelo de consulta. Claramente, vemos dos tipos de actualización: (1) actualización basada en una nueva consulta Qi;(2) Actualización basada en un nuevo resumen de CI. En ambos casos, podemos tratar el modelo actual como un modelo previo del modelo de consulta de contexto y tratar la nueva consulta observada o hacer clic en el resumen como datos observados. Por lo tanto, tenemos las siguientes ecuaciones de actualización: P (W | φi) = C (W, Qi) + µIP (W | φi - 1) | Qi |+ µi p (w | φi) = c (w, ci) + νip (w | φi) | Ci |+ νi donde µI es el tamaño de muestra equivalente para el anterior cuando se actualiza el modelo basado en una consulta, mientras que νi es el tamaño de muestra equivalente para el anterior cuando se actualiza el modelo basado en un resumen haciendo clic. Si establecemos µi = 0 (o νi = 0) esencialmente ignoramos el modelo anterior, por lo tanto, iniciaría un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen de resumen hecho). Por otro lado, si establecemos µi = +∞ (o νi = +∞) esencialmente ignoramos la consulta observada (o el resumen haciendo clic) y no actualizamos nuestro modelo. Por lo tanto, el modelo sigue siendo el mismo que si no observamos ninguna evidencia de texto nueva. En general, los parámetros µI y νi pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, podemos tener un historial de consultas muy escaso, por lo que podríamos usar un µI más pequeño, pero más tarde como el historial de consultas es más rico, podemos considerar usar un µI más grande. Pero en nuestros experimentos, a menos que se indiquen lo contrario, los colocamos en las mismas constantes, es decir, ∀i, j, µi = µJ, νi = νJ. Tenga en cuenta que podemos tomar P (W | φi) o P (W | φi) como nuestro modelo de consulta de contexto para documentos de clasificación. Esto sugiere que no tenemos que esperar hasta que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperación;En su lugar, tan pronto como recolectamos un resumen de CI, podemos actualizar el modelo de consulta y usar P (w | φi) para volver a relacionar inmediatamente cualquier documento que un usuario aún no haya visto. Para calificar documentos después de ver la consulta QK, usamos P (W | φk), es decir, P (W | θk) = P (W | φk) 3.5 Actualización bayesiana por lotes (BatchUp) Si establecemos los parámetros de tamaño de muestra equivalentes en constante fijo., el algoritmo en línea introduciría un factor en descomposición: la interpolación repetida haría que los datos tempranos tengan un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario se vuelve cada vez mejor en la formulación de la consulta a medida que pasa el tiempo, pero no es necesariamente apropiado para la información de clics, especialmente porque usamos el resumen mostrado, en lugar deEl contenido real de un documento haciendo clic. Una forma de evitar aplicar una interpolación en descomposición a los datos de clics es hacer en línea solo para el historial de consulta Q = (Q1, ..., Qi - 1), pero no para los datos de clics C. Primero amortamos todos los datos de clics.Juntos y usen toda la parte de los datos de clic para actualizar el modelo generado mediante la ejecución de OnlineUP en consultas anteriores. Las ecuaciones de actualización son las siguientes.p (w | φi) = c (w, qi) + µip (w | φi - 1) | qi |+ µi p (w | ψi) = i - 1 j = 1 c (w, cj) + νip (w | φi) i - 1 j = 1 | cj |+ νi donde µI tiene la misma interpretación que en onlineup, pero ahora indica en qué medida queremos confiar en los resúmenes haciendo clic. Como en OnlineUP, establecemos todos los µis y νis al mismo valor. Y para clasificar los documentos después de ver la consulta actual QK, usamos P (W | θk) = P (W | ψk) 4. Recopilación de datos Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino también el historial de consulta y el historial de clics para cada tema. Dado que no hay dicho conjunto de datos disponible para nosotros, tenemos que crear uno. Hay dos opciones. Una es extraer temas y cualquier historial de consultas asociado e historial de clic para cada tema del registro de un sistema de recuperación (por ejemplo, motor de búsqueda). Pero el problema es que no tenemos juicios de relevancia sobre dichos datos. La otra opción es usar un conjunto de datos TREC, que tiene una base de datos de texto, descripción del tema y archivo de juicio de relevancia. Desafortunadamente, no hay historial de consultas ni datos de historial de clics. Decidimos aumentar un conjunto de datos TREC recopilando el historial de consultas y los datos del historial de clics. Seleccionamos los datos TREC AP88, AP89 y AP90 como nuestra base de datos de texto, porque los datos AP se han utilizado en varias tareas TREC y tiene juicios relativamente completos. Hay artículos de noticias en total 242918 y la longitud promedio del documento es de 416 palabras. La mayoría de los artículos tienen títulos. Si no, seleccionamos la primera oración del texto como título. Para el preprocesamiento, solo hacemos plegamiento de casos y no hacemos la eliminación de la palabra de parada o las derivaciones. Seleccionamos 30 temas relativamente difíciles de TREC TEMICS 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisión entre los temas de TREC 1-150 de acuerdo con algunos experimentos de línea de base utilizando el modelo de divergencia KL con suavizado bayesiano [20]. La razón por la que seleccionamos temas difíciles es que el usuario tendría que tener varias interacciones con el sistema de recuperación para obtener resultados satisfactorios para que podamos esperar recopilar un historial de consultas relativamente más rico y los datos del historial de clics del usuario. En aplicaciones reales, también podemos esperar que nuestros modelos sean más útiles para temas tan difíciles, por lo que nuestra estrategia de recopilación de datos refleja bien las aplicaciones del mundo real. Indexemos el conjunto de datos TREC AP y configuramos un motor de búsqueda y una interfaz web para artículos de noticias TREC AP. Utilizamos 3 sujetos para hacer experimentos para recopilar el historial de consultas y los datos del historial de clics. A cada sujeto se le asigna 10 temas y dadas las descripciones del tema proporcionadas por TREC. Para cada tema, la primera consulta es el título del tema dado en la descripción original del tema TREC. Después de que el sujeto presente la consulta, el motor de búsqueda hará recuperación y devolverá una lista clasificada de resultados de búsqueda al sujeto. El sujeto navegará por los resultados y tal vez haga clic en uno o más resultados para explorar el texto completo de los artículos. El sujeto también puede modificar la consulta para hacer otra búsqueda. Para cada tema, el tema compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el número de tema de un menú de selección antes de enviar la consulta al motor de búsqueda para que podamos detectar fácilmente el límite de la sesión, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones del usuario, incluidas las consultas enviadas y los documentos haciendo clic. Para cada consulta, almacenamos los términos de consulta y las páginas de resultados asociadas. Y para cada documento haciendo clic, almacenamos el resumen como se muestra en la página de resultados de búsqueda. El resumen del artículo depende de la consulta y se calcula en línea utilizando la recuperación de pasaje de longitud fija (modelo de divergencia KL con suavizado bayesiano). Entre las 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de la consulta es de 3.71 palabras. En total, hay 91 documentos que se hacen clic para ver. Entonces, en promedio, hay alrededor de 3 clics por tema. La longitud promedio de la consulta de resumen de resumen en línea en línea de la consulta de lotes (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) Map@20docs mapa pr@20docs mapa pr@20 docs map pr@20docs Q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + hc 0.0311117 0.031110 0.03110 0.031110 0.031110 0.031141110 0.031110 0.031110 0.0311411141110 0.03114111111 215 0.0733 0.0342 0.1100 Mejorar.3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8%39.4% 67.7% 20.2% 92.4% 39.4% Q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 Q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250. 9% 47.8% 6.9% 77.2% 16.4% Tabla1: Efecto del uso del historial de consultas y los datos de clic para la clasificación de documentos.son 34.4 palabras. Entre 91 documentos haciendo clic, 29 documentos se juzgan relevantes de acuerdo con el archivo de juicio de TREC. Este conjunto de datos está disponible públicamente 1.5. Experimentos 5.1 Diseño del experimento Nuestra hipótesis principal es que el uso del contexto de búsqueda (es decir, el historial de consultas e información de clics) puede ayudar a mejorar la precisión de la búsqueda. En particular, el contexto de búsqueda puede proporcionar información adicional para ayudarnos a estimar un mejor modelo de consulta que usar solo la consulta actual. Por lo tanto, la mayoría de nuestros experimentos implican comparar el rendimiento de la recuperación utilizando solo la consulta actual (ignorando así cualquier contexto) con eso utilizando la consulta actual y el contexto de búsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, hacemos tales comparaciones para cada versión de consultas. Utilizamos dos medidas de rendimiento: (1) Precisión promedio media (MAP): esta es la precisión promedio no interpolada estándar y sirve como una buena medida de la precisión general de clasificación.(2) Precisión a 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es más significativa que el mapa y refleja la utilidad para los usuarios que solo leen los 20 documentos principales. En todos los casos, la cifra informada es el promedio sobre los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de búsqueda (es decir, FixInt, BayesInt, Onlineup y Batchup). Cada modelo tiene precisamente dos parámetros (α y β para fijación; µ y ν para otros). Tenga en cuenta que µ y ν pueden necesitar ser interpretados de manera diferente para diferentes métodos. Varimos estos parámetros e identificamos el rendimiento óptimo para cada método. También variamos los parámetros para estudiar la sensibilidad de nuestros algoritmos a la configuración de los parámetros.5.2 Análisis de resultados 5.2.1 Efecto general del contexto de búsqueda Comparamos el rendimiento óptimo de cuatro modelos con aquellos que usan la consulta actual solo en la Tabla 1. Una fila etiquetada con Qi es el rendimiento de línea de base y una fila etiquetada con Qi + HQ + HC es el rendimiento de usar el contexto de búsqueda. Podemos hacer varias observaciones de esta tabla: 1. La comparación de las actuaciones de referencia indica que en promedio las consultas reformuladas son mejores que las consultas anteriores con el desempeño de que Q4 es el mejor. Los usuarios generalmente formulan mejores y mejores consultas.2. El uso del contexto de búsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede ver por el hecho de que la 1 http://sifaka.cs.uiuc.edu/ir/ucair/qchistory.zip, la mejora para Q4 y Q3 es generalmente más sustancial en comparación con Q2. En realidad, en muchos casos con Q2, usar el contexto puede dañar el rendimiento, probablemente porque la historia en ese punto es escasa. Cuando el contexto de búsqueda es rico, la mejora del rendimiento puede ser bastante sustancial. Por ejemplo, Batchup logra una mejora del 92.4% en la precisión promedio media sobre el Q3 y el 77.2% de mejora con respecto a Q4.(Sin embargo, las precisiones generalmente bajas también hacen que la mejora relativa sea engañosamente alta, sin embargo.) 3. Entre los cuatro modelos que usan el contexto de búsqueda, las actuaciones de FixTint y Onlineup son claramente peores que las de BayesInt y Batchup. Dado que BayesInt funciona mejor que la fijación y la principal diferencia entre Bayesint y Fixint es que el primero usa un coeficiente adaptativo para la interpolación, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y una interpolación de estilo bayesiano tiene sentido. La principal diferencia entre Onlineup y Batchup es que OnlineUp utiliza coeficientes de descomposición para combinar los múltiples resúmenes haciendo clic, mientras que Batchup simplemente concatena todos los resúmenes haciendo clic. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que en línea indica que los pesos para combinar los resúmenes haciendo clic en realidad no deberían estar en descomposición. Si bien OnlineUp es teóricamente atractivo, su rendimiento es inferior a Bayesint y Batchup, probablemente debido al coeficiente en descomposición. En general, Batchup parece ser el mejor método cuando variamos la configuración de los parámetros. Tenemos dos tipos diferentes de contexto de búsqueda: el historial de consultas y los datos de clics. Ahora analizamos la contribución de cada tipo de contexto.5.2.2 Utilizando el historial de consultas solo en cada uno de los cuatro modelos, podemos desactivar los datos del historial de clics configurando los parámetros de manera adecuada. Esto nos permite evaluar el efecto de usar el historial de consultas solo. Utilizamos la misma configuración de parámetros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. Aquí vemos que, en general, el beneficio de usar el historial de consultas es muy limitado con resultados mixtos. Esto es diferente de lo que se informa en un estudio anterior [15], donde el uso del historial de consultas es consistentemente útil. Otra observación es que las ejecuciones de contexto funcionan mal en Q2, pero generalmente funcionan (ligeramente) mejor que las líneas de base para Q3 y Q4. Esto es nuevamente probable porque al principio la consulta inicial, que es el título en la descripción del tema original de TREC, puede no ser una buena consulta;De hecho, en promedio, las actuaciones de estas consultas de primera generación son claramente más pobres que las de todas las demás consultas formuladas por el usuario en las generaciones posteriores. Otra observación más es que cuando se usa el historial de consultas solamente, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, en línea y la consulta de Batchint Batchint Bayesint en línea (α = 0.1, β = 0) (µ = 0.2, ν = 0) (µ = 5.0, ν = +∞) (µ = 2.0, ν = =+ ∞) Map PR@20Docs Map PR@20Docs Map PR@20Docs Map PR@20Docs Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 Q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 mejora.-68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2%7.1% 2.3% 5.5% -10.1% 8.1% -2.2% Q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 Q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 mejoran % 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto del uso del historial de consultas solo para la clasificación de documentos.µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.05520.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: Precisión promedio de Batchup usando el historial de consultas Solo son esencialmente el mismo algoritmo. Los resultados mostrados reflejan así la variación causada por el parámetro µ. Una configuración más pequeña de 2.0 se ve mejor que un valor mayor de 5.0. Se puede ver una imagen más completa de la influencia de la configuración de µ en la Tabla 3, donde mostramos las cifras de rendimiento para un rango más amplio de valores de µ. El valor de µ se puede interpretar como cuántas palabras consideramos el historial de consultas. Un valor mayor, por lo tanto, pone más peso en la historia y se considera que duele más el rendimiento cuando la información del historial no es rica. Por lo tanto, mientras que para Q4 el mejor rendimiento tiende a lograrse para µ ∈ [2, 5], solo cuando µ = 0.5 vemos un pequeño beneficio para Q2. Como es de esperar, un µ excesivamente grande dañaría el rendimiento en general, pero el Q2 se duele más y Q4 apenas se duele, lo que indica que a medida que acumulamos más y más información sobre el historial de consultas, podemos poner más y más peso sobre la información del historial. Esto también sugiere que una mejor estrategia probablemente debería ajustar dinámicamente los parámetros de acuerdo con la cantidad de información del historial que tenemos. Los resultados del historial de consultas mixtas sugieren que el efecto positivo del uso de la información de retroalimentación implícita puede provenir en gran medida del uso del historial de clics, lo cual es cierto como discutimos en la próxima subsección.5.2.3 Usando el historial de clics solo ahora apagamos el historial de consultas y solo usamos los resúmenes haciendo clic más la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de usar información de clics es mucho más significativo que el de usar el historial de consultas. Vemos un efecto positivo general, a menudo con una mejora significativa sobre la línea de base. También está claro que cuanto más ricos sean los datos del contexto, más mejora utilizando resúmenes haciendo clic. Aparte de una degradación ocasional de precisión en 20 documentos, la mejora es bastante consistente y, a menudo, bastante sustancial. Estos resultados muestran que el texto de resumen hecho hecho es en general bastante útil para inferir una necesidad de la información del usuario. Intuitivamente, utilizando el texto resumido, en lugar del contenido real del documento, tiene más sentido, ya que es muy posible que el documento detrás de un resumen aparentemente relevante sea realmente no relevante.29 de los 91 documentos haciendo clic son relevantes. La actualización del modelo de consulta basado en tales resúmenes elevaría los rangos de estos documentos relevantes, causando una mejora del rendimiento. Sin embargo, dicha mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuánta mejora hemos logrado para mejorar los rangos de los documentos relevantes invisibles, excluyimos estos 29 documentos relevantes de nuestro archivo de juicio y recomputamos el desempeño de BayesInt y la línea de base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Tenga en cuenta que el rendimiento del método de referencia es menor debido a la eliminación de los 29 documentos relevantes, que generalmente se habrían clasificado en los resultados. De la Tabla 5, vemos claramente que el uso de resúmenes haciendo clic también ayuda a mejorar significativamente las filas de documentos relevantes invisibles. Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0263 0.100 Q2 + HC 0.0314 0.100 Mejorar.19.4% 0% Q3 0.0331 0.125 Q3 + HC 0.0661 0.178 Mejorar 99.7% 42.4% Q4 0.0442 0.165 Q4 + HC 0.0739 0.188 Mejorar 67.2% 13.9% Tabla 5: BayesInt evaluada en documentos relevantes no considerados La pregunta que no tiene siete hace clic si el hecho es útil que los datos siguen siendo útilesSi ninguno de los documentos haciendo clic es relevante. Para responder a esta pregunta, sacamos los 29 resúmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto más pequeño de resúmenes HC y reevaluamos el rendimiento del método BayesInt utilizando HC con la misma configuración de parámetros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisión promedio se mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan en solo 62 clics no relevantes. En realidad, un usuario es más probable que haga clic en algunos resúmenes relevantes, lo que ayudaría a traer documentos más relevantes como hemos visto en la Tabla 4 y la Tabla 5. FixInt Bayesint OnlineUp Batchup Consulter (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i <k, µi = +∞) (µ = 0, ν = 15) MAP PR@20DOCS MAP PR@20DOCS MAP PR@20DOCS MAP PR@20DOCS Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 Q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Mejora.3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% Q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 Q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.05513. 9% 37.1% 47.7% 19.2% 21.9% 11.3% Q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 Q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 mejorar 66.2% 15.5% 72.6%) : Efecto de usar datos de clicsSolo para la clasificación de documentos. Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0312 0.1150 Q2 + HC 0.0313 0.0950 Mejora.0.3% -17.4% Q3 0.0421 0.1483 Q3 + HC 0.0521 0.1820 Mejorar 23.8% 23.0% Q4 0.0536 0.1930 Q4 + HC 0.0620 0.1850 Mejora 15.7% -4.1% Tabla 6: Efecto de uso solo de datos de Clickhrough no relevantes 5.2.2Información de contexto comparando los resultados en la Tabla 1, la Tabla 2 y la Tabla 4, podemos ver que el beneficio de la información del historial de consultas y el de la información de clics son en su mayoría aditivos, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno solo, peroLa mayor parte de la mejora claramente proviene de la información de clics. En la Tabla 7, mostramos este efecto para el método Batchup.5.2.5 Sensibilidad de parámetros Los cuatro modelos tienen dos parámetros para controlar los pesos relativos de HQ, HC y QK, aunque la parametrización es diferente de un modelo a otro. En esta subsección, estudiamos la sensibilidad de los parámetros para Batchup, que parece funcionar relativamente mejor que otros. Batchup tiene dos parámetros µ y ν. Primero miramos µ. Cuando µ se establece en 0, el historial de consultas no se usa en absoluto, y esencialmente usamos los datos de clic combinados con la consulta actual. Si aumentamos µ, incorporaremos gradualmente más información de las consultas anteriores. En la Tabla 8, mostramos cómo cambia la precisión promedio de Batchup a medida que variamos µ con ν fijado a 15.0, donde se logra el mejor rendimiento de Batchup. Vemos que el rendimiento es mayormente insensible al cambio de µ para Q3 y Q4, pero está disminuyendo a medida que µ aumenta para Q2. El patrón también es similar cuando establecemos ν en otros valores. Además del hecho de que Q1 es generalmente peor que Q2, Q3 y Q4, otra posible razón por la cual la sensibilidad es menor para Q3 y Q4 puede ser que generalmente tenemos más datos de clic disponibles para Q3 y Q4 que para Q2 y elLa influencia dominante de los datos de clics ha hecho las pequeñas diferencias causadas por µ menos visibles para Q3 y Q4. El mejor rendimiento generalmente se logra cuando µ es alrededor de 2.0, lo que significa que la información de consulta pasada es tan útil como aproximadamente 2 palabras en la consulta actual. Excepto por Q2, claramente existe cierta compensación entre la consulta actual y el mapa de consultas de consultas anteriores PR@20DOCS Q2 0.0312 0.1150 Q2 + HQ 0.0287 0.0967 Mejora.-8.0% -15.9% Q2 + HC 0.0344 0.1167 Mejora.10.3% 1.5% Q2 + HQ + HC 0.0342 0.1100 Mejorar.9.6% -4.3% Q3 0.0421 0.1483 Q3 + HQ 0.0455 0.1450 Mejorar 8.1% -2.2% Q3 + HC 0.0513 0.1650 Mejorar 21.9% 11.3% Q3 + HQ + HC 0.0810 0.2067 Mejorar 92.4% 39.4% Q4 0.0536 0.1930 Q4 + HQ + HQ + HQ + HQ3.0% -0.8% Q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% Q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: beneficio aditivo de la información de contexto y el uso de una combinación equilibrada de ellos logran un mejor rendimiento que usar cada uno de ellossolo. Ahora pasamos al otro parámetro ν. Cuando ν está configurado en 0, solo usamos los datos de clics;Cuando ν se establece en +∞, solo usamos el historial de consulta y la consulta actual. Con µ establecido en 2.0, donde se logra el mejor rendimiento de Batchup, variamos ν y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando ν ≤ 30, con el mejor rendimiento a menudo logrado en ν = 15. Esto significa que la información combinada del historial de consultas y la consulta actual es tan útil como unas 15 palabras en los datos de clics, lo que indica que la información de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que Batchup no solo funciona mejor que otros métodos, sino que también es bastante robusto.6. Conclusiones y trabajo futuro En este documento, hemos explorado cómo explotar la información de retroalimentación implícita, incluido el historial de consultas y el historial de clics dentro de la misma sesión de búsqueda, para mejorar el rendimiento de la recuperación de la información. Utilizando el modelo de recuperación de divergencia KL como base, propusimos y estudiamos cuatro modelos de lenguaje estadístico para la recuperación de información sensible al contexto, es decir, FixInt, BayesInt, Onlineup y Batchup. Usamos datos de TREC AP para crear un conjunto de prueba µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 Q2 + HQ + HC PR@20 0.1333 0.1233 0.1100 0.1033 0.0217 0.09033333333333333333333333333333333333333333333333333333333333333333333333333333333NA ¿0.10333333333NA. 70.0783 0.0767 0.0750 Mapa 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 Q3 + HQ + HC PR@20 0.210 0.2150 0.2067 0.20567 0.205 0.2067 0.2067 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 Q4 + HQ +HC PR@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de µ en Batchup ν 0 1 2 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0296 0.0290 Q2 + HQ + HCPR@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 mapa 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 Q3 + hq + hq + hc@20 201917 2067 0.2017 0.1783 0.1600 0.1550 Mapa 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.09190.0761 0.0664 0.0625 Q4 + HQ + HC PR@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de ν en lote para evaluar modelos de retroalimentación implícitos. Los resultados del experimento muestran que el uso de la retroalimentación implícita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de la recuperación sin requerir ningún esfuerzo adicional del usuario. El trabajo actual se puede extender de varias maneras: primero, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar información de retroalimentación implícita. Sería interesante desarrollar modelos más sofisticados para explotar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar un resumen hecho de manera diferente dependiendo de si la consulta actual es una generalización o refinamiento de la consulta anterior. En segundo lugar, los modelos propuestos se pueden implementar en cualquier sistema práctico. Actualmente estamos desarrollando un agente de búsqueda personalizado del lado del cliente, que incorporará algunos de los algoritmos propuestos. También haremos un estudio de usuario para evaluar la efectividad de estos modelos en la búsqueda web real. Finalmente, debemos estudiar más a fondo un marco de recuperación general para la toma de decisiones secuenciales en la recuperación de información interactiva y estudiar cómo optimizar algunos de los parámetros en los modelos de recuperación sensibles al contexto.7. Agradecimientos Este material se basa en parte en el trabajo respaldado por la National Science Foundation bajo los números de premios IIS-0347933 e IIS-0428472. Agradecemos a los revisores anónimos por sus útiles comentarios.8. Referencias [1] E. Adar y D. Karger. Haystack: entornos de información por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. Desafíos en la recuperación de la información y el modelado de idiomas. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. SearchPad: Captura explícita del contexto de búsqueda para admitir la búsqueda web. En procedimiento de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. Comentarios y personalización de relevancia: una perspectiva de modelado de idiomas. En el taller de Segundo Delos: Sistemas de personalización y recomendación en bibliotecas digitales, 2001. [5] H. Cui, J.-R.Wen, J.-Y. Nie y W.-Y. Mamá. Expansión de consultas probabilísticas utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Currell, R. Sarin y E. Horvitz. Consultas implícitas (IQ) para la búsqueda contextualizada (descripción de demostración). En Actas de Sigir 2004, página 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocar la búsqueda en contexto: el concepto revisitado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien e Y. Oyang. Sugerencia de término basado en la sesión de consulta para la búsqueda web interactiva. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. Identificación de sesión de registro web dinámico con modelos de lenguaje estadístico. Journal of the American Society for Information Science and Technology, 55 (14): 1290-1303, 2004. [10] G. Jeh y J. Widom. Escala de búsqueda web personalizada. En procedimiento de WWW 2003, 2003. [11] T. Joachims. Optimización de los motores de búsqueda utilizando datos de clics. En Actas de Sigkdd 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar tiempo como retroalimentación implícita: comprensión de los efectos de la tarea. En Actas de Sigir 2004, 2004. [13] D. Kelly y J. Teevan. Comentarios implícitos para inferir la preferencia del usuario. Sigir Forum, 32 (2), 2003. [14] J. Rocchio. Relevancia de retroalimentación de retroalimentación. En los experimentos del sistema de recuperación inteligente en el procesamiento automático de documentos, páginas 313-323, Kansas City, MO, 1971. Prentice Hall.[15] X. Shen y C. Zhai. Explotación del historial de consultas para la clasificación de documentos en la recuperación de información interactiva (póster). En Actas de Sigir 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de búsqueda basado en sesión (póster). En Actas de Sigir 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil de usuario construido sin ningún esfuerzo de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. Van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentación implícitos. En Actas de ECIR 2004, páginas 311-326, 2004. [19] C. Zhai y J. Lafferty. Comentarios basados en modelos en el modelo de recuperación de divergencia KL. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. En Actas de Sigir 2001, 2001.",
    "original_sentences": [
        "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
        "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
        "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
        "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
        "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
        "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
        "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
        "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
        "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
        "There are many kinds of context that we can exploit.",
        "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
        "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
        "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
        "Thus the effectiveness of relevance feedback may be limited in real applications.",
        "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
        "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
        "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
        "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
        "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
        "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
        "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
        "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
        "However, any particular user is unlikely searching for both types of documents.",
        "Such an ambiguity can be resolved by exploiting history information.",
        "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
        "Implicit feedback was studied in several previous works.",
        "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
        "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
        "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
        "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
        "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
        "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
        "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
        "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
        "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
        "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
        "To the best of our knowledge, this is the first test set for implicit feedback.",
        "We evaluate the proposed models using this data set.",
        "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
        "The remaining sections are organized as follows.",
        "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
        "In Section 3, we propose several implicit feedback models based on statistical language models.",
        "In Section 4, we describe how we create the data set for implicit feedback experiments.",
        "In Section 5, we evaluate different implicit feedback models on the created data set.",
        "Section 6 is our conclusions and future work. 2.",
        "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
        "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
        "A session can be considered as a period consisting of all interactions for the same information need.",
        "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
        "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
        "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
        "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
        "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
        "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
        "In a single search session, a user may interact with the search system several times.",
        "During interactions, the user would continuously modify the query.",
        "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
        "Note that we assume that the session boundaries are known in this paper.",
        "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
        "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
        "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
        "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
        "In addition to the query history, there may be other short-term context information available.",
        "For example, a user would presumably frequently click some documents to view.",
        "We refer to data associated with these actions as clickthrough history.",
        "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
        "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
        "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
        "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
        "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
        "Previous work has also shown positive results using similar clickthrough information [11, 17].",
        "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
        "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
        "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
        "An important research question is how we can exploit such information effectively.",
        "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
        "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
        "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
        "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
        "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
        "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
        "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
        "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
        "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
        "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
        "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
        "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
        "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
        "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
        "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
        "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
        "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
        "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
        "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
        "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
        "This means that all previous queries are treated equally and so are all clicked summaries.",
        "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
        "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
        "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
        "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
        "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
        "In order to rank documents, the system must have some model for the users information need.",
        "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
        "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
        "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
        "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
        "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
        "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
        "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
        "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
        "Initially, before we see any user query, we may already have some information about the user.",
        "For example, we may have some information about what documents the user has viewed in the past.",
        "We use such information to define a prior on the query model, which is denoted by φ0.",
        "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
        "The updated query model φ1 can then be used for ranking documents in response to Q1.",
        "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
        "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
        "In general, we may repeat such an updating process to iteratively update the query model.",
        "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
        "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
        "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
        "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
        "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
        "Thus the model remains the same as if we do not observe any new text evidence.",
        "In general, the parameters µi and νi may have different values for different i.",
        "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
        "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
        "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
        "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
        "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
        "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
        "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
        "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
        "As in OnlineUp, we set all µis and νis to the same value.",
        "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
        "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
        "Since there is no such data set available to us, we have to create one.",
        "There are two choices.",
        "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
        "But the problem is that we have no relevance judgments on such data.",
        "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
        "Unfortunately, there are no query history and clickthrough history data.",
        "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
        "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
        "There are altogether 242918 news articles and the average document length is 416 words.",
        "Most articles have titles.",
        "If not, we select the first sentence of the text as the title.",
        "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
        "We select 30 relatively difficult topics from TREC topics 1-150.",
        "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
        "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
        "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
        "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
        "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
        "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
        "For each topic, the first query is the title of the topic given in the original TREC topic description.",
        "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
        "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
        "The subject may also modify the query to do another search.",
        "For each topic, the subject composes at least 4 queries.",
        "In our experiment, only the first 4 queries for each topic are used.",
        "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
        "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
        "For each query, we store the query terms and the associated result pages.",
        "And for each clicked document, we store the summary as shown on the search result page.",
        "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
        "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
        "Altogether there are 91 documents clicked to view.",
        "So on average, there are around 3 clicks per topic.",
        "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
        "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
        "This data set is publicly available 1 . 5.",
        "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
        "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
        "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
        "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
        "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
        "In all cases, the reported figure is the average over all of the 30 topics.",
        "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
        "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
        "Note that µ and ν may need to be interpreted differently for different methods.",
        "We vary these parameters and identify the optimal performance for each method.",
        "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
        "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
        "We can make several observations from this table: 1.",
        "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
        "Users generally formulate better and better queries. 2.",
        "Using search context generally has positive effect, especially when the context is rich.",
        "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
        "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
        "When the search context is rich, the performance improvement can be quite substantial.",
        "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
        "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
        "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
        "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
        "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
        "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
        "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
        "We have two different kinds of search context - query history and clickthrough data.",
        "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
        "This allows us to evaluate the effect of using query history alone.",
        "We use the same parameter setting for query history as in Table 1.",
        "The results are shown in Table 2.",
        "Here we see that in general, the benefit of using query history is very limited with mixed results.",
        "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
        "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
        "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
        "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
        "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
        "The displayed results thus reflect the variation caused by parameter µ.",
        "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
        "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
        "The value of µ can be interpreted as how many words we regard the query history is worth.",
        "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
        "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
        "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
        "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
        "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
        "The results are shown in Table 4.",
        "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
        "We see an overall positive effect, often with significant improvement over the baseline.",
        "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
        "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
        "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
        "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
        "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
        "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
        "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
        "The results are shown in Table 5.",
        "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
        "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
        "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
        "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
        "The results are shown in Table 6.",
        "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
        "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
        "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
        "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
        "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
        "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
        "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
        "BatchUp has two parameters µ and ν.",
        "We first look at µ.",
        "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
        "If we increase µ, we will gradually incorporate more information from the previous queries.",
        "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
        "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
        "The pattern is also similar when we set ν to other values.",
        "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
        "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
        "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
        "We now turn to the other parameter ν.",
        "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
        "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
        "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
        "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
        "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
        "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
        "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
        "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
        "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
        "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
        "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
        "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
        "Second, the proposed models can be implemented in any practical systems.",
        "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
        "We will also do a user study to evaluate effectiveness of these models in the real web search.",
        "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
        "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
        "We thank the anonymous reviewers for their useful comments. 8.",
        "REFERENCES [1] E. Adar and D. Karger.",
        "Haystack: Per-user information environments.",
        "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
        "Challenges in information retrieval and language modeling.",
        "Workshop at University of Amherst, 2002. [3] K. Bharat.",
        "Searchpad: Explicit capture of search context to support web search.",
        "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
        "Relevance feedback and personalization: A language modeling perspective.",
        "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
        "Nie, and W.-Y.",
        "Ma.",
        "Probabilistic query expansion using query logs.",
        "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
        "Implicit queries (IQ) for contextualized search (demo description).",
        "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
        "Placing search in context: The concept revisited.",
        "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
        "Query session based term suggestion for interactive web search.",
        "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
        "An, and D. Schuurmans.",
        "Dynamic web log session identification with statistical language models.",
        "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
        "Scaling personalized web search.",
        "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
        "Optimizing search engines using clickthrough data.",
        "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
        "Display time as implicit feedback: Understanding task effects.",
        "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
        "Implicit feedback for inferring user preference.",
        "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
        "Relevance feedback information retrieval.",
        "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
        "Prentice-Hall. [15] X. Shen and C. Zhai.",
        "Exploiting query history for document ranking in interactive information retrieval (poster).",
        "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
        "A session-based search engine (poster).",
        "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
        "Adaptive web search based on user profile constructed without any effort from users.",
        "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
        "A simulated study of implicit feedback models.",
        "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
        "Model-based feedback in the KL-divergence retrieval model.",
        "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
        "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
        "In Proceedings of SIGIR 2001, 2001."
    ],
    "error_count": 0,
    "keys": {
        "retrieval accuracy": {
            "translated_key": "precisión de recuperación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve <br>retrieval accuracy</br> in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve <br>retrieval accuracy</br>, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving <br>retrieval accuracy</br>.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the <br>retrieval accuracy</br> without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve <br>retrieval accuracy</br>.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving <br>retrieval accuracy</br>.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, estudiamos cómo explotar la información de retroalimentación implícita, incluidas consultas anteriores e información de clics, para mejorar la \"precisión de recuperación\" en un entorno de recuperación de información interactiva.",
                "Por lo tanto, un sistema de recuperación óptimo debe tratar de explotar la mayor cantidad de información de contexto adicional posible para mejorar la \"precisión de recuperación\", siempre que esté disponible.",
                "La retroalimentación de relevancia [14] puede considerarse como una forma para que un usuario proporcione más contexto de búsqueda y se sabe que es efectivo para mejorar la \"precisión de recuperación\".",
                "Una ventaja importante de la retroalimentación implícita es que podemos mejorar la \"precisión de recuperación\" sin requerir ningún esfuerzo del usuario.",
                "Específicamente, desarrollamos modelos para usar información de retroalimentación implícita, como consulta e historial de clics de la sesión de búsqueda actual para mejorar la \"precisión de recuperación\".",
                "De hecho, nuestro trabajo anterior [15] ha demostrado que el historial de consultas a corto plazo es útil para mejorar la \"precisión de recuperación\"."
            ],
            "translated_text": "",
            "candidates": [
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación",
                "precisión de recuperación"
            ],
            "error": []
        },
        "implicit feedback information": {
            "translated_key": "Información de retroalimentación implícita",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit <br>implicit feedback information</br>, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such <br>implicit feedback information</br> can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using <br>implicit feedback information</br> such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with <br>implicit feedback information</br>, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using <br>implicit feedback information</br>, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are <br>implicit feedback information</br>, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using <br>implicit feedback information</br> may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit <br>implicit feedback information</br>, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating <br>implicit feedback information</br>.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, estudiamos cómo explotar la \"información de retroalimentación implícita\", incluidas consultas anteriores e información de clics, para mejorar la precisión de la recuperación en un entorno de recuperación de información interactiva.",
                "En [11], Joachims exploró cómo capturar y explotar la información de clics y demostró que dicha \"información de retroalimentación implícita\" puede mejorar la precisión de búsqueda para un grupo de personas.",
                "Específicamente, desarrollamos modelos para usar \"información de retroalimentación implícita\", como la consulta y el historial de clics de la sesión de búsqueda actual para mejorar la precisión de la recuperación.",
                "Por lo tanto, usamos los datos de TREC AP para crear una recopilación de pruebas con \"información de retroalimentación implícita\", que puede usarse para evaluar cuantitativamente modelos de retroalimentación implícitos.",
                "Los resultados experimentales muestran que el uso de \"información de retroalimentación implícita\", especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de la recuperación sin requerir un esfuerzo adicional del usuario.",
                "Tanto el historial de consultas como el historial de clics son \"información de retroalimentación implícita\", que naturalmente existe en la recuperación de información interactiva, por lo tanto, no se necesita ningún esfuerzo adicional para el usuario para recopilarlos.",
                "Los resultados del historial de consultas mixtas sugieren que el efecto positivo del uso de la \"información de retroalimentación implícita\" puede provenir en gran medida del uso del historial de clics, lo que de hecho es cierto como discutimos en la próxima subsección.5.2.3 Usando el historial de clics solo ahora apagamos el historial de consultas y solo usamos los resúmenes haciendo clic más la consulta actual.",
                "Conclusiones y trabajo futuro En este documento, hemos explorado cómo explotar la \"información de retroalimentación implícita\", incluido el historial de consultas y el historial de clics dentro de la misma sesión de búsqueda, para mejorar el rendimiento de la recuperación de la información.",
                "El trabajo actual se puede extender de varias maneras: primero, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar \"información de retroalimentación implícita\"."
            ],
            "translated_text": "",
            "candidates": [
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita",
                "Información de retroalimentación implícita",
                "información de retroalimentación implícita"
            ],
            "error": []
        },
        "relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "<br>relevance feedback</br> [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, <br>relevance feedback</br> requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of <br>relevance feedback</br> may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "<br>relevance feedback</br> and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "<br>relevance feedback</br> information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"retroalimentación de relevancia\" [14] puede considerarse como una forma para que un usuario proporcione más contexto de búsqueda y se sabe que es efectivo para mejorar la precisión de la recuperación.",
                "Sin embargo, la \"retroalimentación de relevancia\" requiere que un usuario proporcione explícitamente información de retroalimentación, como especificar la categoría de la necesidad de información o marcar un subconjunto de documentos recuperados como documentos relevantes.",
                "Por lo tanto, la efectividad de la \"retroalimentación de relevancia\" puede ser limitada en aplicaciones reales.",
                "\"Comentarios de relevancia\" y personalización: una perspectiva de modelado de idiomas.",
                "Recuperación de información de \"retroalimentación de relevancia\"."
            ],
            "translated_text": "",
            "candidates": [
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "Comentarios de relevancia",
                "retroalimentación de relevancia"
            ],
            "error": []
        },
        "interactive retrieval": {
            "translated_key": "recuperación interactiva",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an <br>interactive retrieval</br> scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an <br>interactive retrieval</br> system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an <br>interactive retrieval</br> process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este escenario de \"recuperación interactiva\", la información disponible naturalmente para el sistema de recuperación es más que la consulta actual del usuario y la recopilación de documentos; en general, todo el historial de interacción puede estar disponible para el sistema de recuperación, que incluye consultas anteriores, informaciónsobre qué documentos ha elegido ver el usuario, e incluso cómo un usuario ha leído un documento (por ejemplo, que parte de un documento que el usuario pasa mucho tiempo en lectura).",
                "Dado que una estrategia de actualización en línea incremental puede usarse para explotar cualquier evidencia en un sistema de \"recuperación interactiva\", la presentamos de una manera más general.",
                "Con tal conjugado anterior, la distribución predictiva de φ (o de manera equivalente, la media de la distribución posterior de φ viene dada por p (w | φ) = c (w, t) + µt p (w | φ) | t |+ µt (1) donde c (w, t) es el recuento de w en t y | t | es la longitud de T. parámetro µt indica nuestra confianza en el anterior expresado en términos de una muestra de texto equivalente comparable con T. para T.Ejemplo, µt = 1 indica que la influencia del anterior es equivalente a agregar una palabra adicional a T. 3.4.2 Actualización del modelo de consulta secuencial Ahora discutimos cómo podemos actualizar nuestro modelo de consulta a lo largo del tiempo durante un proceso de \"recuperación interactiva\" utilizando bayesianoEstimación."
            ],
            "translated_text": "",
            "candidates": [
                "recuperación interactiva",
                "recuperación interactiva",
                "recuperación interactiva",
                "recuperación interactiva",
                "Recuperación interactiva",
                "recuperación interactiva"
            ],
            "error": []
        },
        "kl-divergence retrieval model": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the <br>kl-divergence retrieval model</br> [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the <br>kl-divergence retrieval model</br> as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the <br>kl-divergence retrieval model</br>.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Utilizamos el \"modelo de recuperación de divergencia KL\" [19] como base y proponemos tratar la recuperación sensible al contexto como estimación de un modelo de lenguaje de consulta basado en la consulta actual y cualquier información de contexto de búsqueda.",
                "Utilizando el \"modelo de recuperación de divergencia KL\" como base, propusimos y estudiamos cuatro modelos de lenguaje estadístico para la recuperación de información sensible al contexto, es decir, FixInt, BayesInt, Onlineup y Batchup.",
                "Comentarios basados en modelos en el \"modelo de recuperación de divergencia KL\"."
            ],
            "translated_text": "",
            "candidates": [
                "modelo de recuperación de KL-divergencia",
                "modelo de recuperación de divergencia KL",
                "modelo de recuperación de KL-divergencia",
                "modelo de recuperación de divergencia KL",
                "modelo de recuperación de KL-divergencia",
                "modelo de recuperación de divergencia KL"
            ],
            "error": []
        },
        "context-sensitive language": {
            "translated_key": "lenguaje sensible al contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new <br>context-sensitive language</br> models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific <br>context-sensitive language</br> models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Si bien el trabajo anterior se ha centrado principalmente en usar información de clics, en este documento, utilizamos información de clics y consultas anteriores, y nos centramos en desarrollar nuevos modelos de \"lenguaje sensible al contexto\" para la recuperación.",
                "Proponemos usar modelos de lenguaje estadístico para modelar la necesidad de información de los usuarios y desarrollar cuatro modelos específicos de \"lenguaje sensible al contexto\" para incorporar información de contexto en un modelo de recuperación básica.3.1 Modelo de recuperación básica Usamos el método de divergencia Kullback-Leibbler (KL) [19] como nuestro método de recuperación básica."
            ],
            "translated_text": "",
            "candidates": [
                "lenguaje sensible al contexto",
                "lenguaje sensible al contexto",
                "lenguaje sensible al contexto",
                "lenguaje sensible al contexto"
            ],
            "error": []
        },
        "long-term context": {
            "translated_key": "contexto a largo plazo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is <br>long-term context</br>, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "<br>long-term context</br> can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some <br>long-term context</br>.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El otro tipo de contexto es el \"contexto a largo plazo\", que se refiere a información como un nivel educativo de usuarios e interés general, el historial de consultas de usuarios acumulados e información de clics de los usuarios anteriores;Dicha información es generalmente estable durante mucho tiempo y a menudo se acumula con el tiempo.",
                "El \"contexto a largo plazo\" puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisión de la búsqueda para una sesión en particular.",
                "En este artículo, nos centramos en el contexto a corto plazo, aunque algunos de nuestros métodos también se pueden usar para incorporar naturalmente algún \"contexto a largo plazo\"."
            ],
            "translated_text": "",
            "candidates": [
                "contexto a largo plazo",
                "contexto a largo plazo",
                "contexto a largo plazo",
                "contexto a largo plazo",
                "contexto a largo plazo",
                "contexto a largo plazo"
            ],
            "error": []
        },
        "short-term context": {
            "translated_key": "contexto a corto plazo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is <br>short-term context</br>, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of <br>short-term context</br>.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, <br>short-term context</br> is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the <br>short-term context</br> in improving search accuracy for a particular session.",
                "In this paper, we focus on the <br>short-term context</br>, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other <br>short-term context</br> information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Uno es el \"contexto a corto plazo\", que es la información circundante inmediata que arroja luz sobre la necesidad actual de la información actual de los usuarios en una sola sesión.",
                "La categoría de información de los usuarios necesita (por ejemplo, niños o deportes), consultas anteriores y documentos recientemente vistos son ejemplos de \"contexto a corto plazo\".",
                "En general, el \"contexto a corto plazo\" es más útil para mejorar la búsqueda en la sesión actual, pero puede no ser tan útil para las actividades de búsqueda en una sesión diferente.",
                "El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el \"contexto a corto plazo\" para mejorar la precisión de la búsqueda para una sesión en particular.",
                "En este artículo, nos centramos en el \"contexto a corto plazo\", aunque algunos de nuestros métodos también se pueden utilizar para incorporar naturalmente algún contexto a largo plazo.",
                "Además del historial de consultas, puede haber otra información de \"contexto a corto plazo\" disponible."
            ],
            "translated_text": "",
            "candidates": [
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo",
                "contexto a corto plazo"
            ],
            "error": []
        },
        "fixed coefficient interpolation": {
            "translated_key": "interpolación de coeficiente fijo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 <br>fixed coefficient interpolation</br> (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a <br>fixed coefficient interpolation</br> of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Usaremos | x |para denotar la longitud del texto x o el número total de palabras en X. 3.2 \"Interpolación de coeficiente fijo\" (Fixint) Nuestra primera idea es resumir la HQ del historial de consultas con un modelo de lenguaje unigram P (W | HQ) y la historia de Clickhrough.HC con otro modelo de idioma unigram P (W | HC).",
                "Si combinamos estas ecuaciones, vemos que p (w | θk) = αp (w | qk) + (1 - α) [βp (w | hc) + (1 - β) p (w | hq)] es decir, el modelo de consulta de contexto estimado es solo una \"interpolación de coeficiente fijo\" de tres modelos P (W | QK), P (W | HQ) y P (W | HC).3.3 Interpolación bayesiana (BayesInt) Un posible problema con el enfoque de fijación es que los coeficientes, especialmente α, se fijan en todas las consultas."
            ],
            "translated_text": "",
            "candidates": [
                "interpolación de coeficiente fijo",
                "Interpolación de coeficiente fijo",
                "interpolación de coeficiente fijo",
                "interpolación de coeficiente fijo"
            ],
            "error": []
        },
        "bayesian estimation": {
            "translated_key": "Estimación bayesiana",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use <br>bayesian estimation</br>, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply <br>bayesian estimation</br> to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using <br>bayesian estimation</br>.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Una forma principalmente de actualizar el modelo de consulta es utilizar la \"estimación bayesiana\", que discutimos a continuación.3.4.1 Actualización bayesiana Primero discutimos cómo aplicamos la \"estimación bayesiana\" para actualizar un modelo de consulta en general.",
                "Con tal conjugado anterior, la distribución predictiva de φ (o de manera equivalente, la media de la distribución posterior de φ viene dada por p (w | φ) = c (w, t) + µt p (w | φ) | t |+ µt (1) donde c (w, t) es el recuento de w en t y | t | es la longitud de T. parámetro µt indica nuestra confianza en el anterior expresado en términos de una muestra de texto equivalente comparable con T. para T.Ejemplo, µt = 1 indica que la influencia del anterior es equivalente a agregar una palabra adicional a T. 3.4.2 Actualización del modelo de consulta secuencial Ahora discutimos cómo podemos actualizar nuestro modelo de consulta a lo largo del tiempo durante un proceso de recuperación interactivo utilizando \"Estimación bayesiana\"."
            ],
            "translated_text": "",
            "candidates": [
                "Estimación bayesiana",
                "estimación bayesiana",
                "estimación bayesiana",
                "Estimación bayesiana",
                "Estimación bayesiana"
            ],
            "error": []
        },
        "trec datum set": {
            "translated_key": "Conjunto de datos de TREC",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "mean average precision": {
            "translated_key": "Precisión promedio media",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) <br>mean average precision</br> (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the <br>mean average precision</br> over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Utilizamos dos medidas de rendimiento: (1) \"Precisión promedio media\" (MAP): esta es la precisión promedio no interpolada estándar y sirve como una buena medida de la precisión general de clasificación.(2) Precisión a 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es más significativa que el mapa y refleja la utilidad para los usuarios que solo leen los 20 documentos principales.",
                "Por ejemplo, Batchup logra una mejora del 92.4% en la \"precisión promedio media\" en el Q3 y el 77.2% de mejora con respecto a Q4.(Sin embargo, las precisiones generalmente bajas también hacen que la mejora relativa sea engañosamente alta, sin embargo.) 3."
            ],
            "translated_text": "",
            "candidates": [
                "Precisión promedio media",
                "Precisión promedio media",
                "Precisión promedio media",
                "precisión promedio media"
            ],
            "error": []
        },
        "query history information": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more <br>query history information</br>, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the <br>query history information</br> and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como es de esperar, un µ excesivamente grande dañaría el rendimiento en general, pero el Q2 se duele más y el Q4 apenas se duele, lo que indica que a medida que acumulamos cada vez más \"información sobre el historial de consultas\", podemos poner más y más peso en elInformación de la historia.",
                "Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0312 0.1150 Q2 + HC 0.0313 0.0950 Mejora.0.3% -17.4% Q3 0.0421 0.1483 Q3 + HC 0.0521 0.1820 Mejorar 23.8% 23.0% Q4 0.0536 0.1930 Q4 + HC 0.0620 0.1850 Mejora 15.7% -4.1% Tabla 6: Efecto de uso solo de datos de Clickhrough no relevantes 5.2.2Información de contexto Al comparar los resultados en la Tabla 1, la Tabla 2 y la Tabla 4, podemos ver que el beneficio de la \"información del historial de consultas\" y el de la información de clics son en su mayoría aditivos, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno solo, pero la mayoría de la mejora claramente proviene de la información de clics."
            ],
            "translated_text": "",
            "candidates": [
                "Información del historial de consultas",
                "información sobre el historial de consultas",
                "Información del historial de consultas",
                "información del historial de consultas"
            ],
            "error": []
        },
        "current query": {
            "translated_key": "consulta actual",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the <br>current query</br> for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the <br>current query</br> is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the <br>current query</br> and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the <br>current query</br> Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the <br>current query</br> Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the <br>current query</br> Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the <br>current query</br> Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the <br>current query</br> be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the <br>current query</br> Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the <br>current query</br> model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the <br>current query</br> and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our <br>current query</br> Qk is very long, we should trust the <br>current query</br> more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our <br>current query</br> model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a <br>current query</br> model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the <br>current query</br> Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the <br>current query</br>.",
                "So most of our experiments involve comparing the retrieval performance using the <br>current query</br> only (thus ignoring any context) with that using the <br>current query</br> as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the <br>current query</br> only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the <br>current query</br>.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the <br>current query</br>.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the <br>current query</br>.",
                "Except for q2, there is clearly some tradeoff between the <br>current query</br> and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the <br>current query</br>.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the <br>current query</br> is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the <br>current query</br> is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Proponemos varios algoritmos de recuperación sensibles al contexto basados en modelos de lenguaje estadístico para combinar las consultas anteriores y hacer clic en resúmenes de documentos con la \"consulta actual\" para una mejor clasificación de documentos.",
                "Por ejemplo, si la \"consulta actual\" es Java, sin conocer ninguna información adicional, sería imposible saber si está destinado a significar el lenguaje de programación de Java o la isla Java en Indonesia.",
                "Utilizamos el modelo de recuperación de divergencia KL [19] como base y proponemos tratar la recuperación sensible al contexto como estimación de un modelo de lenguaje de consulta basado en la \"consulta actual\" y cualquier información del contexto de búsqueda.",
                "Por lo tanto, para la \"consulta actual\" QK (a excepción de la primera consulta de una sesión de búsqueda), hay un historial de consulta, HQ = (Q1, ..., QK - 1) asociado con ella, que consiste en las consultas anteriores dadaspor el mismo usuario en la sesión actual.",
                "Tradicionalmente, el sistema de recuperación solo utiliza la \"consulta actual\" QK para hacer la recuperación.",
                "También explotaremos dicho historial de clics HC = (C1, ..., CK - 1) para mejorar nuestra precisión de búsqueda para la \"consulta actual\" QK.",
                "Los modelos de lenguaje para contextenciales de información sensible a Reval intuitivamente, el historial de consultas HQ y el historial de clics HC son útiles para mejorar la precisión de búsqueda para la \"consulta actual\" QK.",
                "Formalmente, deje que HQ = (Q1, ..., Qk - 1) sea el historial de consulta y la \"consulta actual\" sea qk.",
                "Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos mediante P (w | θk), basado en la \"consulta actual\" QK, así como en el historial de consultas y el historial de clics HC.",
                "Finalmente, interpolamos el modelo de historia P (W | H) con el modelo P (W | QK) \"W | QK).",
                "Estos modelos se definen de la siguiente manera.p (w | qi) = c (w, qi) | qi |p (w | hq) = 1 k - 1 i = k - 1 i = 1 p (w | qi) p (w | ci) = c (w, ci) | ci |p (w | hc) = 1 k - 1 i = k - 1 i = 1 p (w | ci) p (w | h) = βp (w | hc) + (1 - β) p (w | hq)p (w | θk) = αp (w | qk) + (1 - α) p (w | h) donde β ∈ [0, 1] es un parámetro para controlar el peso en cada modelo de historia, y donde α ∈ [0, 1] es un parámetro para controlar el peso en la \"consulta actual\" y la información del historial.",
                "Pero intuitivamente, si nuestra \"consulta actual\" QK es muy larga, deberíamos confiar más en la \"consulta actual\", mientras que si QK solo tiene una palabra, puede ser beneficioso poner más peso en la historia.",
                "Sea P (W | φ) nuestro modelo de \"consulta actual\" y t sea una nueva evidencia de texto observada (por ejemplo, t puede ser una consulta o un resumen de clic).",
                "En general, suponemos que el sistema de recuperación mantiene un modelo de \"consulta actual\" φi en cualquier momento.",
                "Y para clasificar los documentos después de ver la \"consulta actual\" QK, usamos P (W | θk) = P (W | ψk) 4.",
                "En particular, el contexto de búsqueda puede proporcionar información adicional para ayudarnos a estimar un mejor modelo de consulta que usar solo la \"consulta actual\".",
                "Por lo tanto, la mayoría de nuestros experimentos implican comparar el rendimiento de la recuperación utilizando solo la \"consulta actual\" (ignorando así cualquier contexto) con eso utilizando la \"consulta actual\", así como el contexto de búsqueda.",
                "También variamos los parámetros para estudiar la sensibilidad de nuestros algoritmos a la configuración de los parámetros.5.2 Análisis de resultados 5.2.1 Efecto general del contexto de búsqueda Comparamos el rendimiento óptimo de cuatro modelos con aquellos que usan la \"consulta actual\" solo en la Tabla 1.",
                "Los resultados del historial de consultas mixtas sugieren que el efecto positivo del uso de la información de retroalimentación implícita puede provenir en gran medida del uso del historial de clics, lo cual es cierto como discutimos en la próxima subsección.5.2.3 Usando el historial de clics solo ahora apagamos el historial de consultas y solo usamos los resúmenes haciendo clic más la \"consulta actual\".",
                "Cuando µ se establece en 0, el historial de consultas no se usa en absoluto, y esencialmente usamos los datos de clic combinados con la \"consulta actual\".",
                "El mejor rendimiento generalmente se logra cuando µ es alrededor de 2.0, lo que significa que la información de consulta pasada es tan útil como aproximadamente 2 palabras en la \"consulta actual\".",
                "Excepto por Q2, claramente hay alguna compensación entre la \"consulta actual\" y el mapa de consultas de consultas anteriores PR@20docs Q2 0.0312 0.1150 Q2 + HQ 0.0287 0.0967 Mejorar.-8.0% -15.9% Q2 + HC 0.0344 0.1167 Mejora.10.3% 1.5% Q2 + HQ + HC 0.0342 0.1100 Mejorar.9.6% -4.3% Q3 0.0421 0.1483 Q3 + HQ 0.0455 0.1450 Mejorar 8.1% -2.2% Q3 + HC 0.0513 0.1650 Mejorar 21.9% 11.3% Q3 + HQ + HC 0.0810 0.2067 Mejorar 92.4% 39.4% Q4 0.0536 0.1930 Q4 + HQ + HQ + HQ + HQ3.0% -0.8% Q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% Q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: beneficio aditivo de la información de contexto y el uso de una combinación equilibrada de ellos logran un mejor rendimiento que usar cada uno de ellossolo.",
                "Cuando ν está configurado en 0, solo usamos los datos de clics;Cuando ν se establece en +∞, solo usamos el historial de consulta y la \"consulta actual\".",
                "Esto significa que la información combinada del historial de consultas y la \"consulta actual\" es tan útil como aproximadamente 15 palabras en los datos de clics, lo que indica que la información de clics es muy valiosa.",
                "Por ejemplo, podemos tratar un resumen hecho de manera diferente dependiendo de si la \"consulta actual\" es una generalización o refinamiento de la consulta anterior."
            ],
            "translated_text": "",
            "candidates": [
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "Consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "Consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual",
                "consulta actual"
            ],
            "error": []
        },
        "clickthrough information": {
            "translated_key": "Información de clics",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and <br>clickthrough information</br>, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the <br>clickthrough information</br> and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting <br>clickthrough information</br> were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using <br>clickthrough information</br>, in this paper, we use both <br>clickthrough information</br> and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user <br>clickthrough information</br>; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar <br>clickthrough information</br> [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the <br>clickthrough information</br>, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and <br>clickthrough information</br>) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using <br>clickthrough information</br> is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of <br>clickthrough information</br> are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the <br>clickthrough information</br>.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the <br>clickthrough information</br> is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este documento, estudiamos cómo explotar la información de retroalimentación implícita, incluidas consultas anteriores e \"información de clics\", para mejorar la precisión de la recuperación en un entorno de recuperación de información interactiva.",
                "En [11], Joachims exploró cómo capturar y explotar la \"información de clics\" y demostró que dicha información de retroalimentación implícita puede mejorar la precisión de búsqueda para un grupo de personas.",
                "En [18], se realizó un estudio de simulación de la efectividad de diferentes algoritmos de retroalimentación implícitos, y se propusieron y evaluaron varios modelos de recuperación diseñados para explotar \"información de clics\".",
                "Si bien el trabajo anterior se ha centrado principalmente en usar \"información de clics\", en este documento, utilizamos tanto \"información de clics\" como consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperación.",
                "El otro tipo de contexto es el contexto a largo plazo, que se refiere a información como un nivel educativo de usuarios e interés general, el historial de consultas de usuarios acumulados y la \"información de clics\" del usuario anterior;Dicha información es generalmente estable durante mucho tiempo y a menudo se acumula con el tiempo.",
                "El trabajo anterior también ha mostrado resultados positivos utilizando \"información de clics\" similar [11, 17].",
                "Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario se vuelve cada vez mejor en la formulación de la consulta a medida que pasa el tiempo, pero no es necesariamente apropiado para la \"información de clics\", especialmente porque usamos el resumen mostrado,en lugar del contenido real de un documento haciendo clic.",
                "Experimentos 5.1 Diseño del experimento Nuestra hipótesis principal es que el uso del contexto de búsqueda (es decir, el historial de consultas y la \"información de clics\") puede ayudar a mejorar la precisión de la búsqueda.",
                "Vemos que el beneficio de usar \"información de clics\" es mucho más significativo que el de usar el historial de consultas.",
                "Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0312 0.1150 Q2 + HC 0.0313 0.0950 Mejora.0.3% -17.4% Q3 0.0421 0.1483 Q3 + HC 0.0521 0.1820 Mejorar 23.8% 23.0% Q4 0.0536 0.1930 Q4 + HC 0.0620 0.1850 Mejora 15.7% -4.1% Tabla 6: Efecto de uso solo de datos de Clickhrough no relevantes 5.2.2Información de contexto Al comparar los resultados en la Tabla 1, la Tabla 2 y la Tabla 4, podemos ver que el beneficio de la información del historial de consultas y el de la \"información de clics\" son principalmente aditivas, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno solo, pero la mayoría de la mejora claramente proviene de la \"información de clics\".",
                "Esto significa que la información combinada del historial de consultas y la consulta actual es tan útil como unas 15 palabras en los datos de clics, lo que indica que la \"información de clics\" es muy valiosa."
            ],
            "translated_text": "",
            "candidates": [
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "Información de clics",
                "información de clics",
                "información de clics",
                "Información de clics",
                "información de clics"
            ],
            "error": []
        },
        "query history": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user <br>query history</br> and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a <br>query history</br>, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term <br>query history</br> clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term <br>query history</br> is useful for improving retrieval accuracy.",
                "In addition to the <br>query history</br>, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both <br>query history</br> and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the <br>query history</br> and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the <br>query history</br> HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the <br>query history</br> and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the <br>query history</br> HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the <br>query history</br> HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse <br>query history</br>, thus we could use a smaller µi, but later as the <br>query history</br> is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the <br>query history</br> as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the <br>query history</br> Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also <br>query history</br> and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated <br>query history</br> and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no <br>query history</br> and clickthrough history data.",
                "We decide to augment a TREC data set by collecting <br>query history</br> and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer <br>query history</br> and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect <br>query history</br> and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using <br>query history</br> and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., <br>query history</br> and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - <br>query history</br> and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using <br>query history</br> only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using <br>query history</br> alone.",
                "We use the same parameter setting for <br>query history</br> as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using <br>query history</br> is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using <br>query history</br> is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using <br>query history</br> only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using <br>query history</br> only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using <br>query history</br> only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the <br>query history</br> is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more <br>query history</br> information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed <br>query history</br> results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the <br>query history</br> and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using <br>query history</br>.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the <br>query history</br> information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the <br>query history</br> is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the <br>query history</br> and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of <br>query history</br> and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including <br>query history</br> and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit <br>query history</br> and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting <br>query history</br> for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El otro tipo de contexto es el contexto a largo plazo, que se refiere a información como un nivel educativo de usuarios e interés general, el \"historial de consultas\" del usuario acumulado e información de clics de los usuarios anteriores;Dicha información es generalmente estable durante mucho tiempo y a menudo se acumula con el tiempo.",
                "Por lo tanto, para la consulta actual QK (a excepción de la primera consulta de una sesión de búsqueda), hay un \"historial de consultas\", HQ = (Q1, ..., QK - 1) asociado con ella, que consiste en las consultas anteriores dadaspor el mismo usuario en la sesión actual.",
                "Pero el \"historial de consultas\" a corto plazo claramente puede proporcionar pistas útiles sobre la necesidad actual de información de los usuarios como se ve en el ejemplo de Java dado en la sección anterior.",
                "De hecho, nuestro trabajo anterior [15] ha demostrado que el \"historial de consultas\" a corto plazo es útil para mejorar la precisión de la recuperación.",
                "Además del \"historial de consultas\", puede haber otra información de contexto a corto plazo disponible.",
                "Tanto el \"historial de consultas\" como el historial de clics son información de retroalimentación implícita, que naturalmente existe en la recuperación de información interactiva, por lo tanto, no se necesita ningún esfuerzo adicional para el usuario para recopilarlos.",
                "En este documento, estudiamos cómo explotar dicha información (HQ y HC), desarrollamos modelos para incorporar el \"historial de consultas\" y hacer clic en una función de clasificación de recuperación y evaluar cuantitativamente estos modelos.3.",
                "Los modelos de lenguaje para contextensidad de información insensible intuitivamente, el HQ \"Historial de consultas\" y el Historio de Clickthrough HC son útiles para mejorar la precisión de búsqueda para la consulta actual QK.",
                "Formalmente, deje que HQ = (Q1, ..., Qk - 1) sea el \"Historial de consulta\" y la consulta actual sea QK.",
                "Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos mediante P (w | θk), basado en la consulta actual QK, así como en la sede de \"Historial de consultas\" e Historio HC.",
                "Usaremos | x |para denotar la longitud del texto x o el número total de palabras en X. 3.2 Interpolación de coeficiente fijo (Fixint) Nuestra primera idea es resumir la sede del \"historial de consultas\" con un modelo de lenguaje unigram P (W | HQ) y la historia de clics de ClickhroughHC con otro modelo de idioma unigram P (W | HC).",
                "Por ejemplo, al principio, podemos tener un \"historial de consultas\" muy escaso, por lo que podríamos usar un µI más pequeño, pero más tarde como el \"historial de consultas\" es más rico, podemos considerar usar un µI más grande.",
                "Esto puede ser apropiado para el \"historial de consultas\", ya que es razonable creer que el usuario se vuelve cada vez mejor en la formulación de la consulta a medida que pasa el tiempo, pero no es necesariamente apropiado para la información de clics, especialmente porque usamos el resumen mostrado,en lugar del contenido real de un documento haciendo clic.",
                "Una forma de evitar aplicar una interpolación en descomposición a los datos de clics es hacer en línea solo para el \"historial de consultas\" q = (Q1, ..., qi - 1), pero no para los datos de clics C. Primero amortamos todo el amortiguador de todos los amortiguación de todos los primeros.Haga clic a través de datos juntos y use toda la parte de los datos de clic para actualizar el modelo generado mediante la ejecución de OnlineUP en consultas anteriores.",
                "Recopilación de datos Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino también \"historial de consultas\" e historial de clic para cada tema.",
                "Una es extraer temas y cualquier \"historial de consultas\" asociado e historial de clics para cada tema del registro de un sistema de recuperación (por ejemplo, motor de búsqueda).",
                "Desafortunadamente, no hay \"historial de consultas\" y datos de historial de clics.",
                "Decidimos aumentar un conjunto de datos TREC recopilando \"Historial de consultas\" y los datos del historial de clics.",
                "La razón por la que seleccionamos temas difíciles es que el usuario tendría que tener varias interacciones con el sistema de recuperación para obtener resultados satisfactorios para que podamos esperar recopilar un \"historial de consultas\" relativamente más rico y hacer clic en los datos del historial del usuario.",
                "Utilizamos 3 sujetos para hacer experimentos para recopilar el \"historial de consultas\" y los datos del historial de clics.",
                "La longitud promedio de la consulta de resumen de resumen en línea en línea de la consulta de lotes (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) Map@20docs mapa pr@20docs mapa pr@20 docs map pr@20docs Q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + hc 0.0311117 0.031110 0.03110 0.031110 0.031110 0.031141110 0.031110 0.031110 0.0311411141110 0.03114111111 215 0.0733 0.0342 0.1100 Mejorar.3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8%39.4% 67.7% 20.2% 92.4% 39.4% Q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 Q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250. 9% 47.8% 6.9% 77.2% 16.4% Tabla1: Efecto del uso del \"historial de consultas\" y los datos de clic para la clasificación de documentos.son 34.4 palabras.",
                "Experimentos 5.1 Diseño del experimento Nuestra hipótesis principal es que el uso del contexto de búsqueda (es decir, \"historial de consultas\" e información de clics) puede ayudar a mejorar la precisión de búsqueda.",
                "Tenemos dos tipos diferentes de contexto de búsqueda: \"historial de consultas\" y datos de clics.",
                "Ahora analizamos la contribución de cada tipo de contexto.5.2.2 Utilizando el \"historial de consultas\" solo en cada uno de los cuatro modelos, podemos desactivar los datos del historial de clics configurando los parámetros adecuadamente.",
                "Esto nos permite evaluar el efecto de usar \"historial de consultas\" solo.",
                "Utilizamos la misma configuración de parámetros para el \"historial de consultas\" que en la Tabla 1.",
                "Aquí vemos que, en general, el beneficio de usar \"historial de consultas\" es muy limitado con resultados mixtos.",
                "Esto es diferente de lo que se informa en un estudio anterior [15], donde el uso del \"historial de consultas\" es consistentemente útil.",
                "Otra observación más es que cuando se usa solo \"historial de consultas\", el modelo BayesInt parece ser mejor que otros modelos.",
                "Dado que se ignoran los datos de clics, en línea y la consulta de Batchint Batchint Bayesint en línea (α = 0.1, β = 0) (µ = 0.2, ν = 0) (µ = 5.0, ν = +∞) (µ = 2.0, ν = =+ ∞) Map PR@20Docs Map PR@20Docs Map PR@20Docs Map PR@20Docs Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 Q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 mejora.-68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2%7.1% 2.3% 5.5% -10.1% 8.1% -2.2% Q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 Q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 mejoran % 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto del uso del \"historial de consultas\" solo para la clasificación de documentos.µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.05520.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: Precisión promedio de Batchup usando \"Historial de consultas\" Solo son esencialmente el mismo algoritmo.",
                "El valor de µ puede interpretarse como cuántas palabras consideramos el \"historial de consultas\".",
                "Como es de esperar, un µ excesivamente grande dañaría el rendimiento en general, pero el Q2 se duele más y el Q4 apenas se duele, lo que indica que a medida que acumulamos más y más información de \"historia de consultas\", podemos poner más y más peso en elInformación de la historia.",
                "Los resultados mixtos de \"historial de consultas\" sugieren que el efecto positivo del uso de la información de retroalimentación implícita puede provenir en gran medida del uso del historial de clics, lo cual es cierto como discutimos en la próxima subsección.5.2.3 Usando el historial de clics solo ahora apagamos el \"historial de consultas\" y solo usamos los resúmenes haciendo clic más la consulta actual.",
                "Vemos que el beneficio de usar información de clics es mucho más significativo que el de usar \"historial de consultas\".",
                "Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0312 0.1150 Q2 + HC 0.0313 0.0950 Mejora.0.3% -17.4% Q3 0.0421 0.1483 Q3 + HC 0.0521 0.1820 Mejorar 23.8% 23.0% Q4 0.0536 0.1930 Q4 + HC 0.0620 0.1850 Mejora 15.7% -4.1% Tabla 6: Efecto de uso solo de datos de Clickhrough no relevantes 5.2.2Información de contexto Al comparar los resultados en la Tabla 1, la Tabla 2 y la Tabla 4, podemos ver que el beneficio de la información del \"historial de consultas\" y el de la información de clics son en su mayoría aditivos, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno solo, pero la mayoría de la mejora claramente proviene de la información de clics.",
                "Cuando µ se establece en 0, el \"historial de consultas\" no se usa en absoluto, y esencialmente usamos los datos de clic combinados con la consulta actual.",
                "Cuando ν está configurado en 0, solo usamos los datos de clics;Cuando ν se establece en +∞, solo usamos el \"historial de consultas\" y la consulta actual.",
                "Esto significa que la información combinada del \"historial de consultas\" y la consulta actual es tan útil como aproximadamente 15 palabras en los datos de clics, lo que indica que la información de clics es muy valiosa.",
                "Conclusiones y trabajo futuro En este documento, hemos explorado cómo explotar la información de retroalimentación implícita, incluido el \"historial de consultas\" y el historial de clics dentro de la misma sesión de búsqueda, para mejorar el rendimiento de la recuperación de la información.",
                "Sería interesante desarrollar modelos más sofisticados para explotar mejor el \"historial de consultas\" y el historial de clics.",
                "Explotación del \"historial de consultas\" para la clasificación de documentos en la recuperación de información interactiva (póster)."
            ],
            "translated_text": "",
            "candidates": [
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "Historial de consultas",
                "historial de consultas",
                "historia de consulta",
                "Historial de consultas",
                "historia de consulta",
                "Historial de consulta",
                "historia de consulta",
                "Historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "Historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "Historial de consultas",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "Historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historia de consultas",
                "historia de consulta",
                "historial de consultas",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas",
                "historia de consulta",
                "historial de consultas"
            ],
            "error": []
        },
        "query expansion": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Context-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional context information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, context-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of context that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more context of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using context includes personalized search [1, 3, 4, 7, 10], query log analysis [5], context factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new context-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat context-sensitive retrieval as estimating a query language model based on the current query and any search context information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of context information we can use for implicit feedback.",
                "One is short-term context, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term context.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term context is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of context is long-term context, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term context can be applicable to all sessions, but may not be as effective as the short-term context in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term context, though some of our methods can also be used to naturally incorporate some long-term context.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term context information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific context-sensitive language models to incorporate context information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search context as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a context query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated context query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a context query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the context query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our context query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search context (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search context can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any context) with that using the current query as well as the search context.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search context (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search context We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search context.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search context generally has positive effect, especially when the context is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the context may hurt the performance, probably because the history at that point is sparse.",
                "When the search context is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search context, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search context - query history and clickthrough data.",
                "We now look into the contribution of each kind of context. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the context runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the context data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of context information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of context information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for context-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the context-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search context to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic <br>query expansion</br> using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in context: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"expansión de consulta\" probabilística utilizando registros de consulta."
            ],
            "translated_text": "",
            "candidates": [
                "expansión de la consulta",
                "expansión de consulta"
            ],
            "error": []
        },
        "context": {
            "translated_key": "contexto",
            "is_in_text": true,
            "original_annotated_sentences": [
                "<br>context</br>-Sensitive Information Retrieval Using Implicit Feedback Xuehua Shen Department of Computer Science University of Illinois at Urbana-Champaign Bin Tan Department of Computer Science University of Illinois at Urbana-Champaign ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign ABSTRACT A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search <br>context</br> is largely ignored.",
                "In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting.",
                "We propose several contextsensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents.",
                "We use the TREC AP data to create a test collection with search <br>context</br> information, and quantitatively evaluate our models using this test set.",
                "Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval models General Terms Algorithms 1.",
                "INTRODUCTION In most existing information retrieval models, the retrieval problem is treated as involving one single query and a set of documents.",
                "From a single query, however, the retrieval system can only have very limited clue about the users information need.",
                "An optimal retrieval system thus should try to exploit as much additional <br>context</br> information as possible to improve retrieval accuracy, whenever it is available.",
                "Indeed, <br>context</br>-sensitive retrieval has been identified as a major challenge in information retrieval research[2].",
                "There are many kinds of <br>context</br> that we can exploit.",
                "Relevance feedback [14] can be considered as a way for a user to provide more <br>context</br> of search and is known to be effective for improving retrieval accuracy.",
                "However, relevance feedback requires that a user explicitly provides feedback information, such as specifying the category of the information need or marking a subset of retrieved documents as relevant documents.",
                "Since it forces the user to engage additional activities while the benefits are not always obvious to the user, a user is often reluctant to provide such feedback information.",
                "Thus the effectiveness of relevance feedback may be limited in real applications.",
                "For this reason, implicit feedback has attracted much attention recently [11, 13, 18, 17, 12].",
                "In general, the retrieval results using the users initial query may not be satisfactory; often, the user would need to revise the query to improve the retrieval/ranking accuracy [8].",
                "For a complex or difficult information need, the user may need to modify his/her query and view ranked documents with many iterations before the information need is completely satisfied.",
                "In such an interactive retrieval scenario, the information naturally available to the retrieval system is more than just the current user query and the document collection - in general, all the interaction history can be available to the retrieval system, including past queries, information about which documents the user has chosen to view, and even how a user has read a document (e.g., which part of a document the user spends a lot of time in reading).",
                "We define implicit feedback broadly as exploiting all such naturally available interaction history to improve retrieval results.",
                "A major advantage of implicit feedback is that we can improve the retrieval accuracy without requiring any user effort.",
                "For example, if the current query is java, without knowing any extra information, it would be impossible to know whether it is intended to mean the Java programming language or the Java island in Indonesia.",
                "As a result, the retrieved documents will likely have both kinds of documents - some may be about the programming language and some may be about the island.",
                "However, any particular user is unlikely searching for both types of documents.",
                "Such an ambiguity can be resolved by exploiting history information.",
                "For example, if we know that the previous query from the user is cgi programming, it would strongly suggest that it is the programming language that the user is searching for.",
                "Implicit feedback was studied in several previous works.",
                "In [11], Joachims explored how to capture and exploit the clickthrough information and demonstrated that such implicit feedback information can indeed improve the search accuracy for a group of people.",
                "In [18], a simulation study of the effectiveness of different implicit feedback algorithms was conducted, and several retrieval models designed for exploiting clickthrough information were proposed and evaluated.",
                "In [17], some existing retrieval algorithms are adapted to improve search results based on the browsing history of a user.",
                "Other related work on using <br>context</br> includes personalized search [1, 3, 4, 7, 10], query log analysis [5], <br>context</br> factors [12], and implicit queries [6].",
                "While the previous work has mostly focused on using clickthrough information, in this paper, we use both clickthrough information and preceding queries, and focus on developing new <br>context</br>-sensitive language models for retrieval.",
                "Specifically, we develop models for using implicit feedback information such as query and clickthrough history of the current search session to improve retrieval accuracy.",
                "We use the KL-divergence retrieval model [19] as the basis and propose to treat <br>context</br>-sensitive retrieval as estimating a query language model based on the current query and any search <br>context</br> information.",
                "We propose several statistical language models to incorporate query and clickthrough history into the KL-divergence model.",
                "One challenge in studying implicit feedback models is that there does not exist any suitable test collection for evaluation.",
                "We thus use the TREC AP data to create a test collection with implicit feedback information, which can be used to quantitatively evaluate implicit feedback models.",
                "To the best of our knowledge, this is the first test set for implicit feedback.",
                "We evaluate the proposed models using this data set.",
                "The experimental results show that using implicit feedback information, especially the clickthrough data, can substantially improve retrieval performance without requiring additional effort from the user.",
                "The remaining sections are organized as follows.",
                "In Section 2, we attempt to define the problem of implicit feedback and introduce some terms that we will use later.",
                "In Section 3, we propose several implicit feedback models based on statistical language models.",
                "In Section 4, we describe how we create the data set for implicit feedback experiments.",
                "In Section 5, we evaluate different implicit feedback models on the created data set.",
                "Section 6 is our conclusions and future work. 2.",
                "PROBLEM DEFINITION There are two kinds of <br>context</br> information we can use for implicit feedback.",
                "One is short-term <br>context</br>, which is the immediate surrounding information which throws light on a users current information need in a single session.",
                "A session can be considered as a period consisting of all interactions for the same information need.",
                "The category of a users information need (e.g., kids or sports), previous queries, and recently viewed documents are all examples of short-term <br>context</br>.",
                "Such information is most directly related to the current information need of the user and thus can be expected to be most useful for improving the current search.",
                "In general, short-term <br>context</br> is most useful for improving search in the current session, but may not be so helpful for search activities in a different session.",
                "The other kind of <br>context</br> is long-term <br>context</br>, which refers to information such as a users education level and general interest, accumulated user query history and past user clickthrough information; such information is generally stable for a long time and is often accumulated over time.",
                "Long-term <br>context</br> can be applicable to all sessions, but may not be as effective as the short-term <br>context</br> in improving search accuracy for a particular session.",
                "In this paper, we focus on the short-term <br>context</br>, though some of our methods can also be used to naturally incorporate some long-term <br>context</br>.",
                "In a single search session, a user may interact with the search system several times.",
                "During interactions, the user would continuously modify the query.",
                "Therefore for the current query Qk (except for the first query of a search session) , there is a query history, HQ = (Q1, ..., Qk−1) associated with it, which consists of the preceding queries given by the same user in the current session.",
                "Note that we assume that the session boundaries are known in this paper.",
                "In practice, we need techniques to automatically discover session boundaries, which have been studied in [9, 16].",
                "Traditionally, the retrieval system only uses the current query Qk to do retrieval.",
                "But the short-term query history clearly may provide useful clues about the users current information need as seen in the java example given in the previous section.",
                "Indeed, our previous work [15] has shown that the short-term query history is useful for improving retrieval accuracy.",
                "In addition to the query history, there may be other short-term <br>context</br> information available.",
                "For example, a user would presumably frequently click some documents to view.",
                "We refer to data associated with these actions as clickthrough history.",
                "The clickthrough data may include the title, summary, and perhaps also the content and location (e.g., the URL) of the clicked document.",
                "Although it is not clear whether a viewed document is actually relevant to the users information need, we may safely assume that the displayed summary/title information about the document is attractive to the user, thus conveys information about the users information need.",
                "Suppose we concatenate all the displayed text information about a document (usually title and summary) together, we will also have a clicked summary Ci in each round of retrieval.",
                "In general, we may have a history of clicked summaries C1, ..., Ck−1.",
                "We will also exploit such clickthrough history HC = (C1, ..., Ck−1) to improve our search accuracy for the current query Qk.",
                "Previous work has also shown positive results using similar clickthrough information [11, 17].",
                "Both query history and clickthrough history are implicit feedback information, which naturally exists in interactive information retrieval, thus no additional user effort is needed to collect them.",
                "In this paper, we study how to exploit such information (HQ and HC ), develop models to incorporate the query history and clickthrough history into a retrieval ranking function, and quantitatively evaluate these models. 3.",
                "LANGUAGE MODELS FOR CONTEXTSENSITIVEINFORMATIONRETRIEVAL Intuitively, the query history HQ and clickthrough history HC are both useful for improving search accuracy for the current query Qk.",
                "An important research question is how we can exploit such information effectively.",
                "We propose to use statistical language models to model a users information need and develop four specific <br>context</br>-sensitive language models to incorporate <br>context</br> information into a basic retrieval model. 3.1 Basic retrieval model We use the Kullback-Leibler (KL) divergence method [19] as our basic retrieval method.",
                "According to this model, the retrieval task involves computing a query language model θQ for a given query and a document language model θD for a document and then computing their KL divergence D(θQ||θD), which serves as the score of the document.",
                "One advantage of this approach is that we can naturally incorporate the search <br>context</br> as additional evidence to improve our estimate of the query language model.",
                "Formally, let HQ = (Q1, ..., Qk−1) be the query history and the current query be Qk.",
                "Let HC = (C1, ..., Ck−1) be the clickthrough history.",
                "Note that Ci is the concatenation of all clicked documents summaries in the i-th round of retrieval since we may reasonably treat all these summaries equally.",
                "Our task is to estimate a <br>context</br> query model, which we denote by p(w|θk), based on the current query Qk, as well as the query history HQ and clickthrough history HC .",
                "We now describe several different language models for exploiting HQ and HC to estimate p(w|θk).",
                "We will use c(w, X) to denote the count of word w in text X, which could be either a query or a clicked documents summary or any other text.",
                "We will use |X| to denote the length of text X or the total number of words in X. 3.2 Fixed Coefficient Interpolation (FixInt) Our first idea is to summarize the query history HQ with a unigram language model p(w|HQ) and the clickthrough history HC with another unigram language model p(w|HC ).",
                "Then we linearly interpolate these two history models to obtain the history model p(w|H).",
                "Finally, we interpolate the history model p(w|H) with the current query model p(w|Qk).",
                "These models are defined as follows. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) where β ∈ [0, 1] is a parameter to control the weight on each history model, and where α ∈ [0, 1] is a parameter to control the weight on the current query and the history information.",
                "If we combine these equations, we see that p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)] That is, the estimated <br>context</br> query model is just a fixed coefficient interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ). 3.3 Bayesian Interpolation (BayesInt) One possible problem with the FixInt approach is that the coefficients, especially α, are fixed across all the queries.",
                "But intuitively, if our current query Qk is very long, we should trust the current query more, whereas if Qk has just one word, it may be beneficial to put more weight on the history.",
                "To capture this intuition, we treat p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed data to estimate a <br>context</br> query model using Bayesian estimator.",
                "The estimated model is given by p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] where µ is the prior sample size for p(w|HQ) and ν is the prior sample size for p(w|HC ).",
                "We see that the only difference between BayesInt and FixInt is the interpolation coefficients are now adaptive to the query length.",
                "Indeed, when viewing BayesInt as FixInt, we see that α = |Qk| |Qk|+µ+ν , β = ν ν+µ , thus with fixed µ and ν, we will have a query-dependent α.",
                "Later we will show that such an adaptive α empirically performs better than a fixed α. 3.4 Online Bayesian Updating (OnlineUp) Both FixInt and BayesInt summarize the history information by averaging the unigram language models estimated based on previous queries or clicked summaries.",
                "This means that all previous queries are treated equally and so are all clicked summaries.",
                "However, as the user interacts with the system and acquires more knowledge about the information in the collection, presumably, the reformulated queries will become better and better.",
                "Thus assigning decaying weights to the previous queries so as to trust a recent query more than an earlier query appears to be reasonable.",
                "Interestingly, if we incrementally update our belief about the users information need after seeing each query, we could naturally obtain decaying weights on the previous queries.",
                "Since such an incremental online updating strategy can be used to exploit any evidence in an interactive retrieval system, we present it in a more general way.",
                "In a typical retrieval system, the retrieval system responds to every new query entered by the user by presenting a ranked list of documents.",
                "In order to rank documents, the system must have some model for the users information need.",
                "In the KL divergence retrieval model, this means that the system must compute a query model whenever a user enters a (new) query.",
                "A principled way of updating the query model is to use Bayesian estimation, which we discuss below. 3.4.1 Bayesian updating We first discuss how we apply Bayesian estimation to update a query model in general.",
                "Let p(w|φ) be our current query model and T be a new piece of text evidence observed (e.g., T can be a query or a clicked summary).",
                "To update the query model based on T, we use φ to define a Dirichlet prior parameterized as Dir(µT p(w1|φ), ..., µT p(wN |φ)) where µT is the equivalent sample size of the prior.",
                "We use Dirichlet prior because it is a conjugate prior for multinomial distributions.",
                "With such a conjugate prior, the predictive distribution of φ (or equivalently, the mean of the posterior distribution of φ is given by p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) where c(w, T) is the count of w in T and |T| is the length of T. Parameter µT indicates our confidence in the prior expressed in terms of an equivalent text sample comparable with T. For example, µT = 1 indicates that the influence of the prior is equivalent to adding one extra word to T. 3.4.2 Sequential query model updating We now discuss how we can update our query model over time during an interactive retrieval process using Bayesian estimation.",
                "In general, we assume that the retrieval system maintains a current query model φi at any moment.",
                "As soon as we obtain some implicit feedback evidence in the form of a piece of text Ti, we will update the query model.",
                "Initially, before we see any user query, we may already have some information about the user.",
                "For example, we may have some information about what documents the user has viewed in the past.",
                "We use such information to define a prior on the query model, which is denoted by φ0.",
                "After we observe the first query Q1, we can update the query model based on the new observed data Q1.",
                "The updated query model φ1 can then be used for ranking documents in response to Q1.",
                "As the user views some documents, the displayed summary text for such documents C1 (i.e., clicked summaries) can serve as some new data for us to further update the query model to obtain φ1.",
                "As we obtain the second query Q2 from the user, we can update φ1 to obtain a new model φ2.",
                "In general, we may repeat such an updating process to iteratively update the query model.",
                "Clearly, we see two types of updating: (1) updating based on a new query Qi; (2) updating based on a new clicked summary Ci.",
                "In both cases, we can treat the current model as a prior of the <br>context</br> query model and treat the new observed query or clicked summary as observed data.",
                "Thus we have the following updating equations: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi where µi is the equivalent sample size for the prior when updating the model based on a query, while νi is the equivalent sample size for the prior when updating the model based on a clicked summary.",
                "If we set µi = 0 (or νi = 0) we essentially ignore the prior model, thus would start a completely new query model based on the query Qi (or the clicked summary Ci).",
                "On the other hand, if we set µi = +∞ (or νi = +∞) we essentially ignore the observed query (or the clicked summary) and do not update our model.",
                "Thus the model remains the same as if we do not observe any new text evidence.",
                "In general, the parameters µi and νi may have different values for different i.",
                "For example, at the very beginning, we may have very sparse query history, thus we could use a smaller µi, but later as the query history is richer, we can consider using a larger µi.",
                "But in our experiments, unless otherwise stated, we set them to the same constants, i.e., ∀i, j, µi = µj, νi = νj.",
                "Note that we can take either p(w|φi) or p(w|φi) as our <br>context</br> query model for ranking documents.",
                "This suggests that we do not have to wait until a user enters a new query to initiate a new round of retrieval; instead, as soon as we collect clicked summary Ci, we can update the query model and use p(w|φi) to immediately rerank any documents that a user has not yet seen.",
                "To score documents after seeing query Qk, we use p(w|φk), i.e., p(w|θk) = p(w|φk) 3.5 Batch Bayesian updating (BatchUp) If we set the equivalent sample size parameters to fixed constant, the OnlineUp algorithm would introduce a decaying factor - repeated interpolation would cause the early data to have a low weight.",
                "This may be appropriate for the query history as it is reasonable to believe that the user becomes better and better at query formulation as time goes on, but it is not necessarily appropriate for the clickthrough information, especially because we use the displayed summary, rather than the actual content of a clicked document.",
                "One way to avoid applying a decaying interpolation to the clickthrough data is to do OnlineUp only for the query history Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first buffer all the clickthrough data together and use the whole chunk of clickthrough data to update the model generated through running OnlineUp on previous queries.",
                "The updating equations are as follows. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi where µi has the same interpretation as in OnlineUp, but νi now indicates to what extent we want to trust the clicked summaries.",
                "As in OnlineUp, we set all µis and νis to the same value.",
                "And to rank documents after seeing the current query Qk, we use p(w|θk) = p(w|ψk) 4.",
                "DATA COLLECTION In order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.",
                "Since there is no such data set available to us, we have to create one.",
                "There are two choices.",
                "One is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).",
                "But the problem is that we have no relevance judgments on such data.",
                "The other choice is to use a TREC data set, which has a text database, topic description and relevance judgment file.",
                "Unfortunately, there are no query history and clickthrough history data.",
                "We decide to augment a TREC data set by collecting query history and clickthrough history data.",
                "We select TREC AP88, AP89 and AP90 data as our text database, because AP data has been used in several TREC tasks and has relatively complete judgments.",
                "There are altogether 242918 news articles and the average document length is 416 words.",
                "Most articles have titles.",
                "If not, we select the first sentence of the text as the title.",
                "For the preprocessing, we only do case folding and do not do stopword removal or stemming.",
                "We select 30 relatively difficult topics from TREC topics 1-150.",
                "These 30 topics have the worst average precision performance among TREC topics 1-150 according to some baseline experiments using the KL-Divergence model with Bayesian prior smoothing [20].",
                "The reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a relatively richer query history and clickthrough history data from the user.",
                "In real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.",
                "We index the TREC AP data set and set up a search engine and web interface for TREC AP news articles.",
                "We use 3 subjects to do experiments to collect query history and clickthrough history data.",
                "Each subject is assigned 10 topics and given the topic descriptions provided by TREC.",
                "For each topic, the first query is the title of the topic given in the original TREC topic description.",
                "After the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.",
                "The subject will browse the results and maybe click one or more results to browse the full text of article(s).",
                "The subject may also modify the query to do another search.",
                "For each topic, the subject composes at least 4 queries.",
                "In our experiment, only the first 4 queries for each topic are used.",
                "The user needs to select the topic number from a selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.",
                "We use a relational database to store user interactions, including the submitted queries and clicked documents.",
                "For each query, we store the query terms and the associated result pages.",
                "And for each clicked document, we store the summary as shown on the search result page.",
                "The summary of the article is query dependent and is computed online using fixed-length passage retrieval (KL divergence model with Bayesian prior smoothing).",
                "Among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.",
                "Altogether there are 91 documents clicked to view.",
                "So on average, there are around 3 clicks per topic.",
                "The average length of clicked summary FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Table 1: Effect of using query history and clickthrough data for document ranking. is 34.4 words.",
                "Among 91 clicked documents, 29 documents are judged relevant according to TREC judgment file.",
                "This data set is publicly available 1 . 5.",
                "EXPERIMENTS 5.1 Experiment design Our major hypothesis is that using search <br>context</br> (i.e., query history and clickthrough information) can help improve search accuracy.",
                "In particular, the search <br>context</br> can provide extra information to help us estimate a better query model than using just the current query.",
                "So most of our experiments involve comparing the retrieval performance using the current query only (thus ignoring any <br>context</br>) with that using the current query as well as the search <br>context</br>.",
                "Since we collected four versions of queries for each topic, we make such comparisons for each version of queries.",
                "We use two performance measures: (1) Mean Average Precision (MAP): This is the standard non-interpolated average precision and serves as a good measure of the overall ranking accuracy. (2) Precision at 20 documents (pr@20docs): This measure does not average well, but it is more meaningful than MAP and reflects the utility for users who only read the top 20 documents.",
                "In all cases, the reported figure is the average over all of the 30 topics.",
                "We evaluate the four models for exploiting search <br>context</br> (i.e., FixInt, BayesInt, OnlineUp, and BatchUp).",
                "Each model has precisely two parameters (α and β for FixInt; µ and ν for others).",
                "Note that µ and ν may need to be interpreted differently for different methods.",
                "We vary these parameters and identify the optimal performance for each method.",
                "We also vary the parameters to study the sensitivity of our algorithms to the setting of the parameters. 5.2 Result analysis 5.2.1 Overall effect of search <br>context</br> We compare the optimal performances of four models with those using the current query only in Table 1.",
                "A row labeled with qi is the baseline performance and a row labeled with qi + HQ + HC is the performance of using search <br>context</br>.",
                "We can make several observations from this table: 1.",
                "Comparing the baseline performances indicates that on average reformulated queries are better than the previous queries with the performance of q4 being the best.",
                "Users generally formulate better and better queries. 2.",
                "Using search <br>context</br> generally has positive effect, especially when the <br>context</br> is rich.",
                "This can be seen from the fact that the 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip improvement for q4 and q3 is generally more substantial compared with q2.",
                "Actually, in many cases with q2, using the <br>context</br> may hurt the performance, probably because the history at that point is sparse.",
                "When the search <br>context</br> is rich, the performance improvement can be quite substantial.",
                "For example, BatchUp achieves 92.4% improvement in the mean average precision over q3 and 77.2% improvement over q4. (The generally low precisions also make the relative improvement deceptively high, though.) 3.",
                "Among the four models using search <br>context</br>, the performances of FixInt and OnlineUp are clearly worse than those of BayesInt and BatchUp.",
                "Since BayesInt performs better than FixInt and the main difference between BayesInt and FixInt is that the former uses an adaptive coefficient for interpolation, the results suggest that using adaptive coefficient is quite beneficial and a Bayesian style interpolation makes sense.",
                "The main difference between OnlineUp and BatchUp is that OnlineUp uses decaying coefficients to combine the multiple clicked summaries, while BatchUp simply concatenates all clicked summaries.",
                "Therefore the fact that BatchUp is consistently better than OnlineUp indicates that the weights for combining the clicked summaries indeed should not be decaying.",
                "While OnlineUp is theoretically appealing, its performance is inferior to BayesInt and BatchUp, likely because of the decaying coefficient.",
                "Overall, BatchUp appears to be the best method when we vary the parameter settings.",
                "We have two different kinds of search <br>context</br> - query history and clickthrough data.",
                "We now look into the contribution of each kind of <br>context</br>. 5.2.2 Using query history only In each of four models, we can turn off the clickthrough history data by setting parameters appropriately.",
                "This allows us to evaluate the effect of using query history alone.",
                "We use the same parameter setting for query history as in Table 1.",
                "The results are shown in Table 2.",
                "Here we see that in general, the benefit of using query history is very limited with mixed results.",
                "This is different from what is reported in a previous study [15], where using query history is consistently helpful.",
                "Another observation is that the <br>context</br> runs perform poorly at q2, but generally perform (slightly) better than the baselines for q3 and q4.",
                "This is again likely because at the beginning the initial query, which is the title in the original TREC topic description, may not be a good query; indeed, on average, performances of these first-generation queries are clearly poorer than those of all other user-formulated queries in the later generations.",
                "Yet another observation is that when using query history only, the BayesInt model appears to be better than other models.",
                "Since the clickthrough data is ignored, OnlineUp and BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Table 2: Effect of using query history only for document ranking. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Table 3: Average Precision of BatchUp using query history only are essentially the same algorithm.",
                "The displayed results thus reflect the variation caused by parameter µ.",
                "A smaller setting of 2.0 is seen better than a larger value of 5.0.",
                "A more complete picture of the influence of the setting of µ can be seen from Table 3, where we show the performance figures for a wider range of values of µ.",
                "The value of µ can be interpreted as how many words we regard the query history is worth.",
                "A larger value thus puts more weight on the history and is seen to hurt the performance more when the history information is not rich.",
                "Thus while for q4 the best performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we see some small benefit for q2.",
                "As we would expect, an excessively large µ would hurt the performance in general, but q2 is hurt most and q4 is barely hurt, indicating that as we accumulate more and more query history information, we can put more and more weight on the history information.",
                "This also suggests that a better strategy should probably dynamically adjust parameters according to how much history information we have.",
                "The mixed query history results suggest that the positive effect of using implicit feedback information may have largely come from the use of clickthrough history, which is indeed true as we discuss in the next subsection. 5.2.3 Using clickthrough history only We now turn off the query history and only use the clicked summaries plus the current query.",
                "The results are shown in Table 4.",
                "We see that the benefit of using clickthrough information is much more significant than that of using query history.",
                "We see an overall positive effect, often with significant improvement over the baseline.",
                "It is also clear that the richer the <br>context</br> data is, the more improvement using clicked summaries can achieve.",
                "Other than some occasional degradation of precision at 20 documents, the improvement is fairly consistent and often quite substantial.",
                "These results show that the clicked summary text is in general quite useful for inferring a users information need.",
                "Intuitively, using the summary text, rather than the actual content of the document, makes more sense, as it is quite possible that the document behind a seemingly relevant summary is actually non-relevant. 29 out of the 91 clicked documents are relevant.",
                "Updating the query model based on such summaries would bring up the ranks of these relevant documents, causing performance improvement.",
                "However, such improvement is really not beneficial for the user as the user has already seen these relevant documents.",
                "To see how much improvement we have achieved on improving the ranks of the unseen relevant documents, we exclude these 29 relevant documents from our judgment file and recompute the performance of BayesInt and the baseline using the new judgment file.",
                "The results are shown in Table 5.",
                "Note that the performance of the baseline method is lower due to the removal of the 29 relevant documents, which would have been generally ranked high in the results.",
                "From Table 5, we see clearly that using clicked summaries also helps improve the ranks of unseen relevant documents significantly.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0263 0.100 q2 + HC 0.0314 0.100 Improve. 19.4% 0% q3 0.0331 0.125 q3 + HC 0.0661 0.178 Improve 99.7% 42.4% q4 0.0442 0.165 q4 + HC 0.0739 0.188 Improve 67.2% 13.9% Table 5: BayesInt evaluated on unseen relevant documents One remaining question is whether the clickthrough data is still helpful if none of the clicked documents is relevant.",
                "To answer this question, we took out the 29 relevant summaries from our clickthrough history data HC to obtain a smaller set of clicked summaries HC , and re-evaluated the performance of the BayesInt method using HC with the same setting of parameters as in Table 4.",
                "The results are shown in Table 6.",
                "We see that although the improvement is not as substantial as in Table 4, the average precision is improved across all generations of queries.",
                "These results should be interpreted as very encouraging as they are based on only 62 non-relevant clickthroughs.",
                "In reality, a user would more likely click some relevant summaries, which would help bring up more relevant documents as we have seen in Table 4 and Table 5.",
                "FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650 Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3% q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1% Table 4: Effect of using clickthrough data only for document ranking.",
                "Query BayesInt(µ = 0, ν = 5.0) MAP pr@20docs q2 0.0312 0.1150 q2 + HC 0.0313 0.0950 Improve. 0.3% -17.4% q3 0.0421 0.1483 q3 + HC 0.0521 0.1820 Improve 23.8% 23.0% q4 0.0536 0.1930 q4 + HC 0.0620 0.1850 Improve 15.7% -4.1% Table 6: Effect of using only non-relevant clickthrough data 5.2.4 Additive effect of <br>context</br> information By comparing the results across Table 1, Table 2 and Table 4, we can see that the benefit of the query history information and that of clickthrough information are mostly additive, i.e., combining them can achieve better performance than using each alone, but most improvement has clearly come from the clickthrough information.",
                "In Table 7, we show this effect for the BatchUp method. 5.2.5 Parameter sensitivity All four models have two parameters to control the relative weights of HQ, HC , and Qk, though the parameterization is different from model to model.",
                "In this subsection, we study the parameter sensitivity for BatchUp, which appears to perform relatively better than others.",
                "BatchUp has two parameters µ and ν.",
                "We first look at µ.",
                "When µ is set to 0, the query history is not used at all, and we essentially just use the clickthrough data combined with the current query.",
                "If we increase µ, we will gradually incorporate more information from the previous queries.",
                "In Table 8, we show how the average precision of BatchUp changes as we vary µ with ν fixed to 15.0, where the best performance of BatchUp is achieved.",
                "We see that the performance is mostly insensitive to the change of µ for q3 and q4, but is decreasing as µ increases for q2.",
                "The pattern is also similar when we set ν to other values.",
                "In addition to the fact that q1 is generally worse than q2, q3, and q4, another possible reason why the sensitivity is lower for q3 and q4 may be that we generally have more clickthrough data available for q3 and q4 than for q2, and the dominating influence of the clickthrough data has made the small differences caused by µ less visible for q3 and q4.",
                "The best performance is generally achieved when µ is around 2.0, which means that the past query information is as useful as about 2 words in the current query.",
                "Except for q2, there is clearly some tradeoff between the current query and the previous queries Query MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Improve. -8.0% -15.9% q2 + HC 0.0344 0.1167 Improve. 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Improve. 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Improve 8.1% -2.2% q3 + HC 0.0513 0.1650 Improve 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Improve 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Improve 3.0% -0.8% q4 + HC 0.0623 0.2050 Improve 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Improve 77.2% 16.4% Table 7: Additive benefit of <br>context</br> information and using a balanced combination of them achieves better performance than using each of them alone.",
                "We now turn to the other parameter ν.",
                "When ν is set to 0, we only use the clickthrough data; When ν is set to +∞, we only use the query history and the current query.",
                "With µ set to 2.0, where the best performance of BatchUp is achieved, we vary ν and show the results in Table 9.",
                "We see that the performance is also not very sensitive when ν ≤ 30, with the best performance often achieved at ν = 15.",
                "This means that the combined information of query history and the current query is as useful as about 15 words in the clickthrough data, indicating that the clickthrough information is highly valuable.",
                "Overall, these sensitivity results show that BatchUp not only performs better than other methods, but also is quite robust. 6.",
                "CONCLUSIONS AND FUTURE WORK In this paper, we have explored how to exploit implicit feedback information, including query history and clickthrough history within the same search session, to improve information retrieval performance.",
                "Using the KL-divergence retrieval model as the basis, we proposed and studied four statistical language models for <br>context</br>-sensitive information retrieval, i.e., FixInt, BayesInt, OnlineUp and BatchUp.",
                "We use TREC AP Data to create a test set µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Table 8: Sensitivity of µ in BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Table 9: Sensitivity of ν in BatchUp for evaluating implicit feedback models.",
                "Experiment results show that using implicit feedback, especially clickthrough history, can substantially improve retrieval performance without requiring any additional user effort.",
                "The current work can be extended in several ways: First, we have only explored some very simple language models for incorporating implicit feedback information.",
                "It would be interesting to develop more sophisticated models to better exploit query history and clickthrough history.",
                "For example, we may treat a clicked summary differently depending on whether the current query is a generalization or refinement of the previous query.",
                "Second, the proposed models can be implemented in any practical systems.",
                "We are currently developing a client-side personalized search agent, which will incorporate some of the proposed algorithms.",
                "We will also do a user study to evaluate effectiveness of these models in the real web search.",
                "Finally, we should further study a general retrieval framework for sequential decision making in interactive information retrieval and study how to optimize some of the parameters in the <br>context</br>-sensitive retrieval models. 7.",
                "ACKNOWLEDGMENTS This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and IIS-0428472.",
                "We thank the anonymous reviewers for their useful comments. 8.",
                "REFERENCES [1] E. Adar and D. Karger.",
                "Haystack: Per-user information environments.",
                "In Proceedings of CIKM 1999, 1999. [2] J. Allan and et al.",
                "Challenges in information retrieval and language modeling.",
                "Workshop at University of Amherst, 2002. [3] K. Bharat.",
                "Searchpad: Explicit capture of search <br>context</br> to support web search.",
                "In Proceeding of WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.",
                "Relevance feedback and personalization: A language modeling perspective.",
                "In Proeedings of Second DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries, 2001. [5] H. Cui, J.-R. Wen, J.-Y.",
                "Nie, and W.-Y.",
                "Ma.",
                "Probabilistic query expansion using query logs.",
                "In Proceedings of WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz.",
                "Implicit queries (IQ) for contextualized search (demo description).",
                "In Proceedings of SIGIR 2004, page 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin.",
                "Placing search in <br>context</br>: The concept revisited.",
                "In Proceedings of WWW 2002, 2001. [8] C. Huang, L. Chien, and Y. Oyang.",
                "Query session based term suggestion for interactive web search.",
                "In Proceedings of WWW 2001, 2001. [9] X. Huang, F. Peng, A.",
                "An, and D. Schuurmans.",
                "Dynamic web log session identification with statistical language models.",
                "Journal of the American Society for Information Science and Technology, 55(14):1290-1303, 2004. [10] G. Jeh and J. Widom.",
                "Scaling personalized web search.",
                "In Proceeding of WWW 2003, 2003. [11] T. Joachims.",
                "Optimizing search engines using clickthrough data.",
                "In Proceedings of SIGKDD 2002, 2002. [12] D. Kelly and N. J. Belkin.",
                "Display time as implicit feedback: Understanding task effects.",
                "In Proceedings of SIGIR 2004, 2004. [13] D. Kelly and J. Teevan.",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum, 32(2), 2003. [14] J. Rocchio.",
                "Relevance feedback information retrieval.",
                "In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313-323, Kansas City, MO, 1971.",
                "Prentice-Hall. [15] X. Shen and C. Zhai.",
                "Exploiting query history for document ranking in interactive information retrieval (poster).",
                "In Proceedings of SIGIR 2003, 2003. [16] S. Sriram, X. Shen, and C. Zhai.",
                "A session-based search engine (poster).",
                "In Proceedings of SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano, and M. Yoshikawa.",
                "Adaptive web search based on user profile constructed without any effort from users.",
                "In Proceedings of WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and I. Ruthven.",
                "A simulated study of implicit feedback models.",
                "In Proceedings of ECIR 2004, pages 311-326, 2004. [19] C. Zhai and J. Lafferty.",
                "Model-based feedback in the KL-divergence retrieval model.",
                "In Proceedings of CIKM 2001, 2001. [20] C. Zhai and J. Lafferty.",
                "A study of smoothing methods for language models applied to ad-hoc information retrieval.",
                "In Proceedings of SIGIR 2001, 2001."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "\"Contexto\"-Recuperación de información sensible al uso de comentarios implícitos Xuehua Shen Departamento de Informática Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Informática Universidad de Illinois en Urbana-ChampainUna limitación importante de la mayoría de los modelos y sistemas de recuperación existentes es que la decisión de recuperación se toma basada únicamente en la consulta y la recopilación de documentos;La información sobre el usuario real y el \"contexto\" de búsqueda se ignora en gran medida.",
                "Utilizamos los datos de TREC AP para crear una recopilación de pruebas con información de \"contexto\" de búsqueda y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas.",
                "Por lo tanto, un sistema de recuperación óptimo debería tratar de explotar la mayor cantidad de información adicional de \"contexto\" como sea posible para mejorar la precisión de la recuperación, siempre que esté disponible.",
                "De hecho, la recuperación sensible al \"contexto\" se ha identificado como un desafío importante en la investigación de recuperación de información [2].",
                "Hay muchos tipos de \"contexto\" que podemos explotar.",
                "La retroalimentación de relevancia [14] puede considerarse como una forma para que un usuario proporcione más \"contexto\" de búsqueda y se sabe que es efectivo para mejorar la precisión de la recuperación.",
                "Otro trabajo relacionado sobre el uso del \"contexto\" incluye búsqueda personalizada [1, 3, 4, 7, 10], análisis de registro de consultas [5], factores de \"contexto\" [12] y consultas implícitas [6].",
                "Si bien el trabajo anterior se ha centrado principalmente en usar información de clics, en este documento, utilizamos información de clics y consultas anteriores, y nos centramos en desarrollar nuevos modelos de lenguaje sensibles a \"contexto\" para la recuperación.",
                "Utilizamos el modelo de recuperación de divergencia KL [19] como base y proponemos tratar la recuperación sensible al \"contexto\" como estimación de un modelo de lenguaje de consulta basado en la consulta actual y cualquier información de \"contexto\" de búsqueda.",
                "Definición del problema Hay dos tipos de información de \"contexto\" que podemos usar para la retroalimentación implícita.",
                "Uno es el \"contexto\" a corto plazo, que es la información circundante inmediata que arroja luz sobre la necesidad actual de información de los usuarios en una sola sesión.",
                "La categoría de información de los usuarios necesita (por ejemplo, niños o deportes), consultas anteriores y documentos recientemente vistos son ejemplos de \"contexto\" a corto plazo.",
                "En general, el \"contexto\" a corto plazo es más útil para mejorar la búsqueda en la sesión actual, pero puede no ser tan útil para las actividades de búsqueda en una sesión diferente.",
                "El otro tipo de \"contexto\" es el \"contexto\" a largo plazo, que se refiere a información como un nivel educativo de usuarios e interés general, el historial de consultas de usuarios acumulados e información de clics de los usuarios anteriores;Dicha información es generalmente estable durante mucho tiempo y a menudo se acumula con el tiempo.",
                "El \"contexto\" a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el \"contexto\" a corto plazo para mejorar la precisión de la búsqueda para una sesión en particular.",
                "En este artículo, nos centramos en el \"contexto\" a corto plazo, aunque algunos de nuestros métodos también se pueden utilizar para incorporar naturalmente algún \"contexto\" a largo plazo.",
                "Además del historial de consultas, puede haber otra información de \"contexto\" a corto plazo disponible.",
                "Proponemos utilizar modelos de lenguaje estadístico para modelar una necesidad de información de los usuarios y desarrollar cuatro modelos de lenguaje sensibles a \"contexto\" específicos para incorporar información de \"contexto\" en un modelo de recuperación básica.3.1 Modelo de recuperación básica Usamos el método de divergencia Kullback-Leibbler (KL) [19] como nuestro método de recuperación básica.",
                "Una ventaja de este enfoque es que podemos incorporar naturalmente el \"contexto\" de búsqueda como evidencia adicional para mejorar nuestra estimación del modelo de lenguaje de consulta.",
                "Nuestra tarea es estimar un modelo de consulta de \"contexto\", que denotamos mediante P (w | θk), basado en la consulta actual QK, así como en el HQ History de consultas e Historio HIS HC.",
                "Si combinamos estas ecuaciones, vemos que p (w | θk) = αp (w | qk) + (1 - α) [βp (w | hc) + (1 - β) p (w | hq)] es decir, el modelo estimado de consulta de \"contexto\" es solo una interpolación de coeficiente fijo de tres modelos P (W | QK), P (W | HQ) y P (W | HC).3.3 Interpolación bayesiana (BayesInt) Un posible problema con el enfoque de fijación es que los coeficientes, especialmente α, se fijan en todas las consultas.",
                "Para capturar esta intuición, tratamos P (W | HQ) y P (W | HC) como Dirichlet Priors y QK como datos observados para estimar un modelo de consulta de \"contexto\" utilizando estimador bayesiano.",
                "En ambos casos, podemos tratar el modelo actual como un modelo previo del modelo de consulta de \"contexto\" y tratar la nueva consulta observada o hacer clic en el resumen como datos observados.",
                "Tenga en cuenta que podemos tomar P (W | φi) o P (W | φi) como nuestro modelo de consulta de \"contexto\" para documentos de clasificación.",
                "Experimentos 5.1 Diseño del experimento Nuestra hipótesis principal es que el uso de \"contexto\" de búsqueda (es decir, historial de consultas e información de clics) puede ayudar a mejorar la precisión de la búsqueda.",
                "En particular, el \"contexto\" de búsqueda puede proporcionar información adicional para ayudarnos a estimar un mejor modelo de consulta que usar solo la consulta actual.",
                "Por lo tanto, la mayoría de nuestros experimentos implican comparar el rendimiento de la recuperación utilizando solo la consulta actual (ignorando así cualquier \"contexto\") con eso utilizando la consulta actual, así como el \"contexto\" de búsqueda.",
                "Evaluamos los cuatro modelos para explotar el \"contexto\" de búsqueda (es decir, FixInt, BayesInt, Onlineup y Batchup).",
                "También variamos los parámetros para estudiar la sensibilidad de nuestros algoritmos a la configuración de los parámetros.5.2 Análisis de resultados 5.2.1 Efecto general del \"contexto\" de búsqueda Comparamos el rendimiento óptimo de cuatro modelos con aquellos que usan la consulta actual solo en la Tabla 1.",
                "Una fila etiquetada con Qi es el rendimiento de línea de base y una fila etiquetada con Qi + HQ + HC es el rendimiento de usar \"contexto\" de búsqueda.",
                "El uso de \"contexto\" de búsqueda generalmente tiene un efecto positivo, especialmente cuando el \"contexto\" es rico.",
                "En realidad, en muchos casos con Q2, usar el \"contexto\" puede dañar el rendimiento, probablemente porque la historia en ese punto es escasa.",
                "Cuando el \"contexto\" de búsqueda es rico, la mejora del rendimiento puede ser bastante sustancial.",
                "Entre los cuatro modelos que utilizan \"contexto\" de búsqueda, las actuaciones de Fixint y OnlineUP son claramente peores que las de Bayesint y Batchup.",
                "Tenemos dos tipos diferentes de \"contexto\" de búsqueda: historial de consultas y datos de clics.",
                "Ahora analizamos la contribución de cada tipo de \"contexto\".5.2.2 Utilizando el historial de consultas solo en cada uno de los cuatro modelos, podemos desactivar los datos del historial de clics configurando los parámetros de manera adecuada.",
                "Otra observación es que las ejecuciones de \"contexto\" funcionan mal en Q2, pero generalmente funcionan (ligeramente) mejor que las líneas de base para Q3 y Q4.",
                "También está claro que cuanto más ricos sean los datos del \"contexto\", más mejora el uso de resúmenes haciendo clic.",
                "Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0312 0.1150 Q2 + HC 0.0313 0.0950 Mejora.0.3% -17.4% Q3 0.0421 0.1483 Q3 + HC 0.0521 0.1820 Mejorar 23.8% 23.0% Q4 0.0536 0.1930 Q4 + HC 0.0620 0.1850 Mejora 15.7% -4.1% Tabla 6: Efecto de uso solo de datos de Clickhrough no relevantes 5.2.2Información de \"contexto\" comparando los resultados en la Tabla 1, la Tabla 2 y la Tabla 4, podemos ver que el beneficio de la información del historial de consultas y el de la información de clics son en su mayoría aditivos, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno solo, pero la mayoría de la mejora claramente proviene de la información de clics.",
                "Excepto por Q2, claramente existe cierta compensación entre la consulta actual y el mapa de consultas de consultas anteriores PR@20DOCS Q2 0.0312 0.1150 Q2 + HQ 0.0287 0.0967 Mejora.-8.0% -15.9% Q2 + HC 0.0344 0.1167 Mejora.10.3% 1.5% Q2 + HQ + HC 0.0342 0.1100 Mejorar.9.6% -4.3% Q3 0.0421 0.1483 Q3 + HQ 0.0455 0.1450 Mejorar 8.1% -2.2% Q3 + HC 0.0513 0.1650 Mejorar 21.9% 11.3% Q3 + HQ + HC 0.0810 0.2067 Mejorar 92.4% 39.4% Q4 0.0536 0.1930 Q4 + HQ + HQ + HQ + HQ3.0% -0.8% Q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% Q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: beneficio aditivo de la información de \"contexto\" y el uso de una combinación equilibrada de ellos logra un mejor rendimiento que usar cada uno con cada uno de los cuales el uso de cada uno de los cuales usa cada uno de ellos.de ellos solos.",
                "Utilizando el modelo de recuperación de divergencia KL como base, propusimos y estudiamos cuatro modelos de lenguaje estadístico para la recuperación de información sensible al \"contexto\", es decir, fixTint, bayesInt, onlineup y batchup.",
                "Finalmente, debemos estudiar más a fondo un marco de recuperación general para la toma de decisiones secuenciales en la recuperación de información interactiva y estudiar cómo optimizar algunos de los parámetros en los modelos de recuperación sensibles al \"contexto\".7.",
                "SearchPad: Captura explícita del \"contexto\" de búsqueda para admitir la búsqueda web.",
                "Colocar la búsqueda en \"contexto\": el concepto revisitado."
            ],
            "translated_text": "",
            "candidates": [
                "contexto",
                "Contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "Contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "Contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto",
                "contexto"
            ],
            "error": []
        }
    }
}