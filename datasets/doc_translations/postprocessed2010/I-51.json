{
    "id": "I-51",
    "original_text": "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication. The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques. For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision. We experimentally show that the argumentation among committees of agents improves both the individual and joint performance. For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance. Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1. INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution. In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication. Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation. Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand. However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited. Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process. Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]). Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments. However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation. In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience. Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework. Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples. Counterexamples offer the possibility of agents learning during the argumentation process. Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments. Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples. This paper presents a case-based approach to address both issues. The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation. We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases. In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments. Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them. The paper is structured as follows. Section 2 discusses the relation among argumentation, collaboration and learning. Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction. After that, Section 4 formally defines our argumentation framework. Sections 5 and 6 present our case-based preference relation and argument generation policies respectively. Later, Section 7 presents the argumentation protocol in our AMAL framework. After that, Section 8 presents an exemplification of the argumentation framework. Finally, Section 9 presents an empirical evaluation of our two main hypotheses. The paper closes with related work and conclusions sections. 2. ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance. In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings. Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources. Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems. In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance. An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance. In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand. Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process. Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations. In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3. MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci. Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci. A case base Ci = {c1, ..., cm} is a collection of cases. Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems. In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes. In the following we will note the set of all the solution classes by S = {S1, ..., SK }. Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly. Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S. Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process. However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents). The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user. The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system. Most of the existing work on explanation generation focuses on generating explanations to be provided to the user. However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents. We are interested in justifications since they can be used as arguments. For that purpose, we will benefit from the ability of some machine learning methods to provide justifications. A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar. Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common. For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems). In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait. Thus, since only this attribute has been used, it is the only one appearing in the justification. The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system. Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification. In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class. In the rest of the paper, we will use to denote the subsumption relation. In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1. When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1. A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9]. In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4. ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct. In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate. In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D . Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples. A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D. In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red. A counterargument β is an argument offered in opposition to another argument α. In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D . In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed. A counterexample c is a case that contradicts an argument α. Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α. By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process. However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples). In the following sections we will present these elements. 5. PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data). For that reason, we are going to define a preference relation over contradicting justified predictions based on cases. Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one. The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it. The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence. Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D. With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents. An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples. Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases. Notice that this correction follows the same idea than the Laplace correction to estimate probabilities. Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument. In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6. GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods. Any learning method able to provide a justified prediction can be used to generate arguments. For instance, decision trees and LID [2] are suitable learning methods. Specifically, in the experiments reported in this paper agents use LID. Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1. For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification. In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent. The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge. Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples. Let us explain how they can be generated. An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai. Moreover, while generating such counterargument β, Ai expects that β is preferred over α. For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10]. The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed. Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut. However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence. Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence. Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines). Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D). The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task. For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term. Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced. When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class. To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch. In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α. However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D. Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID. Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set. Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1. Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2. If not found, then Ai searches for a counterexample c ∈ Ci of α. If a case c is found, then c is sent to the other agent as a counterexample of α. 3. If no counterexamples are found, then Ai cannot rebut the argument α. 7. ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process. If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution. Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents. The AMAL protocol consists on a series of rounds. In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents. The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent. Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds). When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not. Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument. When all the agents have had the token once, the token returns to the first agent, and so on. If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends. Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required). At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α. An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α. We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i. The protocol is initiated because one of the agents receives a problem P to be solved. After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1. At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method. Then, each agent Ai sends the performative assert(α0 i ) to the other agents. Thus, the agents know H0 = α0 i , ..., α0 n . Once all the predictions have been sent the token is given to the first agent A1. 2. At each round t (other than 0), the agents check whether their arguments in Ht agree. If they do, the protocol moves to step 5. Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5. Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1). Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj). If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj. Otherwise (i.e. CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj. In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj. The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3. The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence. If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents. Otherwise (i.e. CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly. Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4. The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them. Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5. The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction. The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8. EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3. One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it. For that reason, invites A2 and A3 to take part in the argumentation process. They accept the invitation, and the argumentation protocol starts. Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents. Thus, all of them can compute H0 = α0 1, α0 2, α0 3 . In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3. Thus, A1 sends the the message rebut( c13, α0 3) to A3. A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration. A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3). Thus, all of them know the new H1 = α0 1, α0 2, α1 3 . Round 1 starts and A2 gets the token. A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3. The counterargument is sent to A3 with the message rebut(β1 2 , α1 3). Agent A3 receives the counterargument and assesses its local confidence. The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3. Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 . Round 2 starts and A3 gets the token. A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2). Agent A2 receives the counterargument and assesses its local confidence. The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2. Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ). After that, H3 = α0 1, β2 3 , α1 3 . At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9. EXPERIMENTAL EVALUATION 980 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents. In this section we empirically evaluate the AMAL argumentation framework. We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set). The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes. In an experimental run, the data set is divided in 2 sets: the training set and the test set. The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents. In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution. The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process. Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account). Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents). Figure 5 shows the result of those experiments in the sponge and soybean data sets. Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown. For each number of agents, three bars are shown: individual, Voting, and AMAL. The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation. The results shown are the average of 5 10-fold cross validation runs. Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving. Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account. We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process. For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%). Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set. The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions). These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting). Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process. Figure 6 shows the result of that experiment for the two data sets. Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples). Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation). For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience. The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication). In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication). Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10. RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents. Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning. Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation. The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process. Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation. Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13]. Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments. In our framework we have addressed both argument selection and preference relations using a case-based approach. 11. CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning. Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments. The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation. The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication. Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents. Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12. REFERENCES [1] Agnar Aamodt and Enric Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza. Lazy induction of descriptions for relational case-based learning. In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka. Dynamic argument systems: A formal model of argumentation processes based on situation calculus. Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari. Formalizing Defeasible Argumentation using Labelled Deductive Systems. Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi. Automatically selecting strategies for multi-case-base reasoning. In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos. Knowledge and experience reuse through communications among competent (peer) agents. International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth. Collaborative case-based reasoning: Applications in personalized route planning. In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376. Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza. Justification-based multiagent learning. In ICML2003, pages 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón. The explanatory power of symbolic similarity in case-based reasoning. Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole. On the comparison of theories: Preferring the most specific explanation. In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander. Retrieval and reasoning in distributed case bases. Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik. Reaching agreements through argumentation: a logical model and implementation. Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra. Agents that reason and negotiate by arguing. Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley. Explanation component of software systems. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)",
    "original_translation": "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas de múltiples agentes Santi Ontañón CCL, Lab de computación cognitiva Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Consejo Español para la Investigación Científica para la Investigación CientíficaCampus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es Resumen En este documento presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado para dos fines: (1) para la deliberación conjunta y (2) paraAprendiendo de la comunicación. El marco AMAL se basa completamente en el aprendizaje de los ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de acuartiones son técnicas basadas en casos. Para unirse a la deliberación, los agentes de aprendizaje comparten su experiencia al formar un comité para decidir sobre alguna decisión conjunta. Mostramos experimentalmente que la argumentación entre los comités de los agentes mejora el desempeño individual y conjunto. Para aprender de la comunicación, un agente se dedica a discutir con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos;El proceso de argumentación mejora su alcance de aprendizaje y su desempeño individual. Categorías y descriptores de sujetos I.2.6 [Inteligencia artificial]: aprendizaje;I.2.11 [Inteligencia artificial]: Sistemas distribuidos de inteligencia artificial-multiagente, agentes inteligentes 1. Introducción Los marcos de argumentación para sistemas de múltiples agentes se pueden utilizar para diferentes fines como deliberación conjunta, persuasión, negociación y resolución de conflictos. En este artículo presentaremos un marco de argumentación para los agentes de aprendizaje y demostraremos que puede usarse para dos fines: (1) deliberación conjunta y (2) aprendizaje de la comunicación. La deliberación conjunta basada en la argumentación implica la discusión sobre el resultado de una situación particular o el curso de acción apropiado para una situación particular. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que los ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado de la situación en cuestión. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también es limitada. Por lo tanto, los agentes de aprendizaje que son capaces de discutir sus predicciones individuales con otros agentes pueden alcanzar una mejor precisión de predicción después de tal proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas de múltiples agentes se basan en la lógica deductiva o en algún otro formalismo lógico deductivo diseñado específicamente para apoyar la argumentación, como la lógica predeterminada [3]). Por lo general, un argumento se considera una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento [4, 13];Los agentes usan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en la lógica asumen agentes con conocimiento precargado y relación de preferencia. En este documento, nos centramos en un marco de aprendizaje múltiple (AMAL) basado en argumentación donde se aprende tanto el conocimiento como la relación de preferencia de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) funcionan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender de ejemplos y (3) comunicarse utilizando un marco argumentativo. Tener capacidades de aprendizaje permite que los agentes usen efectivamente una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos cuestiones: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos contradictorios que han sido inducidos por ejemplos. Este documento presenta un enfoque basado en casos para abordar ambos problemas. Los agentes usan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) para predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco AMAL en los agentes de los apoyos para alcanzar una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre los agentes de aprendizaje puede producir una predicción conjunta que mejore sobre el rendimiento del aprendizaje individual y (2) si el aprendizaje de los contraexperios transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se usan mientras discuten entrea ellos. El papel está estructurado de la siguiente manera. La Sección 2 discute la relación entre argumentación, colaboración y aprendizaje. Luego, la Sección 3 introduce nuestro marco CBR (MAC) de múltiples agentes y la noción de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y políticas de generación de argumentos respectivamente. Más tarde, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El documento se cierra con secciones de trabajo y conclusiones relacionadas.2. La argumentación, la colaboración y el aprendizaje tanto el aprendizaje como la colaboración son formas en que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en los sistemas de múltiples agentes, ya que ambas son formas en que los agentes pueden lidiar con sus deficiencias. Demostremos cuáles son las principales motivaciones que un agente puede tener que aprender o colaborar.• Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Aumentar el rango de problemas solucionables.• Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Aumentar el rango de problemas solucionables, aumentar el rango de recursos accesibles. Mirando las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas de múltiples agentes. De hecho, con la excepción del último elemento en las motivaciones para colaborar la lista, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, colaborando o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo, propondremos Amal, un marco de argumentación para los agentes de aprendizaje, y también mostraremos cómo se puede usar Amal tanto para aprender de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa a través de la participación deUn proceso de argumentación sobre la predicción de la situación en cuestión. Usando esta colaboración, la predicción se puede hacer de una manera más informada, ya que la información conocida por varios agentes se ha tenido en cuenta.• Los agentes también pueden aprender de la comunicación con otros agentes involucrando un proceso de argumentación. Los agentes que participan en tales procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y usar esta información para predecir los resultados de situaciones futuras. En el resto de este documento, propondremos un marco de argumentación y mostraremos cómo se puede usar tanto para aprender como para resolver problemas de manera colaborativa.3. Sistemas CBR de múltiples agentes Un sistema de razonamiento basado en casos de múltiples agentes (Mac) M = {(a1, c1), ..., (an, cn)} es un sistema de múltiples agentes compuesto de a = {ai, .. ..., An}, un conjunto de agentes CBR, donde cada agente ai ∈ A posee una base de casos individual CI. Cada agente individual IA en una Mac es completamente autónoma y cada IA de agente solo tiene acceso a su CI de base individual y privada. Una base base CI = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver individualmente problemas, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos restringiremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. A continuación, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso C = P, S es una tupla que contiene una descripción de caso P y una clase de solución S ∈ S. A continuación, usaremos el problema de los términos y la descripción del caso de manera indistintiva. Además, utilizaremos la notación DOT para referirnos a elementos dentro de una tupla;por ejemplo, para referirnos a la clase de solución de un caso C, escribiremos C.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta, cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que los otros agentes pueden examinar y criticar). La siguiente sección aborda este problema.3.1 Predicciones justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] a cargo de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, lo que aumenta la confiabilidad del sistema. La mayor parte del trabajo existente en la generación de explicaciones se centra en generar explicaciones que se proporcionarán al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y la coordinación entre los agentes. Estamos interesados en las justificaciones, ya que pueden usarse como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación creada por un método CBR después de determinar que la solución de un problema particular P fue SK es una descripción que contiene la información relevante del problema P que el método CBR ha considerado predecir SK como la solución de P. En particular, CBRLos métodos funcionan recuperando casos similares al problema en cuestión y luego reutilizando sus soluciones para el problema actual, esperando que, dado que el problema y los casos sean similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., CN para resolver un problema particular P La justificación creada contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, es decirContendrá la información relevante que P y C1, ..., CN tienen en común. Por ejemplo, la Figura 1 muestra una construcción de justificación por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Traffic_light y Cars_Passing), el mecanismo de recuperación del sistema CBR se da cuenta de que al considerar solo el atributo Traffic_light, puede recuperar dos casos que predicen la misma solución: espera. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de los atributos son irrelevantes, ya que sea cual sea su valor, la clase de solución habría sido el mismo.976 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (Aamas 07) Problema Traffic_light: Red Cars_Passing: No Case 1 Traffic_light: Red Cars_Passing: No Solución: Wait Case 3 Traffic_light: Red Cars_Passing: Sí Solución: Caso de espera 4 Traffic_light: Green Cars_Passing: Sí.: Wait Case 2 Traffic_light: Green Cars_Passing: No Solución: Casos de Cruz Recuperados Solución: Justificación de espera Traffic_light: Rojo Figura 1: Un ejemplo de generación de justificación en un sistema CBR. Observe que, dado que la única característica relevante para decidir es TRATFER_LIGHT (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría de) los casos en la base de casos de un agente que satisface la justificación (es decir, todos los casos que se subsumen por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsunción. En nuestro trabajo, usamos Lid [2], un método CBR capaz de construir justificaciones simbólicas, como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: Definición 3.1. Una predicción justificada es una tupla j = a, p, s, d donde el agente A considera la solución correcta para el problema P, y esa predicción está justificada una descripción simbólica d de tal manera que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas CBR [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, para permitir que los agentes de aprendizaje participen en procesos de argumentación.4. Argumentos y contraargumentos para nuestros propósitos Un argumento α generado por un agente A está compuesto por una declaración sy alguna evidencia d que respalda S como correcta. En el resto de esta sección, veremos cómo esta definición general de argumento puede instanciarse en un tipo de argumentos específicos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre las predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S y B) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente AI para argumentar que AI cree que la solución correcta para un problema P es α.s, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente AI puede generar el argumento α = ai, p, espera, (tráfico_light = rojo), lo que significa que el agente AI cree que la solución correcta para P es esperar porque el atributo tráfico_light es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco, un contraargumento consiste en una predicción justificada AJ, P, S, D generada por un agente AJ con la intención de refutar un argumento α generado por otro agente AI, que respalda una solución de solución diferente de la de α.spara el problema en cuestión y justifica esto con una justificación d. En el ejemplo de la Figura 1, si un agente genera el argumento α = ai, p, walk, (cars_passing = no), un agente que piensa que la solución correcta es la espera podría responder con el contraargumento β = AJ, P, Wait,(Cars_passing = no ∧ tráfico_light = rojo), lo que significa que, aunque no hay automóviles que pasen, el semáforo es rojo y la calle no se puede cruzar. Un contraejemplo C es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que establece que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c.Específicamente, para que un caso C sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D C y α.s = C.S, es decir, el caso debe satisfacer la justificación α.d y la solución de Cser diferente al predicho por α. Al intercambiar argumentos y contraargumentos (incluidos los contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden involucrar un proceso de deliberación conjunta. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre los argumentos contradictorios y una política de decisión para generar argumentos contra los contratiempos (incluidas las contraexampres). En las siguientes secciones presentaremos estos elementos.5. Relación de preferencia Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre las predicciones justificadas basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tiene en cuenta los casos propiedad de cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son contraejemplos. Cuanto más sean los casos de respaldo, mayor es la confianza;Y cuanto más sean las contraejemplos, menor será la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individual que subsume por α.d. Con ellos, un agente ai obtiene los valores Y (aye) y n (no): • y ai α = | {c ∈ Ci |α.D C.P ∧ α.S = C.S} |es el número de casos en la base del caso de los agentes subsumidos por la justificación α.d que pertenecen a la clase de solución α.s, • nai α = | {c ∈ Ci |α.D C.P ∧ α.S = C.S} |es el número de casos en la base del caso de los agentes subsumidos por justificación α.d que no pertenecen a esa clase de solución. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 977 + + + + + + - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: Cai (α) = y ai α 1 + y ai α + nai α, es decir, la confianza en una predicción justificada es el número de casos de respaldo divididos por el número de casos de respaldo más contraexplatos. Tenga en cuenta que agregamos 1 al denominador, esto es para evitar dar confidencias excesivamente altas a las predicciones justificadas cuya confianza se ha calculado utilizando un pequeño número de casos. Observe que esta corrección sigue la misma idea que la corrección de Laplace para estimar las probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, tres casos de respaldo y un contraejemplo se encuentran en la base de casos de IA de los agentes, dando una confianza estimada de 0.6 Además, también podemos definir la confianza conjunta de un argumentoα a medida que la confianza se calculó utilizando los casos presentes en las bases de casos de todos los agentes del grupo: c (α) = i y ai α 1 + i y ai α + nai α notifica que, calcular colaborativamente la confianza articular, laLos agentes solo tienen que hacer públicos los valores de AYE y no calculados localmente para un argumento determinado. En nuestro marco, los agentes usan esta confianza articular como relación de preferencia: se prefiere una predicción justificada α sobre otro β si c (α) ≥ c (β).6. Generación de argumentos En nuestro marco, los agentes generan argumentos de los casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede usarse para generar argumentos. Por ejemplo, los árboles de decisión y la tapa [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos informados en este documento, los agentes usan la tapa. Por lo tanto, cuando un agente quiere generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por la tapa después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, una nueva esponja para determinar su orden), el agente usa la tapa para generar un argumento (que consiste en una predicción justificada) utilizando los casos en elBase de casos del agente. La justificación que se muestra en la Figura 3 se puede interpretar diciendo que la solución predicha es Hadromerida porque la forma suave de los megascleros del esqueleto espiculado de la esponja es de tipo tylostyle, el esqueleto espikulado de la esponja no tiene una longitud uniforme, y no hayGemmules en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, Hadromerida, D1.6.1 Generación de contraargumentos Como se indicó anteriormente, los agentes pueden tratar de refutar los argumentos generando contraargumentos o encontrando contraejemplos. Expliquemos cómo se pueden generar. Un agente AI quiere generar un contraargumento β para refutar un argumento α cuando α está en contradicción con la base de casos local de IA. Además, mientras se genera tal contraargumento β, la IA espera que se prefieran β sobre α. Para ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se usa ampliamente en marcos deductivos para la argumentación, y afirma que entre dos argumentos conflictivos, los más específicos deben preferirse, ya que está, en principio, más informado. Por lo tanto, se espera que los argumentos generados generados en función del criterio de especificidad sean preferibles (ya que están más informados) a los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales argumentos contra los contraar ganan, ya que, como hemos dicho en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno puede pensar que sería mejor que los agentes generen contraargumentos basados en la relación de preferencia de confianza conjunta;Sin embargo, no es obvio cómo generar argumentos en contra de la confianza conjunta de una manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar acuartiones (actualmente una de nuestras futuras líneas de investigación). Por lo tanto, en nuestro marco, cuando un agente quiere generar un contraargumento β a un argumento α, β debe ser más específico que α (es decir, α.D <β.D). La generación de argumentos contra los que utilizan el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 se pueden adaptar fácilmente a esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción que comienza desde cero y agregue heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se hace más específica que en el paso anterior, y el número de casos que se subsume por esa descripción se reduce. Cuando la descripción cubre solo (o casi) casos de una sola clase de solución termina y predice esa clase de solución. Para generar un argumento contra un argumento, la tapa α solo tiene que usar como punto de partida, la descripción α.d en lugar de comenzar desde cero. De esta manera, la justificación proporcionada por la tapa siempre será subsumida por α.D y, por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, observe que la tapa a veces puede no ser capaz de generar contraargumentos, ya que la tapa puede no especializar la descripción α.d más o porque la IA del agente no tiene un caso inci que sea subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento que se muestra en la Figura 3, genera un contraargumento usando la tapa. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 El sexto intl. Conf.Sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Solución: Justificación de Hadromerida: D1 Sponge Spikule Skeleton Características externas Características externas Gemmules: No Spikule Skeleton Megascleros Longitud uniforme: No Megascleros Forma lisa: Base de caso de estilo tylostyle de A1 NUEVO esponja P Figura 3 Figura 3: Ejemplo de una justificación real generada por la tapa en el conjunto de datos de las esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente AI quiere refutar un argumento α, utiliza la siguiente política: 1. Agent AI usa la tapa para tratar de encontrar un contraargumento β más específico que α;Si se encuentra, β se envía al otro agente como un contraargumento de α.2. Si no se encuentra, entonces AI busca un contraejemplo C ∈ Ci de α. Si se encuentra un caso C, entonces C se envía al otro agente como un contraejemplo de α.3. Si no se encuentran contraejemplos, entonces AI no puede refutar el argumento α.7. Aprendizaje múltiple basado en argumentación El protocolo de interacción de AMAL permite que un grupo de agentes A1 ..., y deliberaran sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta finaliza;De lo contrario, se utiliza un voto ponderado para determinar la solución articular. Además, Amal también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo Amal consiste en una serie de rondas. En la ronda inicial, cada agente establece que es su predicción individual para P. Entonces, en cada ronda, un agente puede tratar de refutar la predicción realizada por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de aprobación de tokens para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción realizada por cualquier otro agente. Específicamente, cada agente puede enviar un contraargumento o un contraejemplo cada vez que obtiene el token (observe que esta restricción es solo para simplificar el protocolo, y que no restringe el número de contraargumentos que un agente puede enviar, ya que puede retrasarsepara rondas posteriores). Cuando un agente recibe un contraargumento o un contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los argumentos contra los años cuando reciben el token, tratando de generar un acoplamiento contra el contraargumento. Cuando todos los agentes han tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento en el protocolo, todos los agentes están de acuerdo o durante las últimas n rondas, ningún agente ha generado ningún argumento contra la contraer, el protocolo termina. Además, si al final de la argumentación, los agentes no han llegado a un acuerdo, entonces un mecanismo de votación que usa la confianza de cada predicción como pesas se usa para decidir la solución final (por lo tanto, Amal sigue el mismo mecanismo que los comités humanos, primeroCada miembro individual de un comité expone sus argumentos y discuso los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • Afirmar (α): la predicción justificada mantenida durante la próxima ronda será α. Un agente solo puede contener una sola predicción en cada ronda, por lo que se envían múltiples afirmaciones, solo la última se considera la predicción actualmente sostenida.• RECUTAR (β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos HT = αT 1, ..., αT N como las predicciones que cada uno de los n agentes contiene en una ronda t.Además, también definiremos contradecir (αT i) = {α ∈ Ht | α.S = αT i.s} como el conjunto de argumentos contradictorios para un agente ai en una ronda t, es decir, el conjunto de argumentos en la redonclase de solución diferente que αT i. El protocolo se inicia porque uno de los agentes recibe un problema P para resolverse. Después de eso, el agente informa a todos los demás agentes sobre el problema P para resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método CBR. Luego, cada agente AI envía el afirmación performativo (α0 I) a los otros agentes. Por lo tanto, los agentes saben H0 = α0 I, ..., α0 n. Una vez que se han enviado todas las predicciones, el token se entrega al primer agente A1.2. En cada ronda T (que no sea 0), los agentes verifican si sus argumentos en HT están de acuerdo. Si lo hacen, el protocolo se mueve al paso 5. Además, si durante las últimas n rondas, ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también se mueve al paso 5. De lo contrario, el propietario del agente AI del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradicto (αT I) ⊆ HT (ver Sección 6.1). Luego, el contraargumento βT I contra la predicción αT J con la confianza más baja C (αT J) se selecciona (ya que αT J es más probable que la predicción se refutara con éxito).• Si βT I es un contraargumento, entonces, AI compara localmente αT I con βT I evaluando su confianza contra su CI de la base de casos individual (ver Sección 5) (observe que la IA está comparando su argumento anterior con el contraargumento de que la IA tiene sologenerado y eso es aproximadamente el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 979 SPONGE SPIKULE SKELETA CARACTERÍSTICAS EXTERNAS CARACTERÍSTICAS EXTERNAS: D2 Figura 4: Generación de un contraargumento usando la tapa en el conjunto de datos de Sponges.para enviar a AJ). Si Cai (βt i)> Cai (αT I), entonces AI considera que βT I es más fuerte que su argumento anterior, cambia su argumento a βT I enviando afirmación (βt I) al resto de los agentes (la intuición detrás de estoes que, dado que un contraargumento también es un argumento, la IA verifica si el recién contraargumento es un mejor argumento que el que estaba sosteniendo anteriormente) y refutar (βT I, αT J) a AJ. De lo contrario (es decir, CAI (βT I) ≤ Cai (αT I)), AI solo enviará refut (βt I, αT J) a AJ. En cualquiera de las dos situaciones, el protocolo se mueve al paso 3. • Si βT I es un contraejemplo C, entonces AI envía refut (C, αT J) a AJ. El protocolo se mueve al paso 4. • Si la IA no puede generar ningún argumento contra la contraarrato o el contraejemplo, el token se envía al siguiente agente, una nueva ronda T + 1 comienza y el protocolo se mueve al estado 2. 3. El agente AJ que ha recibido el contraargumento βT I, lo compara localmente con su propio argumento, αT J, evaluando localmente su confianza. Si Caj (βT I)> Caj (αT J), entonces AJ aceptará el contraargumento como más fuerte que su propio argumento, y enviará afirmación (βT I) a los otros agentes. De lo contrario (es decir, CAJ (βT I) ≤ Caj (αT J)), AJ no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones comienza una nueva ronda T + 1, AI envía el token al siguiente agente, y el protocolo regresa al estado 2. 4. El agente AJ que ha recibido el contraejemplo C lo retiene en su base de casos y genera un nuevo argumento αT+1 J que tiene en cuenta C, e informa al resto de los agentes enviando Assert (αT+1 J) a todos ellos. Luego, AI envía el token al siguiente agente, una nueva ronda T + 1 comienza y el protocolo regresa al paso 2. 5. El protocolo termina produciendo una predicción conjunta, de la siguiente manera: si los argumentos en HT están de acuerdo, entonces su predicción es la predicción conjunta, de lo contrario se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: s = arg max sk∈S αi∈Ht | αi.s = SK c (αi) Además, para evitar iteraciones infinitas, si un agente envía dos vecesEl mismo argumento o contraargumento al mismo agente, el mensaje no se considera.8. Ejemplificación consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1 recibe un problema P para resolver, y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, Hadromérida, D1 • α0 2 = A2, P, Astrophorida, D2 • α0 3 = A3, P, Axinellida, D3 A1 comienza a ser propietario del token e intenta generar los contraargumentos.Para α0 2 y α0 3, pero no tiene éxito, sin embargo, tiene un contraecomprador C13 para α0 3. Por lo tanto, A1 envía la refut del mensaje (C13, α0 3) a A3. A3 incorpora C13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta C13. A3 se le ocurre la predicción justificada α1 3 = A3, P, Hadromerida, D4, y la transmite al resto de los agentes con la afirmación del mensaje (α1 3). Por lo tanto, todos saben el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene la ficha. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo tiene éxito para generar un contraargumento β1 2 = A2, P, Astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con la refutación del mensaje (β1 2, α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento y, por lo tanto, H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 obtiene la ficha. A3 genera un contraargumento β2 3 = A3, P, Hadromerida, D6 para α0 2 y lo envía a A2 con la refutación del mensaje (β2 3, α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con la afirmación del mensaje (β2 3). Después de eso, H3 = α0 1, β2 3, α1 3. En la ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen Hadromérida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera a Hadromérida como la solución conjunta para el problema P. 9. Evaluación experimental 980 El sexto intl. Conf.sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) esponja 75 77 79 81 83 85 87 89 91 2 3 4 5 Amal Votación de soja individual 55 60 65 70 75 80 85 90 2 3 4 5 Votación Amal Figura 5: individual e individual e individual e individual e individual e individual e individual e individual e individualPrecisión conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacionales). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de la esponja tiene 280 ejemplos y 3 clases de soluciones. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de pruebas. Los ejemplos del conjunto de capacitación se distribuyen entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de prueba, los problemas en el conjunto de pruebas llegan al azar a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación;y (H2) que aprender de la comunicación mejora el desempeño individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a partir de la argumentación aumente a medida que aumenta el número de agentes que participan en la argumentación (ya que se tendrá en cuenta más información). Con respecto a H1 (la argumentación es un marco útil para la deliberación conjunta), ejecutamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que la capacitación siempre se distribuyeentre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponja y soja. La precisión de la clasificación se traza en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, votación y amal. La barra individual muestra la precisión promedio de las predicciones de agentes individuales;La barra de votación muestra la precisión promedio de la predicción conjunta lograda por votación pero sin ninguna argumentación;y finalmente la barra de Amal muestra la precisión promedio de la predicción conjunta utilizando la argumentación. Los resultados mostrados son el promedio de 5 corridas de validación cruzada de 10 veces. La Figura 5 muestra que la colaboración (votación y amal) supera a la resolución de problemas individuales. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera a la votación estándar, lo que demuestra que las decisiones conjuntas se basan en una mejor información según lo previsto por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de la esponja es de 87.57% para AMAL y 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por Amal sobre la votación es aún mayor en el conjunto de datos de la soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que Amal explota efectivamente la oportunidad de mejorar: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la votación). Con respecto a H2 (aprender de la comunicación en los procesos de argumentación mejora la predicción individual), ejecutamos el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de capacitación entre los cinco agentes;Después de eso, el resto de los casos en el conjunto de capacitación se envían a los agentes uno por uno;Cuando un agente recibe un nuevo caso de capacitación, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo o el agente puede usarlo para involucrar un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres parcelas, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje en absoluto;L (aprendizaje), muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden al retener los casos de capacitación que reciben individualmente (observe que cuando todos los casos de capacitación se han retenido al 100%, la precisión debe ser igual a la de la Figura 5 paraagentes individuales);Y finalmente, LFC (aprender de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden al retener los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de los ejemplos de capacitación como de los contraegumas). La Figura 6 muestra que si un agente AI aprende también de la comunicación, la IA puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (aquellos seleccionados como contraejemplos relevantes para la IA durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación versus una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de Sponges, los agentes han conservado solo muy pocos casos adicionales y mejoraron significativamente la precisión individual;a saber, conservan 59.96 casos en promedio (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja, se aprenden más contraejemplos para mejorar significativamente la precisión individual, a saber, conservan 87.16 casos en promedio (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para aprender de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más que los necesarios), y tienen el efecto previsto.10. Trabajo relacionado con respecto a CBR en un entorno de múltiples agentes, la primera investigación fue sobre la recuperación de casos negociados [11] entre los grupos de agentes. Nuestro trabajo sobre aprendizaje basado en casos de múltiples agentes comenzó en 1999 [6];Más tarde, MC Ginty y Smyth [7] presentaron un enfoque CBR colaborativo de múltiples agentes (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de bases múltiples (MCBR) [5], que se ocupa del sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL Soybean 20 30 40 50 60 70 90 25% 40% 55% 55%70% 85% 100% LFC L NL Figura 6: Aprendizaje de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes.Sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación base transversal. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de reutilización de CBR (usando un sistema de votación) mientras cada agente realiza la recuperación individualmente;Los otros enfoques de CBR multiagente, sin embargo, se centran en distribuir el proceso de recuperación. La investigación sobre la argumentación de MAS se centra en varios temas como A) lógicas, protocolos e idiomas que apoyan la argumentación, b) selección de argumentos e interpretación de argumentos. Los enfoques para la lógica y los idiomas que respaldan la argumentación incluyen lógica defensible [4] y modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre los argumentos. En nuestro marco hemos abordado las relaciones de selección de argumentos y preferencias utilizando un enfoque basado en casos.11. Conclusiones y trabajo futuro En este documento hemos presentado un marco basado en la argumentación para el aprendizaje de múltiples agentes. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje se pueden usar para generar argumentos y contraargumentos. La evaluación experimental muestra que la mayor cantidad de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, y especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para los agentes de aprendizaje;b) una relación de preferencia basada en casos sobre los argumentos, basada en calcular una estimación general de la confianza de los argumentos;c) una política basada en casos para generar argumentos contra la contraerrota y seleccionar contraejemplos;y d) un enfoque basado en la argumentación para aprender de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje conservaría todos los contraejemplos presentados por el otro agente;Sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales podrían mejorar significativamente usando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y las obras futuras tienen como objetivo incorporar un aprendizaje inductivo ansioso dentro del marco argumentativo para aprender de la comunicación.12. Referencias [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: problemas fundamentales, variaciones metodológicas y enfoques del sistema. Comunicaciones de inteligencia artificial, 7 (1): 39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje relacional basado en casos. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentos dinámicos: un modelo formal de procesos de argumentación basados en el cálculo de la situación. Journal of Logic and Computation, 11 (2): 257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación defensora utilizando sistemas deductivos etiquetados. Journal of Computer Science & Technology, 1 (4): 18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando automáticamente estrategias para el razonamiento de bases múltiples. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag.[6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. El conocimiento y la reutilización de la experiencia a través de las comunicaciones entre agentes competentes (pares). International Journal of Software Engineering and Knowledge Engineering, 9 (3): 319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento colaborativo basado en casos: aplicaciones en planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAi, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificación. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Artificial Intelligence Review, 24 (2): 145-161, 2005. [10] David Poole. Sobre la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M v Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidos. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzar acuerdos a través de la argumentación: un modelo lógico e implementación. Artificial Intelligence Journal, 104: 1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra. Agentes que razonan y negocian discutiendo. Journal of Logic and Computation, 8: 261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de los sistemas de software. ACM Crossroads, 5.1, 1998. 982 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07)",
    "original_sentences": [
        "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
        "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
        "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
        "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
        "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
        "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
        "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
        "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
        "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
        "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
        "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
        "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
        "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
        "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
        "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
        "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
        "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
        "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
        "Counterexamples offer the possibility of agents learning during the argumentation process.",
        "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
        "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
        "This paper presents a case-based approach to address both issues.",
        "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
        "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
        "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
        "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
        "The paper is structured as follows.",
        "Section 2 discusses the relation among argumentation, collaboration and learning.",
        "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
        "After that, Section 4 formally defines our argumentation framework.",
        "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
        "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
        "After that, Section 8 presents an exemplification of the argumentation framework.",
        "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
        "The paper closes with related work and conclusions sections. 2.",
        "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
        "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
        "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
        "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
        "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
        "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
        "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
        "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
        "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
        "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
        "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
        "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
        "A case base Ci = {c1, ..., cm} is a collection of cases.",
        "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
        "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
        "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
        "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
        "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
        "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
        "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
        "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
        "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
        "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
        "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
        "We are interested in justifications since they can be used as arguments.",
        "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
        "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
        "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
        "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
        "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
        "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
        "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
        "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
        "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
        "In the rest of the paper, we will use to denote the subsumption relation.",
        "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
        "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
        "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
        "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
        "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
        "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
        "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
        "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
        "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
        "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
        "A counterargument β is an argument offered in opposition to another argument α.",
        "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
        "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
        "A counterexample c is a case that contradicts an argument α.",
        "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
        "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
        "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
        "In the following sections we will present these elements. 5.",
        "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
        "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
        "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
        "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
        "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
        "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
        "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
        "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
        "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
        "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
        "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
        "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
        "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
        "Any learning method able to provide a justified prediction can be used to generate arguments.",
        "For instance, decision trees and LID [2] are suitable learning methods.",
        "Specifically, in the experiments reported in this paper agents use LID.",
        "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
        "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
        "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
        "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
        "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
        "Let us explain how they can be generated.",
        "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
        "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
        "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
        "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
        "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
        "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
        "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
        "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
        "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
        "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
        "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
        "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
        "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
        "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
        "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
        "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
        "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
        "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
        "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
        "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
        "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
        "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
        "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
        "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
        "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
        "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
        "The AMAL protocol consists on a series of rounds.",
        "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
        "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
        "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
        "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
        "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
        "When all the agents have had the token once, the token returns to the first agent, and so on.",
        "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
        "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
        "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
        "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
        "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
        "The protocol is initiated because one of the agents receives a problem P to be solved.",
        "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
        "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
        "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
        "Thus, the agents know H0 = α0 i , ..., α0 n .",
        "Once all the predictions have been sent the token is given to the first agent A1. 2.",
        "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
        "If they do, the protocol moves to step 5.",
        "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
        "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
        "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
        "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
        "Otherwise (i.e.",
        "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
        "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
        "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
        "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
        "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
        "Otherwise (i.e.",
        "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
        "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
        "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
        "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
        "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
        "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
        "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
        "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
        "For that reason, invites A2 and A3 to take part in the argumentation process.",
        "They accept the invitation, and the argumentation protocol starts.",
        "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
        "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
        "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
        "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
        "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
        "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
        "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
        "Round 1 starts and A2 gets the token.",
        "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
        "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
        "Agent A3 receives the counterargument and assesses its local confidence.",
        "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
        "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
        "Round 2 starts and A3 gets the token.",
        "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
        "Agent A2 receives the counterargument and assesses its local confidence.",
        "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
        "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
        "After that, H3 = α0 1, β2 3 , α1 3 .",
        "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
        "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
        "In this section we empirically evaluate the AMAL argumentation framework.",
        "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
        "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
        "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
        "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
        "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
        "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
        "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
        "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
        "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
        "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
        "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
        "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
        "The results shown are the average of 5 10-fold cross validation runs.",
        "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
        "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
        "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
        "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
        "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
        "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
        "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
        "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
        "Figure 6 shows the result of that experiment for the two data sets.",
        "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
        "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
        "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
        "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
        "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
        "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
        "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
        "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
        "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
        "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
        "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
        "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
        "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
        "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
        "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
        "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
        "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
        "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
        "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
        "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
        "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
        "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
        "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
        "Lazy induction of descriptions for relational case-based learning.",
        "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
        "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
        "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
        "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
        "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
        "Automatically selecting strategies for multi-case-base reasoning.",
        "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
        "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
        "Knowledge and experience reuse through communications among competent (peer) agents.",
        "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
        "Collaborative case-based reasoning: Applications in personalized route planning.",
        "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
        "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
        "Justification-based multiagent learning.",
        "In ICML2003, pages 576-583.",
        "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
        "The explanatory power of symbolic similarity in case-based reasoning.",
        "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
        "On the comparison of theories: Preferring the most specific explanation.",
        "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
        "Retrieval and reasoning in distributed case bases.",
        "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
        "Reaching agreements through argumentation: a logical model and implementation.",
        "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
        "Agents that reason and negotiate by arguing.",
        "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
        "Explanation component of software systems.",
        "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
    ],
    "error_count": 0,
    "keys": {
        "multi-agent system": {
            "translated_key": "sistema de múltiples agentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a <br>multi-agent system</br> composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Sistemas CBR de múltiples agentes Un sistema de razonamiento basado en casos de múltiples agentes (Mac) m = {(a1, c1), ..., (an, cn)} es un \"sistema de múltiples agentes\" compuesto de a = {ai,..., un}, un conjunto de agentes CBR, donde cada agente ai ∈ A posee una base de casos individual CI."
            ],
            "translated_text": "",
            "candidates": [
                "sistema de múltiples agentes",
                "sistema de múltiples agentes"
            ],
            "error": []
        },
        "argumentation framework": {
            "translated_key": "Marco de argumentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an <br>argumentation framework</br> for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an <br>argumentation framework</br> for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our <br>argumentation framework</br>.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the <br>argumentation framework</br>.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an <br>argumentation framework</br> for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an <br>argumentation framework</br> and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL <br>argumentation framework</br>.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an <br>argumentation framework</br> for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas de múltiples agentes Santi Ontañón CCL, Lab de computación cognitiva Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Consejo Español para la Investigación Científica para la Investigación CientíficaCampus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es Resumen En este documento presentaremos un \"Marco de argumentación\" para los agentes de aprendizaje (AMAL) diseñado para dos fines: (1) para el deliberación conjunta, y (2) para aprender de la comunicación.",
                "En este artículo presentaremos un \"marco de argumentación\" para los agentes de aprendizaje y mostraremos que puede usarse para dos fines: (1) deliberación conjunta y (2) aprendizaje de la comunicación.",
                "Después de eso, la Sección 4 define formalmente nuestro \"Marco de argumentación\".",
                "Después de eso, la Sección 8 presenta una ejemplificación del \"Marco de argumentación\".",
                "En este documento, propondremos a Amal, un \"marco de argumentación\" para los agentes de aprendizaje, y también mostraremos cómo se puede usar Amal tanto para aprender de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativaAl involucrar un proceso de argumentación sobre la predicción de la situación en cuestión.",
                "En el resto de este documento, propondremos un \"marco de argumentación\" y mostraremos cómo se puede usar tanto para aprender como para resolver problemas de manera colaborativa.3.",
                "En esta sección evaluamos empíricamente el \"Marco de argumentación\" de AMAL.",
                "Las principales contribuciones de este trabajo son: a) un \"marco de argumentación\" para los agentes de aprendizaje;b) una relación de preferencia basada en casos sobre los argumentos, basada en calcular una estimación general de la confianza de los argumentos;c) una política basada en casos para generar argumentos contra la contraerrota y seleccionar contraejemplos;y d) un enfoque basado en la argumentación para aprender de la comunicación."
            ],
            "translated_text": "",
            "candidates": [
                "Marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "marco de argumentación",
                "Marco de argumentación",
                "marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "Marco de argumentación",
                "marco de argumentación"
            ],
            "error": []
        },
        "learning agent": {
            "translated_key": "agente de aprendizaje",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a <br>learning agent</br> participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a <br>learning agent</br> would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación;y (H2) que aprender de la comunicación mejora el desempeño individual de un \"agente de aprendizaje\" que participa en un proceso de argumentación.",
                "Finalmente, en los experimentos presentados aquí, un \"agente de aprendizaje\" conservaría todos los contraejemplos presentados por el otro agente;Sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales podrían mejorar significativamente usando solo un pequeño conjunto de casos propuestos por otros agentes."
            ],
            "translated_text": "",
            "candidates": [
                "agente de aprendizaje",
                "agente de aprendizaje",
                "agente de aprendizaje",
                "agente de aprendizaje"
            ],
            "error": []
        },
        "learning from communication": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for <br>learning from communication</br>.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For <br>learning from communication</br>, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) <br>learning from communication</br>.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for <br>learning from communication</br> and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that <br>learning from communication</br> improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (<br>learning from communication</br> in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (<br>learning from communication</br>) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to <br>learning from communication</br>: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: <br>learning from communication</br> resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for <br>learning from communication</br>.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for <br>learning from communication</br>. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aprendizaje y deliberación conjunta a través de la argumentación en sistemas de múltiples agentes Santi Ontañón CCL, Lab de computación cognitiva Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Consejo Español para la Investigación Científica para la Investigación CientíficaCampus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es Resumen En este documento presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado para dos fines: (1) para la deliberación conjunta y (2) para\"Aprender de la comunicación\".",
                "Para \"aprender de la comunicación\", un agente se dedica a discutir con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos;El proceso de argumentación mejora su alcance de aprendizaje y su desempeño individual.",
                "En este artículo presentaremos un marco de argumentación para los agentes de aprendizaje y mostraremos que puede usarse para dos fines: (1) deliberación conjunta y (2) \"Aprender de la comunicación\".",
                "En este artículo, propondremos a Amal, un marco de argumentación para los agentes de aprendizaje, y también mostraremos cómo se puede usar Amal tanto para \"aprender de la comunicación\" como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativaAl involucrar un proceso de argumentación sobre la predicción de la situación en cuestión.",
                "Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación;y (H2) que \"aprender de la comunicación\" mejora el desempeño individual de un agente de aprendizaje que participa en un proceso de argumentación.",
                "Con respecto a H2 (\"aprender de la comunicación\" en los procesos de argumentación mejora la predicción individual), ejecutamos el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de capacitación entre los cinco agentes;Después de eso, el resto de los casos en el conjunto de capacitación se envían a los agentes uno por uno;Cuando un agente recibe un nuevo caso de capacitación, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo o el agente puede usarlo para involucrar un proceso de argumentación.",
                "La Figura 6 contiene tres parcelas, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje en absoluto;L (aprendizaje), muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden al retener los casos de capacitación que reciben individualmente (observe que cuando todos los casos de capacitación se han retenido al 100%, la precisión debe ser igual a la de la Figura 5 paraagentes individuales);Y finalmente, LFC (\"Aprender de la comunicación\") muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden al retener los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de los ejemplos de capacitación como de los contraexperimentos).",
                "Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para \"aprender de la comunicación\": los casos útiles se seleccionan como contraejemplos (y no más que los necesarios), y tienen elefecto previsto.10.",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL Soybean 20 30 40 50 60 70 90 25% 40% 55% 55%70% 85% 100% LFC L NL Figura 6: \"Aprendizaje de la comunicación\" resultante de la argumentación en un sistema compuesto por 5 agentes.Sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación base transversal.",
                "Las principales contribuciones de este trabajo son: a) un marco de argumentación para los agentes de aprendizaje;b) una relación de preferencia basada en casos sobre los argumentos, basada en calcular una estimación general de la confianza de los argumentos;c) una política basada en casos para generar argumentos contra la contraerrota y seleccionar contraejemplos;y d) un enfoque basado en la argumentación para \"aprender de la comunicación\".",
                "Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y las obras futuras tienen como objetivo incorporar un aprendizaje inductivo ansioso dentro del marco argumentativo para \"aprender de la comunicación\".12."
            ],
            "translated_text": "",
            "candidates": [
                "Aprender de la comunicación",
                "Aprender de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación",
                "Aprender de la comunicación",
                "Aprender de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación",
                "Aprender de la comunicación",
                "Aprender de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación",
                "Aprender de la comunicación",
                "Aprendizaje de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación",
                "Aprender de la comunicación",
                "aprender de la comunicación"
            ],
            "error": []
        },
        "joint deliberation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and <br>joint deliberation</br> through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for <br>joint deliberation</br>, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like <br>joint deliberation</br>, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) <br>joint deliberation</br>, and (2) learning from communication.",
                "Argumentation-based <br>joint deliberation</br> involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform <br>joint deliberation</br>, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a <br>joint deliberation</br> process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the <br>joint deliberation</br> ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (<br>joint deliberation</br>), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for <br>joint deliberation</br> and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for <br>joint deliberation</br>), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aprendizaje y \"deliberación conjunta\" a través de la argumentación en sistemas de múltiples agentes Santi Ontañón CCL, Laboratorio de computación cognitiva Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Consejo Español para el Consejo Español paraCampus de investigación científica UAB, 08193 bellaterra, Catalonia (España) enric@iiia.csic.es Resumen En este documento presentaremos un marco de argumentación para los agentes de aprendizaje (AMAL) diseñado para dos fines: (1) para la \"deliberación conjunta\", y(2) Para aprender de la comunicación.",
                "Introducción Los marcos de argumentación para sistemas de múltiples agentes pueden usarse para diferentes fines como \"deliberación conjunta\", persuasión, negociación y resolución de conflictos.",
                "En este artículo presentaremos un marco de argumentación para los agentes de aprendizaje y demostraremos que puede usarse para dos propósitos: (1) \"Deliberación conjunta\" y (2) aprendizaje de la comunicación.",
                "La \"deliberación conjunta\" basada en la argumentación implica la discusión sobre el resultado de una situación particular o el curso de acción apropiado para una situación particular.",
                "Por lo tanto, decimos que un grupo de agentes realiza una \"deliberación conjunta\", cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación.",
                "Al intercambiar argumentos y contraargumentos (incluidos los contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden involucrar un proceso de \"deliberación conjunta\".",
                "Si el proceso de argumentación llega a una solución consensuada, termina la \"deliberación conjunta\";De lo contrario, se utiliza un voto ponderado para determinar la solución articular.",
                "Además, si al final de la argumentación, los agentes no han llegado a un acuerdo, entonces un mecanismo de votación que usa la confianza de cada predicción como pesas se usa para decidir la solución final (por lo tanto, Amal sigue el mismo mecanismo que los comités humanos, primeroCada miembro individual de un comité expone sus argumentos y discuso los de los otros miembros (\"deliberación conjunta\"), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación).",
                "Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la \"deliberación conjunta\" y puede mejorar sobre otros métodos típicos como la votación;y (H2) que aprender de la comunicación mejora el desempeño individual de un agente de aprendizaje que participa en un proceso de argumentación.",
                "Con respecto a H1 (la argumentación es un marco útil para la \"deliberación conjunta\"), ejecutamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que la capacitación essiempre distribuido entre 5 agentes)."
            ],
            "translated_text": "",
            "candidates": [
                "deliberación articular",
                "deliberación conjunta",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "Deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta",
                "deliberación articular",
                "deliberación conjunta"
            ],
            "error": []
        },
        "argumentation protocol": {
            "translated_key": "Protocolo de argumentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an <br>argumentation protocol</br> inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the <br>argumentation protocol</br> in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the <br>argumentation protocol</br> starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Proponemos un \"protocolo de argumentación\" dentro del marco AMAL en los agentes de los apoyos para alcanzar una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para apoyar el proceso de argumentación también se basará en los casos.",
                "Más tarde, la Sección 7 presenta el \"Protocolo de argumentación\" en nuestro marco AMAL.",
                "Aceptan la invitación y comienza el \"Protocolo de argumentación\"."
            ],
            "translated_text": "",
            "candidates": [
                "Protocolo de argumentación",
                "protocolo de argumentación",
                "Protocolo de argumentación",
                "Protocolo de argumentación",
                "Protocolo de argumentación",
                "Protocolo de argumentación"
            ],
            "error": []
        },
        "collaboration": {
            "translated_key": "colaboración",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, <br>collaboration</br> and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,<br>collaboration</br> AND LEARNING Both learning and <br>collaboration</br> are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and <br>collaboration</br> in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and <br>collaboration</br> are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and <br>collaboration</br> in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this <br>collaboration</br>, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since <br>collaboration</br> is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that <br>collaboration</br> (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La Sección 2 discute la relación entre la argumentación, la \"colaboración\" y el aprendizaje.",
                "La argumentación, la \"colaboración\" y el aprendizaje tanto del aprendizaje como de la \"colaboración\" son formas en que un agente puede mejorar el rendimiento individual.",
                "De hecho, existe un claro paralelismo entre el aprendizaje y la \"colaboración\" en los sistemas de múltiples agentes, ya que ambas son formas en que los agentes pueden lidiar con sus deficiencias.",
                "Mirando las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la \"colaboración\" están muy relacionados en los sistemas de múltiples agentes.",
                "Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, colaborando o encontrando un punto intermedio que combine el aprendizaje y la \"colaboración\" para mejorar el rendimiento.",
                "Usando esta \"colaboración\", la predicción se puede hacer de una manera más informada, ya que la información conocida por varios agentes ha tenido en cuenta.• Los agentes también pueden aprender de la comunicación con otros agentes involucrando un proceso de argumentación.",
                "Además, uno puede pensar que sería mejor que los agentes generen contraargumentos basados en la relación de preferencia de confianza conjunta;Sin embargo, no es obvio cómo generar argumentos contra la confianza basada en la confianza conjunta de una manera eficiente, ya que se requiere \"colaboración\" para evaluar la confianza conjunta.",
                "La Figura 5 muestra que la \"colaboración\" (votación y amal) supera a la resolución de problemas individuales."
            ],
            "translated_text": "",
            "candidates": [
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración",
                "colaboración"
            ],
            "error": []
        },
        "group": {
            "translated_key": "grupo",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a <br>group</br> of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the <br>group</br>: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a <br>group</br> of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a <br>group</br> of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por lo tanto, decimos que un \"grupo\" de los agentes realiza una deliberación conjunta, cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación.",
                "La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, tres casos de respaldo y un contraejemplo se encuentran en la base de casos de IA de los agentes, dando una confianza estimada de 0.6 Además, también podemos definir la confianza conjunta de un argumentoα a medida que la confianza se calculó utilizando los casos presentes en las bases de casos de todos los agentes en el \"grupo\": c (α) = i y ai α 1 + i y ai α + nai α observa que, calcular colaborativamente la confianza de la articulación, los agentes solo tienen que hacer públicos los valores de AYE y no calculados localmente para un argumento determinado.",
                "Aprendizaje múltiple basado en argumentación El protocolo de interacción de AMAL permite que un \"grupo\" de los agentes A1 ..., y deliberaran sobre la solución correcta de un problema P mediante un proceso de argumentación.",
                "Específicamente, hemos presentado AMAL, un marco que permite que un \"grupo\" de agentes de aprendizaje discutan sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje se pueden usar para generar argumentos y contraargumentos."
            ],
            "translated_text": "",
            "candidates": [
                "grupo",
                "grupo",
                "grupo",
                "grupo",
                "grupo",
                "grupo",
                "grupo",
                "grupo"
            ],
            "error": []
        },
        "predictive accuracy": {
            "translated_key": "precisión predictiva",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their <br>predictive accuracy</br>, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La evaluación experimental muestra que la mayor cantidad de información proporcionada a los agentes por el proceso de argumentación aumenta su \"precisión predictiva\", y especialmente cuando un número adecuado de agentes participa en la argumentación."
            ],
            "translated_text": "",
            "candidates": [
                "precisión predictiva",
                "precisión predictiva"
            ],
            "error": []
        },
        "case-based policy": {
            "translated_key": "política basada en casos",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a <br>case-based policy</br> to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Las principales contribuciones de este trabajo son: a) un marco de argumentación para los agentes de aprendizaje;b) una relación de preferencia basada en casos sobre los argumentos, basada en calcular una estimación general de la confianza de los argumentos;c) una \"Política basada en casos\" para generar argumentos en contra y seleccionar contraejemplos;y d) un enfoque basado en la argumentación para aprender de la comunicación."
            ],
            "translated_text": "",
            "candidates": [
                "política basada en casos",
                "Política basada en casos"
            ],
            "error": []
        },
        "multi-agent learn": {
            "translated_key": "aprendizaje de múltiples agentes",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for <br>multi-agent learn</br>ing.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Conclusiones y trabajo futuro En este documento hemos presentado un marco basado en la argumentación para \"aprendizaje de múltiples agentes\"."
            ],
            "translated_text": "",
            "candidates": [
                "aprendizaje de múltiples agentes",
                "aprendizaje de múltiples agentes"
            ],
            "error": []
        },
        "argumentation": {
            "translated_key": "argumentación",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through <br>argumentation</br> in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an <br>argumentation</br> framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the <br>argumentation</br> among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the <br>argumentation</br> process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION <br>argumentation</br> frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an <br>argumentation</br> framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "<br>argumentation</br>-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an <br>argumentation</br> process.",
                "Most existing <br>argumentation</br> frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support <br>argumentation</br>, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based <br>argumentation</br> frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an <br>argumentation</br>-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the <br>argumentation</br> process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an <br>argumentation</br> protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the <br>argumentation</br> process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if <br>argumentation</br> between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the <br>argumentation</br> process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among <br>argumentation</br>, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our <br>argumentation</br> framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the <br>argumentation</br> protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the <br>argumentation</br> framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "<br>argumentation</br>COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an <br>argumentation</br> framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an <br>argumentation</br> process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an <br>argumentation</br> process.",
                "Agents that engage in such <br>argumentation</br> processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an <br>argumentation</br> framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an <br>argumentation</br> process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in <br>argumentation</br> processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for <br>argumentation</br>, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "<br>argumentation</br>-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an <br>argumentation</br> process.",
                "If the <br>argumentation</br> process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the <br>argumentation</br> the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the <br>argumentation</br> process.",
                "They accept the invitation, and the <br>argumentation</br> protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL <br>argumentation</br> framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that <br>argumentation</br> is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an <br>argumentation</br> process.",
                "Moreover, we also expect that the improvement achieved from <br>argumentation</br> will increase as the number of agents participating in the <br>argumentation</br> increases (since more information will be taken into account).",
                "Concerning H1 (<br>argumentation</br> is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the <br>argumentation</br> processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any <br>argumentation</br>; and finally the AMAL bar shows the average accuracy of the joint prediction using <br>argumentation</br>.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the <br>argumentation</br> process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during <br>argumentation</br> (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in <br>argumentation</br> processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an <br>argumentation</br> process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during <br>argumentation</br> (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during <br>argumentation</br>).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the <br>argumentation</br>-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-case-base reasoning (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from <br>argumentation</br> in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS <br>argumentation</br> focus on several issues like a) logics, protocols and languages that support <br>argumentation</br>, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support <br>argumentation</br> include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated <br>argumentation</br> (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an <br>argumentation</br>-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the <br>argumentation</br> process increases their predictive accuracy, and specially when an adequate number of agents take part in the <br>argumentation</br>.",
                "The main contributions of this work are: a) an <br>argumentation</br> framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an <br>argumentation</br>-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of <br>argumentation</br> processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible <br>argumentation</br> using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-case-base reasoning.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through <br>argumentation</br>: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Aprendizaje y deliberación conjunta a través de \"argumentación\" en sistemas de múltiples agentes Santi Ontañón CCL, Laboratorio de computación cognitiva Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación de Investigación de Inteligencia Artificial, Consejo Español para el Consejo Español paraCampus de investigación científica UAB, 08193 Bellaterra, Catalonia (España) enric@iiia.csic.es Resumen En este documento presentaremos un marco de \"argumentación\" para los agentes de aprendizaje (AMAL) diseñado para dos fines: (1) para la deliberación conjunta y(2) Para aprender de la comunicación.",
                "Mostramos experimentalmente que la \"argumentación\" entre los comités de los agentes mejora el desempeño individual y conjunto.",
                "Para aprender de la comunicación, un agente se dedica a discutir con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos;El proceso de \"argumentación\" mejora su alcance de aprendizaje y su desempeño individual.",
                "Introducción Los marcos de \"argumentación\" para sistemas de múltiples agentes pueden usarse para diferentes fines como deliberación conjunta, persuasión, negociación y resolución de conflictos.",
                "En este artículo presentaremos un marco de \"argumentación\" para los agentes de aprendizaje y mostraremos que puede usarse para dos fines: (1) deliberación conjunta y (2) aprendizaje de la comunicación.",
                "La deliberación conjunta basada en \"argumentación\" implica la discusión sobre el resultado de una situación particular o el curso de acción apropiado para una situación particular.",
                "Por lo tanto, los agentes de aprendizaje que son capaces de discutir sus predicciones individuales con otros agentes pueden alcanzar una mejor precisión de la predicción después de tal proceso de \"argumentación\".",
                "La mayoría de los marcos de \"argumentación\" existentes para sistemas de múltiples agentes se basan en la lógica deductiva o en algún otro formalismo lógico deductivo diseñado específicamente para respaldar la \"argumentación\", como la lógica predeterminada [3]).",
                "Sin embargo, los marcos de \"argumentación\" basados en la lógica asumen agentes con conocimiento precargado y relación de preferencia.",
                "En este artículo, nos centramos en un marco de aprendizaje múltiple (AMAL) basado en \"argumentación\" donde se aprende tanto el conocimiento como la relación de preferencia de la experiencia.",
                "Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de \"argumentación\".",
                "Proponemos un protocolo de \"argumentación\" dentro del marco AMAL en los agentes de los apoyos para alcanzar una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de \"argumentación\" también se basará en los casos.",
                "Finalmente, evaluamos (1) si la \"argumentación\" entre los agentes de aprendizaje puede producir una predicción conjunta que mejora sobre el rendimiento del aprendizaje individual y (2) si el aprendizaje de los contraejemplos transmitidos durante el proceso de \"argumentación\" aumenta el rendimiento individual con precisamente esos casos que son casosusado mientras discute entre ellos.",
                "La Sección 2 discute la relación entre \"argumentación\", colaboración y aprendizaje.",
                "Después de eso, la Sección 4 define formalmente nuestro marco de \"argumentación\".",
                "Más tarde, la Sección 7 presenta el protocolo de \"argumentación\" en nuestro marco AMAL.",
                "Después de eso, la Sección 8 presenta una ejemplificación del marco de \"argumentación\".",
                "La colaboración y el aprendizaje de \"argumentación\" tanto el aprendizaje como la colaboración son formas en que un agente puede mejorar el rendimiento individual.",
                "En este documento, propondremos a Amal, un marco de \"argumentación\" para los agentes de aprendizaje, y también mostraremos cómo se puede usar Amal tanto para aprender de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativaAl involucrar un proceso de \"argumentación\" sobre la predicción de la situación en cuestión.",
                "Usando esta colaboración, la predicción se puede hacer de una manera más informada, ya que la información conocida por varios agentes se ha tenido en cuenta.• Los agentes también pueden aprender de la comunicación con otros agentes involucrando un proceso de \"argumentación\".",
                "Los agentes que participan en tales procesos de \"argumentación\" pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y usar esta información para predecir los resultados de situaciones futuras.",
                "En el resto de este documento, propondremos un marco de \"argumentación\" y mostraremos cómo se puede usar tanto para aprender como para resolver problemas de manera colaborativa.3.",
                "Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta, cuando colaboran para encontrar una solución conjunta mediante un proceso de \"argumentación\".",
                "En este documento, vamos a utilizar las justificaciones como argumentos, para permitir que los agentes de aprendizaje participen en procesos de \"argumentación\".4.",
                "El criterio de especificidad se usa ampliamente en los marcos deductivos para la \"argumentación\", y afirma que entre dos argumentos conflictivos, los más específicos deben preferirse, ya que es, en principio, más informado.",
                "El aprendizaje de múltiples agentes basado en \"argumentación\".",
                "Si el proceso de \"argumentación\" llega a una solución consensuada, la deliberación conjunta finaliza;De lo contrario, se utiliza un voto ponderado para determinar la solución articular.",
                "Además, si al final de la \"argumentación\", los agentes no han llegado a un acuerdo, entonces un mecanismo de votación que usa la confianza de cada predicción como pesas se usa para decidir la solución final (por lo tanto, Amal sigue el mismo mecanismo que los comités humanos, primero, cada miembro individual de un comité expone sus argumentos y descubre los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación).",
                "Por esa razón, invita a A2 y A3 a participar en el proceso de \"argumentación\".",
                "Aceptan la invitación y comienza el protocolo de \"argumentación\".",
                "En esta sección evaluamos empíricamente el marco de \"argumentación\" amal.",
                "Los experimentos están diseñados para probar dos hipótesis: (H1) que la \"argumentación\" es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación;y (H2) que aprender de la comunicación mejora el desempeño individual de un agente de aprendizaje que participa en un proceso de \"argumentación\".",
                "Además, también esperamos que la mejora lograda de la \"argumentación\" aumente a medida que aumenta el número de agentes que participan en la \"argumentación\" (ya que se tendrá en cuenta más información).",
                "Con respecto a H1 (\"argumentación\" es un marco útil para la deliberación conjunta), ejecutamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que la capacitación essiempre distribuido entre 5 agentes).",
                "La precisión de la clasificación se traza en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de \"argumentación\".",
                "La barra individual muestra la precisión promedio de las predicciones de agentes individuales;La barra de votación muestra la precisión promedio de la predicción conjunta lograda por votación pero sin ninguna \"argumentación\";y finalmente la barra de Amal muestra la precisión promedio de la predicción conjunta utilizando \"argumentación\".",
                "También podemos ver que AMAL siempre supera a la votación estándar, lo que demuestra que las decisiones conjuntas se basan en una mejor información según lo proporcionado por el proceso de \"argumentación\".",
                "Estos resultados experimentales muestran que Amal explota efectivamente la oportunidad de mejorar: la precisión es mayor solo porque más agentes han cambiado su opinión durante la \"argumentación\" (de lo contrario, lograrían el mismo resultado que la votación).",
                "Con respecto a H2 (aprender de la comunicación en los procesos de \"argumentación\" mejora la predicción individual), ejecutamos el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de capacitación entre los cinco agentes;Después de eso, el resto de los casos en el conjunto de capacitación se envían a los agentes uno por uno;Cuando un agente recibe un nuevo caso de capacitación, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo o el agente puede usarlo para involucrar un proceso de \"argumentación\".",
                "La Figura 6 contiene tres parcelas, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje en absoluto;L (aprendizaje), muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden al retener los casos de capacitación que reciben individualmente (observe que cuando todos los casos de capacitación se han retenido al 100%, la precisión debe ser igual a la de la Figura 5 paraagentes individuales);Y finalmente, LFC (aprender de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden al retener los contraejemplos recibidos durante la \"argumentación\" (es decir, aprenden tanto de ejemplos de capacitación como de contraejemplos).",
                "La Figura 6 muestra que si un agente AI aprende también de la comunicación, la IA puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (aquellos seleccionados como contraejemplos relevantes para la IA durante la \"argumentación\").",
                "Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la \"argumentación\" para aprender de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más que los necesarios), y tienen elefecto previsto.10.",
                "Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL Soybean 20 30 40 50 60 70 90 25% 40% 55% 55%70% 85% 100% LFC L NL Figura 6: Aprender de la comunicación resultante de la \"argumentación\" en un sistema compuesto por 5 agentes.Sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación base transversal.",
                "La investigación sobre la \"argumentación\" de MAS se centra en varios temas como A) lógicas, protocolos e idiomas que apoyan la \"argumentación\", b) selección de argumentos e interpretación de argumentos.",
                "Los enfoques para la lógica y los idiomas que respaldan la \"argumentación\" incluyen lógica defensible [4] y modelos BDI [13].",
                "Aunque la selección de argumentos es un aspecto clave de la \"argumentación\" automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre los argumentos.",
                "Conclusiones y trabajo futuro En este documento hemos presentado un marco basado en \"argumentación\" para el aprendizaje de múltiples agentes.",
                "La evaluación experimental muestra que la mayor cantidad de información proporcionada a los agentes por el proceso de \"argumentación\" aumenta su precisión predictiva, y especialmente cuando un número adecuado de agentes participa en la \"argumentación\".",
                "Las principales contribuciones de este trabajo son: a) un marco de \"argumentación\" para los agentes de aprendizaje;b) una relación de preferencia basada en casos sobre los argumentos, basada en calcular una estimación general de la confianza de los argumentos;c) una política basada en casos para generar argumentos contra la contraerrota y seleccionar contraejemplos;y d) un enfoque basado en la \"argumentación\" para aprender de la comunicación.",
                "Sistemas de argumentos dinámicos: un modelo formal de procesos de \"argumentación\" basados en el cálculo de la situación.",
                "Formalizando la \"argumentación\" defensible utilizando sistemas deductivos etiquetados.",
                "Alcanzar acuerdos a través de \"argumentación\": un modelo lógico e implementación."
            ],
            "translated_text": "",
            "candidates": [
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "Argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "Argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "Argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación",
                "argumentación"
            ],
            "error": []
        },
        "case-base reason": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems Santi Ontañón CCL, Cognitive Computing Lab Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Artificial Intelligence Research Institute CSIC, Spanish Council for Scientific Research Campus UAB, 08193 Bellaterra, Catalonia (Spain) enric@iiia.csic.es ABSTRACT In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication.",
                "The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques.",
                "For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision.",
                "We experimentally show that the argumentation among committees of agents improves both the individual and joint performance.",
                "For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.",
                "Categories and Subject Descriptors I.2.6 [Artificial Intelligence]: Learning; I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-Multiagent systems, Intelligent Agents 1.",
                "INTRODUCTION Argumentation frameworks for multi-agent systems can be used for different purposes like joint deliberation, persuasion, negotiation, and conflict resolution.",
                "In this paper we will present an argumentation framework for learning agents, and show that it can be used for two purposes: (1) joint deliberation, and (2) learning from communication.",
                "Argumentation-based joint deliberation involves discussion over the outcome of a particular situation or the appropriate course of action for a particular situation.",
                "Learning agents are capable of learning from experience, in the sense that past examples (situations and their outcomes) are used to predict the outcome for the situation at hand.",
                "However, since individual agents experience may be limited, individual knowledge and prediction accuracy is also limited.",
                "Thus, learning agents that are capable of arguing their individual predictions with other agents may reach better prediction accuracy after such an argumentation process.",
                "Most existing argumentation frameworks for multi-agent systems are based on deductive logic or some other deductive logic formalism specifically designed to support argumentation, such as default logic [3]).",
                "Usually, an argument is seen as a logical statement, while a counterargument is an argument offered in opposition to another argument [4, 13]; agents use a preference relation to resolve conflicting arguments.",
                "However, logic-based argumentation frameworks assume agents with preloaded knowledge and preference relation.",
                "In this paper, we focus on an Argumentation-based Multi-Agent Learning (AMAL) framework where both knowledge and preference relation are learned from experience.",
                "Thus, we consider a scenario with agents that (1) work in the same domain using a shared ontology, (2) are capable of learning from examples, and (3) communicate using an argumentative framework.",
                "Having learning capabilities allows agents effectively use a specific form of counterargument, namely the use of counterexamples.",
                "Counterexamples offer the possibility of agents learning during the argumentation process.",
                "Moreover, learning agents allow techniques that use learnt experience to generate adequate arguments and counterarguments.",
                "Specifically, we will need to address two issues: (1) how to define a technique to generate arguments and counterarguments from examples, and (2)how to define a preference relation over two conflicting arguments that have been induced from examples.",
                "This paper presents a case-based approach to address both issues.",
                "The agents use case-based reasoning (CBR) [1] to learn from past cases (where a case is a situation and its outcome) in order to predict the outcome of a new situation.",
                "We propose an argumentation protocol inside the AMAL framework at supports agents in reaching a joint prediction over a specific situation or problem - moreover, the reasoning needed to support the argumentation process will also be based on cases.",
                "In particular, we present two case-based measures, one for generating the arguments and counterarguments adequate to a particular situation and another for determining preference relation among arguments.",
                "Finally, we evaluate (1) if argumentation between learning agents can produce a joint prediction that improves over individual learning performance and (2) if learning from the counterexamples conveyed during the argumentation process increases the individual performance with precisely those cases being used while arguing among them.",
                "The paper is structured as follows.",
                "Section 2 discusses the relation among argumentation, collaboration and learning.",
                "Then Section 3 introduces our multi-agent CBR (MAC) framework and the notion of justified prediction.",
                "After that, Section 4 formally defines our argumentation framework.",
                "Sections 5 and 6 present our case-based preference relation and argument generation policies respectively.",
                "Later, Section 7 presents the argumentation protocol in our AMAL framework.",
                "After that, Section 8 presents an exemplification of the argumentation framework.",
                "Finally, Section 9 presents an empirical evaluation of our two main hypotheses.",
                "The paper closes with related work and conclusions sections. 2.",
                "ARGUMENTATION,COLLABORATION AND LEARNING Both learning and collaboration are ways in which an agent can improve individual performance.",
                "In fact, there is a clear parallelism between learning and collaboration in multi-agent systems, since both are ways in which agents can deal with their shortcomings.",
                "Let us show which are the main motivations that an agent can have to learn or to collaborate. • Motivations to learn: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems. • Motivations to collaborate: - Increase quality of prediction, - Increase efficiency, - Increase the range of solvable problems, - Increase the range of accessible resources.",
                "Looking at the above lists of motivation, we can easily see that learning and collaboration are very related in multi-agent systems.",
                "In fact, with the exception of the last item in the motivations to collaborate list, they are two extremes of a continuum of strategies to improve performance.",
                "An agent may choose to increase performance by learning, by collaborating, or by finding an intermediate point that combines learning and collaboration in order to improve performance.",
                "In this paper we will propose AMAL, an argumentation framework for learning agents, and will also also show how AMAL can be used both for learning from communication and for solving problems in a collaborative way: • Agents can solve problems in a collaborative way via engaging an argumentation process about the prediction for the situation at hand.",
                "Using this collaboration, the prediction can be done in a more informed way, since the information known by several agents has been taken into account. • Agents can also learn from communication with other agents by engaging an argumentation process.",
                "Agents that engage in such argumentation processes can learn from the arguments and counterexamples received from other agents, and use this information for predicting the outcomes of future situations.",
                "In the rest of this paper we will propose an argumentation framework and show how it can be used both for learning and for solving problems in a collaborative way. 3.",
                "MULTI-AGENT CBR SYSTEMS A Multi-Agent Case Based Reasoning System (MAC) M = {(A1, C1), ..., (An, Cn)} is a multi-agent system composed of A = {Ai, ..., An}, a set of CBR agents, where each agent Ai ∈ A possesses an individual case base Ci.",
                "Each individual agent Ai in a MAC is completely autonomous and each agent Ai has access only to its individual and private case base Ci.",
                "A case base Ci = {c1, ..., cm} is a collection of cases.",
                "Agents in a MAC system are able to individually solve problems, but they can also collaborate with other agents to solve problems.",
                "In this framework, we will restrict ourselves to analytical tasks, i.e. tasks like classification, where the solution of a problem is achieved by selecting a solution class from an enumerated set of solution classes.",
                "In the following we will note the set of all the solution classes by S = {S1, ..., SK }.",
                "Therefore, a case c = P, S is a tuple containing a case description P and a solution class S ∈ S. In the following, we will use the terms problem and case description indistinctly.",
                "Moreover, we will use the dot notation to refer to elements inside a tuple; e.g., to refer to the solution class of a case c, we will write c.S.",
                "Therefore, we say a group of agents perform joint deliberation, when they collaborate to find a joint solution by means of an argumentation process.",
                "However, in order to do so, an agent has to be able to justify its prediction to the other agents (i.e. generate an argument for its predicted solution that can be examined and critiqued by the other agents).",
                "The next section addresses this issue. 3.1 Justified Predictions Both expert systems and CBR systems may have an explanation component [14] in charge of justifying why the system has provided a specific answer to the user.",
                "The line of reasoning of the system can then be examined by a human expert, thus increasing the reliability of the system.",
                "Most of the existing work on explanation generation focuses on generating explanations to be provided to the user.",
                "However, in our approach we use explanations (or justifications) as a tool for improving communication and coordination among agents.",
                "We are interested in justifications since they can be used as arguments.",
                "For that purpose, we will benefit from the ability of some machine learning methods to provide justifications.",
                "A justification built by a CBR method after determining that the solution of a particular problem P was Sk is a description that contains the relevant information from the problem P that the CBR method has considered to predict Sk as the solution of P. In particular, CBR methods work by retrieving similar cases to the problem at hand, and then reusing their solutions for the current problem, expecting that since the problem and the cases are similar, the solutions will also be similar.",
                "Thus, if a CBR method has retrieved a set of cases C1, ..., Cn to solve a particular problem P the justification built will contain the relevant information from the problem P that made the CBR system retrieve that particular set of cases, i.e. it will contain the relevant information that P and C1, ..., Cn have in common.",
                "For example, Figure 1 shows a justification build by a CBR system for a toy problem (in the following sections we will show justifications for real problems).",
                "In the figure, a problem has two attributes (Traffic_light, and Cars_passing), the retrieval mechanism of the CBR system notices that by considering only the attribute Traffic_light, it can retrieve two cases that predict the same solution: wait.",
                "Thus, since only this attribute has been used, it is the only one appearing in the justification.",
                "The values of the rest of attributes are irrelevant, since whatever their value the solution class would have been the same. 976 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Problem Traffic_light: red Cars_passing: no Case 1 Traffic_light: red Cars_passing: no Solution: wait Case 3 Traffic_light: red Cars_passing: yes Solution: wait Case 4 Traffic_light: green Cars_passing: yes Solution: wait Case 2 Traffic_light: green Cars_passing: no Solution: cross Retrieved cases Solution: wait Justification Traffic_light: red Figure 1: An example of justification generation in a CBR system.",
                "Notice that, since the only relevant feature to decide is Traffic_light (the only one used to retrieve cases), it is the only one appearing in the justification.",
                "In general, the meaning of a justification is that all (or most of) the cases in the case base of an agent that satisfy the justification (i.e. all the cases that are subsumed by the justification) belong to the predicted solution class.",
                "In the rest of the paper, we will use to denote the subsumption relation.",
                "In our work, we use LID [2], a CBR method capable of building symbolic justifications such as the one exemplified in Figure 1.",
                "When an agent provides a justification for a prediction, the agent generates a justified prediction: DEFINITION 3.1.",
                "A Justified Prediction is a tuple J = A, P, S, D where agent A considers S the correct solution for problem P, and that prediction is justified a symbolic description D such that J.D J.P. Justifications can have many uses for CBR systems [8, 9].",
                "In this paper, we are going to use justifications as arguments, in order to allow learning agents to engage in argumentation processes. 4.",
                "ARGUMENTS AND COUNTERARGUMENTS For our purposes an argument α generated by an agent A is composed of a statement S and some evidence D supporting S as correct.",
                "In the remainder of this section we will see how this general definition of argument can be instantiated in specific kind of arguments that the agents can generate.",
                "In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .",
                "Using this information, we can define three types of arguments: justified predictions, counterarguments, and counterexamples.",
                "A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.",
                "In the example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.",
                "A counterargument β is an argument offered in opposition to another argument α.",
                "In our framework, a counterargument consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .",
                "In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) , meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.",
                "A counterexample c is a case that contradicts an argument α.",
                "Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence provided is the case c. Specifically, for a case c to be a counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e. the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.",
                "By exchanging arguments and counterarguments (including counterexamples), agents can argue about the correct solution of a given problem, i.e. they can engage a joint deliberation process.",
                "However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a decision policy to generate counterarguments (including counterexamples).",
                "In the following sections we will present these elements. 5.",
                "PREFERENCE RELATION A specific argument provided by an agent might not be consistent with the information known to other agents (or even to some of the information known by the agent that has generated the justification due to noise in training data).",
                "For that reason, we are going to define a preference relation over contradicting justified predictions based on cases.",
                "Basically, we will define a confidence measure for each justified prediction (that takes into account the cases owned by each agent), and the justified prediction with the highest confidence will be the preferred one.",
                "The idea behind case-based confidence is to count how many of the cases in an individual case base endorse a justified prediction, and how many of them are counterexamples of it.",
                "The more the endorsing cases, the higher the confidence; and the more the counterexamples, the lower the confidence.",
                "Specifically, to assess the confidence of a justified prediction α, an agent obtains the set of cases in its individual case base that are subsumed by α.D.",
                "With them, an agent Ai obtains the Y (aye) and N (nay) values: • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by the justification α.D that belong to the solution class α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| is the number of cases in the agents case base subsumed by justification α.D that do not belong to that solution class.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 977 + + + + + +  - - + Figure 2: Confidence of arguments is evaluated by contrasting them against the case bases of the agents.",
                "An agent estimates the confidence of an argument as: CAi (α) = Y Ai α 1 + Y Ai α + NAi α i.e. the confidence on a justified prediction is the number of endorsing cases divided by the number of endorsing cases plus counterexamples.",
                "Notice that we add 1 to the denominator, this is to avoid giving excessively high confidences to justified predictions whose confidence has been computed using a small number of cases.",
                "Notice that this correction follows the same idea than the Laplace correction to estimate probabilities.",
                "Figure 2 illustrates the individual evaluation of the confidence of an argument, in particular, three endorsing cases and one counterexample are found in the case base of agents Ai, giving an estimated confidence of 0.6 Moreover, we can also define the joint confidence of an argument α as the confidence computed using the cases present in the case bases of all the agents in the group: C(α) = i Y Ai α 1 + i Y Ai α + NAi α Notice that, to collaboratively compute the joint confidence, the agents only have to make public the aye and nay values locally computed for a given argument.",
                "In our framework, agents use this joint confidence as the preference relation: a justified prediction α is preferred over another one β if C(α) ≥ C(β). 6.",
                "GENERATION OF ARGUMENTS In our framework, arguments are generated by the agents from cases, using learning methods.",
                "Any learning method able to provide a justified prediction can be used to generate arguments.",
                "For instance, decision trees and LID [2] are suitable learning methods.",
                "Specifically, in the experiments reported in this paper agents use LID.",
                "Thus, when an agent wants to generate an argument endorsing that a specific solution class is the correct solution for a problem P, it generates a justified prediction as explained in Section 3.1.",
                "For instance, Figure 3 shows a real justification generated by LID after solving a problem P in the domain of marine sponges identification.",
                "In particular, Figure 3 shows how when an agent receives a new problem to solve (in this case, a new sponge to determine its order), the agent uses LID to generate an argument (consisting on a justified prediction) using the cases in the case base of the agent.",
                "The justification shown in Figure 3 can be interpreted saying that the predicted solution is hadromerida because the smooth form of the megascleres of the spiculate skeleton of the sponge is of type tylostyle, the spikulate skeleton of the sponge has no uniform length, and there is no gemmules in the external features of the sponge.",
                "Thus, the argument generated will be α = A1, P, hadromerida, D1 . 6.1 Generation of Counterarguments As previously stated, agents may try to rebut arguments by generating counterargument or by finding counterexamples.",
                "Let us explain how they can be generated.",
                "An agent Ai wants to generate a counterargument β to rebut an argument α when α is in contradiction with the local case base of Ai.",
                "Moreover, while generating such counterargument β, Ai expects that β is preferred over α.",
                "For that purpose, we will present a specific policy to generate counterarguments based on the specificity criterion [10].",
                "The specificity criterion is widely used in deductive frameworks for argumentation, and states that between two conflicting arguments, the most specific should be preferred since it is, in principle, more informed.",
                "Thus, counterarguments generated based on the specificity criterion are expected to be preferable (since they are more informed) to the arguments they try to rebut.",
                "However, there is no guarantee that such counterarguments will always win, since, as we have stated in Section 5, agents in our framework use a preference relation based on joint confidence.",
                "Moreover, one may think that it would be better that the agents generate counterarguments based on the joint confidence preference relation; however it is not obvious how to generate counterarguments based on joint confidence in an efficient way, since collaboration is required in order to evaluate joint confidence.",
                "Thus, the agent generating the counterargument should constantly communicate with the other agents at each step of the induction algorithm used to generate counterarguments (presently one of our future research lines).",
                "Thus, in our framework, when an agent wants to generate a counterargument β to an argument α, β has to be more specific than α (i.e. α.D < β.D).",
                "The generation of counterarguments using the specificity criterion imposes some restrictions over the learning method, although LID or ID3 can be easily adapted for this task.",
                "For instance, LID is an algorithm that generates a description starting from scratch and heuristically adding features to that term.",
                "Thus, at every step, the description is made more specific than in the previous step, and the number of cases that are subsumed by that description is reduced.",
                "When the description covers only (or almost only) cases of a single solution class LID terminates and predicts that solution class.",
                "To generate a counterargument to an argument α LID just has to use as starting point the description α.D instead of starting from scratch.",
                "In this way, the justification provided by LID will always be subsumed by α.D, and thus the resulting counterargument will be more specific than α.",
                "However, notice that LID may sometimes not be able to generate counterarguments, since LID may not be able to specialize the description α.D any further, or because the agent Ai has no case inCi that is subsumed by α.D.",
                "Figure 4 shows how an agent A2 that disagreed with the argument shown in Figure 3, generates a counterargument using LID.",
                "Moreover, Figure 4 shows the generation of a counterargument β1 2 for the argument α0 1 (in Figure 3) that is a specialization of α0 1. 978 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Solution: hadromerida Justification: D1 Sponge Spikulate skeleton External features External features Gemmules: no Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Case Base of A1 LID New sponge P Figure 3: Example of a real justification generated by LID in the marine sponges data set.",
                "Specifically, in our experiments, when an agent Ai wants to rebut an argument α, uses the following policy: 1.",
                "Agent Ai uses LID to try to find a counterargument β more specific than α; if found, β is sent to the other agent as a counterargument of α. 2.",
                "If not found, then Ai searches for a counterexample c ∈ Ci of α.",
                "If a case c is found, then c is sent to the other agent as a counterexample of α. 3.",
                "If no counterexamples are found, then Ai cannot rebut the argument α. 7.",
                "ARGUMENTATION-BASED MULTI-AGENT LEARNING The interaction protocol of AMAL allows a group of agents A1, ..., An to deliberate about the correct solution of a problem P by means of an argumentation process.",
                "If the argumentation process arrives to a consensual solution, the joint deliberation ends; otherwise a weighted vote is used to determine the joint solution.",
                "Moreover, AMAL also allows the agents to learn from the counterexamples received from other agents.",
                "The AMAL protocol consists on a series of rounds.",
                "In the initial round, each agent states which is its individual prediction for P. Then, at each round an agent can try to rebut the prediction made by any of the other agents.",
                "The protocol uses a token passing mechanism so that agents (one at a time) can send counterarguments or counterexamples if they disagree with the prediction made by any other agent.",
                "Specifically, each agent is allowed to send one counterargument or counterexample each time he gets the token (notice that this restriction is just to simplify the protocol, and that it does not restrict the number of counterargument an agent can sent, since they can be delayed for subsequent rounds).",
                "When an agent receives a counterargument or counterexample, it informs the other agents if it accepts the counterargument (and changes its prediction) or not.",
                "Moreover, agents have also the opportunity to answer to counterarguments when they receive the token, by trying to generate a counterargument to the counterargument.",
                "When all the agents have had the token once, the token returns to the first agent, and so on.",
                "If at any time in the protocol, all the agents agree or during the last n rounds no agent has generated any counterargument, the protocol ends.",
                "Moreover, if at the end of the argumentation the agents have not reached an agreement, then a voting mechanism that uses the confidence of each prediction as weights is used to decide the final solution (Thus, AMAL follows the same mechanism as human committees, first each individual member of a committee exposes his arguments and discuses those of the other members (joint deliberation), and if no consensus is reached, then a voting mechanism is required).",
                "At each iteration, agents can use the following performatives: • assert(α): the justified prediction held during the next round will be α.",
                "An agent can only hold a single prediction at each round, thus is multiple asserts are send, only the last one is considered as the currently held prediction. • rebut(β, α): the agent has found a counterargument β to the prediction α.",
                "We will define Ht = αt 1, ..., αt n as the predictions that each of the n agents hold at a round t. Moreover, we will also define contradict(αt i) = {α ∈ Ht|α.S = αt i.S} as the set of contradicting arguments for an agent Ai in a round t, i.e. the set of arguments at round t that support a different solution class than αt i.",
                "The protocol is initiated because one of the agents receives a problem P to be solved.",
                "After that, the agent informs all the other agents about the problem P to solve, and the protocol starts: 1.",
                "At round t = 0, each one of the agents individually solves P, and builds a justified prediction using its own CBR method.",
                "Then, each agent Ai sends the performative assert(α0 i ) to the other agents.",
                "Thus, the agents know H0 = α0 i , ..., α0 n .",
                "Once all the predictions have been sent the token is given to the first agent A1. 2.",
                "At each round t (other than 0), the agents check whether their arguments in Ht agree.",
                "If they do, the protocol moves to step 5.",
                "Moreover, if during the last n rounds no agent has sent any counterexample or counterargument, the protocol also moves to step 5.",
                "Otherwise, the agent Ai owner of the token tries to generate a counterargument for each of the opposing arguments in contradict(αt i) ⊆ Ht (see Section 6.1).",
                "Then, the counterargument βt i against the prediction αt j with the lowest confidence C(αt j) is selected (since αt j is the prediction more likely to be successfully rebutted). • If βt i is a counterargument, then, Ai locally compares αt i with βt i by assessing their confidence against its individual case base Ci (see Section 5) (notice that Ai is comparing its previous argument with the counterargument that Ai itself has just generated and that is about The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 979 Sponge Spikulate skeleton External features External features Gemmules: no Growing: Spikulate Skeleton Megascleres Uniform length: no Megascleres Smooth form: tylostyle Growing Grow: massive Case Base of A2 LID Solution: astrophorida Justification: D2 Figure 4: Generation of a counterargument using LID in the sponges data set. to send to Aj).",
                "If CAi (βt i ) > CAi (αt i), then Ai considers that βt i is stronger than its previous argument, changes its argument to βt i by sending assert(βt i ) to the rest of the agents (the intuition behind this is that since a counterargument is also an argument, Ai checks if the newly counterargument is a better argument than the one he was previously holding) and rebut(βt i , αt j) to Aj.",
                "Otherwise (i.e.",
                "CAi (βt i ) ≤ CAi (αt i)), Ai will send only rebut(βt i , αt j) to Aj.",
                "In any of the two situations the protocol moves to step 3. • If βt i is a counterexample c, then Ai sends rebut(c, αt j) to Aj.",
                "The protocol moves to step 4. • If Ai cannot generate any counterargument or counterexample, the token is sent to the next agent, a new round t + 1 starts, and the protocol moves to state 2. 3.",
                "The agent Aj that has received the counterargument βt i , locally compares it against its own argument, αt j, by locally assessing their confidence.",
                "If CAj (βt i ) > CAj (αt j), then Aj will accept the counterargument as stronger than its own argument, and it will send assert(βt i ) to the other agents.",
                "Otherwise (i.e.",
                "CAj (βt i ) ≤ CAj (αt j)), Aj will not accept the counterargument, and will inform the other agents accordingly.",
                "Any of the two situations start a new round t + 1, Ai sends the token to the next agent, and the protocol moves back to state 2. 4.",
                "The agent Aj that has received the counterexample c retains it into its case base and generates a new argument αt+1 j that takes into account c, and informs the rest of the agents by sending assert(αt+1 j ) to all of them.",
                "Then, Ai sends the token to the next agent, a new round t + 1 starts, and the protocol moves back to step 2. 5.",
                "The protocol ends yielding a joint prediction, as follows: if the arguments in Ht agree then their prediction is the joint prediction, otherwise a voting mechanism is used to decide the joint prediction.",
                "The voting mechanism uses the joint confidence measure as the voting weights, as follows: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Moreover, in order to avoid infinite iterations, if an agent sends twice the same argument or counterargument to the same agent, the message is not considered. 8.",
                "EXEMPLIFICATION Let us consider a system composed of three agents A1, A2 and A3.",
                "One of the agents, A1 receives a problem P to solve, and decides to use AMAL to solve it.",
                "For that reason, invites A2 and A3 to take part in the argumentation process.",
                "They accept the invitation, and the argumentation protocol starts.",
                "Initially, each agent generates its individual prediction for P, and broadcasts it to the other agents.",
                "Thus, all of them can compute H0 = α0 1, α0 2, α0 3 .",
                "In particular, in this example: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 starts owning the token and tries to generate counterarguments for α0 2 and α0 3, but does not succeed, however it has one counterexample c13 for α0 3.",
                "Thus, A1 sends the the message rebut( c13, α0 3) to A3.",
                "A3 incorporates c13 into its case base and tries to solve the problem P again, now taking c13 into consideration.",
                "A3 comes up with the justified prediction α1 3 = A3, P, hadromerida, D4 , and broadcasts it to the rest of the agents with the message assert(α1 3).",
                "Thus, all of them know the new H1 = α0 1, α0 2, α1 3 .",
                "Round 1 starts and A2 gets the token.",
                "A2 tries to generate counterarguments for α0 1 and α1 3 and only succeeds to generate a counterargument β1 2 = A2, P, astrophorida, D5 against α1 3.",
                "The counterargument is sent to A3 with the message rebut(β1 2 , α1 3).",
                "Agent A3 receives the counterargument and assesses its local confidence.",
                "The result is that the individual confidence of the counterargument β1 2 is lower than the local confidence of α1 3.",
                "Therefore, A3 does not accept the counterargument, and thus H2 = α0 1, α0 2, α1 3 .",
                "Round 2 starts and A3 gets the token.",
                "A3 generates a counterargument β2 3 = A3, P, hadromerida, D6 for α0 2 and sends it to A2 with the message rebut(β2 3 , α0 2).",
                "Agent A2 receives the counterargument and assesses its local confidence.",
                "The result is that the local confidence of the counterargument β2 3 is higher than the local confidence of α0 2.",
                "Therefore, A2 accepts the counterargument and informs the rest of the agents with the message assert(β2 3 ).",
                "After that, H3 = α0 1, β2 3 , α1 3 .",
                "At Round 3, since all the agents agree (all the justified predictions in H3 predict hadromerida as the solution class) The protocol ends, and A1 (the agent that received the problem) considers hadromerida as the joint solution for the problem P. 9.",
                "EXPERIMENTAL EVALUATION 980 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Voting Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Voting Individual Figure 5: Individual and joint accuracy for 2 to 5 agents.",
                "In this section we empirically evaluate the AMAL argumentation framework.",
                "We have made experiments in two different data sets: soybean (from the UCI machine learning repository) and sponge (a relational data set).",
                "The soybean data set has 307 examples and 19 solution classes, while the sponge data set has 280 examples and 3 solution classes.",
                "In an experimental run, the data set is divided in 2 sets: the training set and the test set.",
                "The training set examples are distributed among 5 different agents without replication, i.e. there is no example shared by two agents.",
                "In the testing stage, problems in the test set arrive randomly to one of the agents, and their goal is to predict the correct solution.",
                "The experiments are designed to test two hypotheses: (H1) that argumentation is a useful framework for joint deliberation and can improve over other typical methods such as voting; and (H2) that learning from communication improves the individual performance of a learning agent participating in an argumentation process.",
                "Moreover, we also expect that the improvement achieved from argumentation will increase as the number of agents participating in the argumentation increases (since more information will be taken into account).",
                "Concerning H1 (argumentation is a useful framework for joint deliberation), we ran 4 experiments, using 2, 3, 4, and 5 agents respectively (in all experiments each agent has a 20% of the training data, since the training is always distributed among 5 agents).",
                "Figure 5 shows the result of those experiments in the sponge and soybean data sets.",
                "Classification accuracy is plotted in the vertical axis, and in the horizontal axis the number of agents that took part in the argumentation processes is shown.",
                "For each number of agents, three bars are shown: individual, Voting, and AMAL.",
                "The individual bar shows the average accuracy of individual agents predictions; the voting bar shows the average accuracy of the joint prediction achieved by voting but without any argumentation; and finally the AMAL bar shows the average accuracy of the joint prediction using argumentation.",
                "The results shown are the average of 5 10-fold cross validation runs.",
                "Figure 5 shows that collaboration (voting and AMAL) outperforms individual problem solving.",
                "Moreover, as we expected, the accuracy improves as more agents collaborate, since more information is taken into account.",
                "We can also see that AMAL always outperforms standard voting, proving that joint decisions are based on better information as provided by the argumentation process.",
                "For instance, the joint accuracy for 2 agents in the sponge data set is of 87.57% for AMAL and 86.57% for voting (while individual accuracy is just 80.07%).",
                "Moreover, the improvement achieved by AMAL over Voting is even larger in the soybean data set.",
                "The reason is that the soybean data set is more difficult (in the sense that agents need more data to produce good predictions).",
                "These experimental results show that AMAL effectively exploits the opportunity for improvement: the accuracy is higher only because more agents have changed their opinion during argumentation (otherwise they would achieve the same result as Voting).",
                "Concerning H2 (learning from communication in argumentation processes improves individual prediction ), we ran the following experiment: initially, we distributed a 25% of the training set among the five agents; after that, the rest of the cases in the training set is sent to the agents one by one; when an agent receives a new training case, it has several options: the agent can discard it, the agent can retain it, or the agent can use it for engaging an argumentation process.",
                "Figure 6 shows the result of that experiment for the two data sets.",
                "Figure 6 contains three plots, where NL (not learning) shows accuracy of an agent with no learning at all; L (learning), shows the evolution of the individual classification accuracy when agents learn by retaining the training cases they individually receive (notice that when all the training cases have been retained at 100%, the accuracy should be equal to that of Figure 5 for individual agents); and finally LFC (learning from communication) shows the evolution of the individual classification accuracy of learning agents that also learn by retaining those counterexamples received during argumentation (i.e. they learn both from training examples and counterexamples).",
                "Figure 6 shows that if an agent Ai learns also from communication, Ai can significantly improve its individual performance with just a small number of additional cases (those selected as relevant counterexamples for Ai during argumentation).",
                "For instance, in the soybean data set, individual agents have achieved an accuracy of 70.62% when they also learn from communication versus an accuracy of 59.93% when they only learn from their individual experience.",
                "The number of cases learnt from communication depends on the properties of the data set: in the sponges data set, agents have retained only very few additional cases, and significantly improved individual accuracy; namely they retain 59.96 cases in average (compared to the 50.4 cases retained if they do not learn from communication).",
                "In the soybean data set more counterexamples are learnt to significantly improve individual accuracy, namely they retain 87.16 cases in average (compared to 55.27 cases retained if they do not learn from communication).",
                "Finally, the fact that both data sets show a significant improvement points out the adaptive nature of the argumentation-based approach to learning from communication: the useful cases are selected as counterexamples (and no more than those needed), and they have the intended effect. 10.",
                "RELATED WORK Concerning CBR in a multi-agent setting, the first research was on negotiated case retrieval [11] among groups of agents.",
                "Our work on multi-agent case-based learning started in 1999 [6]; later Mc Ginty and Smyth [7] presented a multi-agent collaborative CBR approach (CCBR) for planning.",
                "Finally, another interesting approach is multi-<br>case-base reason</br>ing (MCBR) [5], that deals with The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOYBEAN 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figure 6: Learning from communication resulting from argumentation in a system composed of 5 agents. distributed systems where there are several case bases available for the same task and addresses the problems of cross-case base adaptation.",
                "The main difference is that our MAC approach is a way to distribute the Reuse process of CBR (using a voting system) while Retrieve is performed individually by each agent; the other multiagent CBR approaches, however, focus on distributing the Retrieve process.",
                "Research on MAS argumentation focus on several issues like a) logics, protocols and languages that support argumentation, b) argument selection and c) argument interpretation.",
                "Approaches for logic and languages that support argumentation include defeasible logic [4] and BDI models [13].",
                "Although argument selection is a key aspect of automated argumentation (see [12] and [13]), most research has been focused on preference relations among arguments.",
                "In our framework we have addressed both argument selection and preference relations using a case-based approach. 11.",
                "CONCLUSIONS AND FUTURE WORK In this paper we have presented an argumentation-based framework for multi-agent learning.",
                "Specifically, we have presented AMAL, a framework that allows a group of learning agents to argue about the solution of a given problem and we have shown how the learning capabilities can be used to generate arguments and counterarguments.",
                "The experimental evaluation shows that the increased amount of information provided to the agents by the argumentation process increases their predictive accuracy, and specially when an adequate number of agents take part in the argumentation.",
                "The main contributions of this work are: a) an argumentation framework for learning agents; b) a case-based preference relation over arguments, based on computing an overall confidence estimation of arguments; c) a case-based policy to generate counterarguments and select counterexamples; and d) an argumentation-based approach for learning from communication.",
                "Finally, in the experiments presented here a learning agent would retain all counterexamples submitted by the other agent; however, this is a very simple case retention policy, and we will like to experiment with more informed policies - with the goal that individual learning agents could significantly improve using only a small set of cases proposed by other agents.",
                "Finally, our approach is focused on lazy learning, and future works aims at incorporating eager inductive learning inside the argumentative framework for learning from communication. 12.",
                "REFERENCES [1] Agnar Aamodt and Enric Plaza.",
                "Case-based reasoning: Foundational issues, methodological variations, and system approaches.",
                "Artificial Intelligence Communications, 7(1):39-59, 1994. [2] E. Armengol and E. Plaza.",
                "Lazy induction of descriptions for relational case-based learning.",
                "In ECML2001, pages 13-24, 2001. [3] Gerhard Brewka.",
                "Dynamic argument systems: A formal model of argumentation processes based on situation calculus.",
                "Journal of Logic and Computation, 11(2):257-282, 2001. [4] Carlos I. Chesñevar and Guillermo R. Simari.",
                "Formalizing Defeasible Argumentation using Labelled Deductive Systems.",
                "Journal of Computer Science & Technology, 1(4):18-33, 2000. [5] D. Leake and R. Sooriamurthi.",
                "Automatically selecting strategies for multi-<br>case-base reason</br>ing.",
                "In S. Craw and A. Preece, editors, ECCBR2002, pages 204-219, Berlin, 2002.",
                "Springer Verlag. [6] Francisco J. Martín, Enric Plaza, and Josep-Lluis Arcos.",
                "Knowledge and experience reuse through communications among competent (peer) agents.",
                "International Journal of Software Engineering and Knowledge Engineering, 9(3):319-341, 1999. [7] Lorraine McGinty and Barry Smyth.",
                "Collaborative case-based reasoning: Applications in personalized route planning.",
                "In I. Watson and Q. Yang, editors, ICCBR, number 2080 in LNAI, pages 362-376.",
                "Springer-Verlag, 2001. [8] Santi Ontañón and Enric Plaza.",
                "Justification-based multiagent learning.",
                "In ICML2003, pages 576-583.",
                "Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol, and Santiago Ontañón.",
                "The explanatory power of symbolic similarity in case-based reasoning.",
                "Artificial Intelligence Review, 24(2):145-161, 2005. [10] David Poole.",
                "On the comparison of theories: Preferring the most specific explanation.",
                "In IJCAI-85, pages 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser, and Susan Lander.",
                "Retrieval and reasoning in distributed case bases.",
                "Technical report, UMass Computer Science Department, 1995. [12] K. Sycara S. Kraus and A. Evenchik.",
                "Reaching agreements through argumentation: a logical model and implementation.",
                "Artificial Intelligence Journal, 104:1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra.",
                "Agents that reason and negotiate by arguing.",
                "Journal of Logic and Computation, 8:261-292, 1998. [14] Bruce A. Wooley.",
                "Explanation component of software systems.",
                "ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Finalmente, otro enfoque interesante es la \"razón de la base de casos\" de \"MCBR) [5], que se ocupa del sexto INTL.",
                ""
            ],
            "translated_text": "",
            "candidates": [
                "Razón de la base de casos",
                "razón de la base de casos",
                ""
            ],
            "error": []
        }
    }
}