{
    "id": "S0957417416303773",
    "original_text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.",
    "original_translation": "",
    "original_sentences": [
        "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
        "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
        "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
        "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
        "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
        "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
        "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
        "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
    ],
    "error_count": 0,
    "keys": {
        "a set of orthogonal vectors": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is <br>a set of orthogonal vectors</br> sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "data mining": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of <br>data mining</br> many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "dimensionality reduction": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for <br>dimensionality reduction</br> have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "finds the mutually-uncorrelated vectors": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that <br>finds the mutually-uncorrelated vectors</br> onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "linear PCA": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (<br>linear PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "N-dimensional vector space basis": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the <br>N-dimensional vector space basis</br>, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "N^ synthetic KPIs": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the <br>N^ synthetic KPIs</br> represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "PCA": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (<br>PCA</br>) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of <br>PCA</br> (linear <br>PCA</br>) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "Principal Component Analysis method": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the <br>Principal Component Analysis method</br> (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "the first N^": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, <br>the first N^</br>, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "the projection of the samples generates the highest variances": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which <br>the projection of the samples generates the highest variances</br>.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "the simplest version of PCA": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, <br>the simplest version of PCA</br> (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "the variance of the projection of the samples is maximum": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which <br>the variance of the projection of the samples is maximum</br>.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, up to N synthetic orthogonal KPIs may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "up to N synthetic orthogonal KPIs": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen.",
                "Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002).",
                "In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances.",
                "The result is a set of orthogonal vectors sorted in descending order of achieved variance.",
                "The first of these vectors is that onto which the variance of the projection of the samples is maximum.",
                "In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance.",
                "To be rigorous, <br>up to N synthetic orthogonal KPIs</br> may be computed.",
                "However, only a small set of them, the first N^, is enough to account for most of the variance of the data."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}