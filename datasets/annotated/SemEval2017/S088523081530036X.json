{
    "id": "S088523081530036X",
    "original_text": "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages. This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame. Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages. A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer. The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ.",
    "original_translation": "",
    "original_sentences": [
        "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
        "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
        "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
        "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
        "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
    ],
    "error_count": 0,
    "keys": {
        "accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, <br>accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)</br>where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "assume that frames are independent and multiply the posterior estimates of the last layer": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to <br>assume that frames are independent and multiply the posterior estimates of the last layer</br>.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "combine the evidence from past frames": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can <br>combine the evidence from past frames</br> to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "DNN": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the <br>DNN</br> defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "DNNs": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the <br>DNNs</br> particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "evidence from past frames": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the <br>evidence from past frames</br> to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "i-vectors": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. <br>i-vectors</br>), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multiplying the output probabilities pl obtained for all of its frames": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by <br>multiplying the output probabilities pl obtained for all of its frames</br>; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "other approaches": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike <br>other approaches</br> (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "target languages": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the <br>target languages</br>.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "test utterance": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages.",
                "This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame.",
                "Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the <br>test utterance</br> and the targetlanguages.",
                "A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer.",
                "The score sl for language l of a given <br>test utterance</br> is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N∑t=1Nlogp(Ll|xt​, θ)where p(Ll|xt​, θ) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters θ."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}