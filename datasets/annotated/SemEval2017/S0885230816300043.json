{
    "id": "S0885230816300043",
    "original_text": "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.",
    "original_translation": "",
    "original_sentences": [
        "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
        "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
        "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
        "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
        "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
        "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
    ],
    "error_count": 0,
    "keys": {
        "aCMLLR transforms": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the <br>aCMLLR transforms</br> on the baseline GMM–HMM model.",
                "However, training show-based <br>aCMLLR transforms</br> on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based <br>aCMLLR transforms</br> was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "adaptive retraining of the GMM–HMM parameters": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an <br>adaptive retraining of the GMM–HMM parameters</br> following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "adaptive training": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how <br>adaptive training</br> provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "aNAT model": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the <br>aNAT model</br> and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "aNAT procedure": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the <br>aNAT procedure</br>.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "factorisation approach using MLLR speaker transforms": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the <br>factorisation approach using MLLR speaker transforms</br> on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "GMM–HMM model": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline <br>GMM–HMM model</br>.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "speaker adaptation": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers <br>speaker adaptation</br>."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "speaker clustering": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate <br>speaker clustering</br> in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "training show-based aCMLLR transforms": {
            "translated_key": "",
            "original_annotated_sentences": [
                "The final set of experiments involved an adaptive retraining of the GMM–HMM parameters following the aNAT procedure.",
                "This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM–HMM model.",
                "However, <br>training show-based aCMLLR transforms</br> on top of the adaptively trained model boosted the improvement to 0.8% absolute.",
                "This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show.",
                "Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested.",
                "This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}